{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7250d25b",
   "metadata": {},
   "source": [
    "### Data ingestion to vector db\n",
    "#### Data parsing, chuncking, embedding, and vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f01cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484d5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing xgboost.pdf...\n",
      "Loaded 13 documents from ../data/pdf/xgboost.pdf\n",
      "\n",
      "Processing embeddings.pdf...\n",
      "Loaded 16 documents from ../data/pdf/embeddings.pdf\n",
      "\n",
      "Processing attention.pdf...\n",
      "Loaded 11 documents from ../data/pdf/attention.pdf\n",
      "\n",
      "Processing object_detection.pdf...\n",
      "Loaded 21 documents from ../data/pdf/object_detection.pdf\n",
      "\n",
      "total documents loaded: 61\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all the pdf files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_directory = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_directory.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing {pdf_file.name}...\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            ## add source info to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "\n",
    "            print(f\"Loaded {len(documents)} documents from {pdf_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "    \n",
    "    print(f\"\\ntotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6658fabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='XGBoost: A Scalable Tree Boosting System\\nTianqi Chen\\nUniversity of Washington\\ntqchen@cs.washington.edu\\nCarlos Guestrin\\nUniversity of Washington\\nguestrin@cs.washington.edu\\nABSTRACT\\nTree boosting is a highly e\ufb00ective and widely used machine\\nlearning method. In this paper, we describe a scalable end-\\nto-end tree boosting system called XGBoost, which is used\\nwidely by data scientists to achieve state-of-the-art results\\non many machine learning challenges. We propose a novel\\nsparsity-aware algorithm for sparse data and weighted quan-\\ntile sketch for approximate tree learning. More importantly,\\nwe provide insights on cache access patterns, data compres-\\nsion and sharding to build a scalable tree boosting system.\\nBy combining these insights, XGBoost scales beyond billions\\nof examples using far fewer resources than existing systems.\\nKeywords\\nLarge-scale Machine Learning\\n1. INTRODUCTION\\nMachine learning and data-driven approaches are becom-\\ning very important in many areas. Smart spam classi\ufb01ers\\nprotect our email by learning from massive amounts of spam\\ndata and user feedback; advertising systems learn to match\\nthe right ads with the right context; fraud detection systems\\nprotect banks from malicious attackers; anomaly event de-\\ntection systems help experimental physicists to \ufb01nd events\\nthat lead to new physics. There are two important factors\\nthat drive these successful applications: usage of e\ufb00ective\\n(statistical) models that capture the complex data depen-\\ndencies and scalable learning systems that learn the model\\nof interest from large datasets.\\nAmong the machine learning methods used in practice,\\ngradient tree boosting [10] 1 is one technique that shines\\nin many applications. Tree boosting has been shown to\\ngive state-of-the-art results on many standard classi\ufb01cation\\nbenchmarks [16]. LambdaMART [5], a variant of tree boost-\\ning for ranking, achieves state-of-the-art result for ranking\\n1Gradient tree boosting is also known as gradient boosting\\nmachine (GBM) or gradient boosted regression tree (GBRT)\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\\non the \ufb01rst page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD \u201916, August 13-17, 2016, San Francisco, CA, USA\\nc\u20dd2016 Copyright held by the owner/author(s).\\nACM ISBN .\\nDOI:\\nproblems. Besides being used as a stand-alone predictor, it\\nis also incorporated into real-world production pipelines for\\nad click through rate prediction [15]. Finally, it is the de-\\nfacto choice of ensemble method and is used in challenges\\nsuch as the Net\ufb02ix prize [3].\\nIn this paper, we describe XGBoost, a scalable machine\\nlearning system for tree boosting. The system is available as\\nan open source package2. The impact of the system has been\\nwidely recognized in a number of machine learning and data\\nmining challenges. Take the challenges hosted by the ma-\\nchine learning competition site Kaggle for example. Among\\nthe 29 challenge winning solutions 3 published at Kaggle\u2019s\\nblog during 2015, 17 solutions used XGBoost. Among these\\nsolutions, eight solely used XGBoost to train the model,\\nwhile most others combined XGBoost with neural nets in en-\\nsembles. For comparison, the second most popular method,\\ndeep neural nets, was used in 11 solutions. The success\\nof the system was also witnessed in KDDCup 2015, where\\nXGBoost was used by every winning team in the top-10.\\nMoreover, the winning teams reported that ensemble meth-\\nods outperform a well-con\ufb01gured XGBoost by only a small\\namount [1].\\nThese results demonstrate that our system gives state-of-\\nthe-art results on a wide range of problems. Examples of\\nthe problems in these winning solutions include: store sales\\nprediction; high energy physics event classi\ufb01cation; web text\\nclassi\ufb01cation; customer behavior prediction; motion detec-\\ntion; ad click through rate prediction; malware classi\ufb01cation;\\nproduct categorization; hazard risk prediction; massive on-\\nline course dropout rate prediction. While domain depen-\\ndent data analysis and feature engineering play an important\\nrole in these solutions, the fact that XGBoost is the consen-\\nsus choice of learner shows the impact and importance of\\nour system and tree boosting.\\nThe most important factor behind the success of XGBoost\\nis its scalability in all scenarios. The system runs more than\\nten times faster than existing popular solutions on a single\\nmachine and scales to billions of examples in distributed or\\nmemory-limited settings. The scalability of XGBoost is due\\nto several important systems and algorithmic optimizations.\\nThese innovations include: a novel tree learning algorithm\\nis for handling sparse data; a theoretically justi\ufb01ed weighted\\nquantile sketch procedure enables handling instance weights\\nin approximate tree learning. Parallel and distributed com-\\nputing makes learning faster which enables quicker model ex-\\nploration. More importantly, XGBoost exploits out-of-core\\n2https://github.com/dmlc/xgboost\\n3Solutions come from of top-3 teams of each competitions.\\narXiv:1603.02754v3  [cs.LG]  10 Jun 2016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='computation and enables data scientists to process hundred\\nmillions of examples on a desktop. Finally, it is even more\\nexciting to combine these techniques to make an end-to-end\\nsystem that scales to even larger data with the least amount\\nof cluster resources. The major contributions of this paper\\nis listed as follows:\\n\u2022We design and build a highly scalable end-to-end tree\\nboosting system.\\n\u2022We propose a theoretically justi\ufb01ed weighted quantile\\nsketch for e\ufb03cient proposal calculation.\\n\u2022We introduce a novel sparsity-aware algorithm for par-\\nallel tree learning.\\n\u2022We propose an e\ufb00ective cache-aware block structure\\nfor out-of-core tree learning.\\nWhile there are some existing works on parallel tree boost-\\ning [22, 23, 19], the directions such as out-of-core compu-\\ntation, cache-aware and sparsity-aware learning have not\\nbeen explored. More importantly, an end-to-end system\\nthat combines all of these aspects gives a novel solution for\\nreal-world use-cases. This enables data scientists as well as\\nresearchers to build powerful variants of tree boosting al-\\ngorithms [7, 8]. Besides these major contributions, we also\\nmake additional improvements in proposing a regularized\\nlearning objective, which we will include for completeness.\\nThe remainder of the paper is organized as follows. We\\nwill \ufb01rst review tree boosting and introduce a regularized\\nobjective in Sec. 2. We then describe the split \ufb01nding meth-\\nods in Sec. 3 as well as the system design in Sec. 4, including\\nexperimental results when relevant to provide quantitative\\nsupport for each optimization we describe. Related work\\nis discussed in Sec. 5. Detailed end-to-end evaluations are\\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7.\\n2. TREE BOOSTING IN A NUTSHELL\\nWe review gradient tree boosting algorithms in this sec-\\ntion. The derivation follows from the same idea in existing\\nliteratures in gradient boosting. Specicially the second order\\nmethod is originated from Friedman et al. [12]. We make mi-\\nnor improvements in the reguralized objective, which were\\nfound helpful in practice.\\n2.1 Regularized Learning Objective\\nFor a given data set with n examples and m features\\nD= {(xi,yi)}(|D|= n,xi \u2208Rm,yi \u2208R), a tree ensem-\\nble model (shown in Fig. 1) uses K additive functions to\\npredict the output.\\n\u02c6yi = \u03c6(xi) =\\nK\u2211\\nk=1\\nfk(xi), fk \u2208F, (1)\\nwhere F = {f(x) = wq(x)}(q : Rm \u2192T,w \u2208RT) is the\\nspace of regression trees (also known as CART). Here q rep-\\nresents the structure of each tree that maps an example to\\nthe corresponding leaf index. T is the number of leaves in the\\ntree. Each fk corresponds to an independent tree structure\\nq and leaf weights w. Unlike decision trees, each regression\\ntree contains a continuous score on each of the leaf, we use\\nwi to represent score on i-th leaf. For a given example, we\\nwill use the decision rules in the trees (given byq) to classify\\nFigure 1: Tree Ensemble Model. The \ufb01nal predic-\\ntion for a given example is the sum of predictions\\nfrom each tree.\\nit into the leaves and calculate the \ufb01nal prediction by sum-\\nming up the score in the corresponding leaves (given by w).\\nTo learn the set of functions used in the model, we minimize\\nthe following regularized objective.\\nL(\u03c6) =\\n\u2211\\ni\\nl(\u02c6yi,yi) +\\n\u2211\\nk\\n\u2126(fk)\\nwhere \u2126(f) = \u03b3T + 1\\n2\u03bb\u2225w\u22252\\n(2)\\nHere l is a di\ufb00erentiable convex loss function that measures\\nthe di\ufb00erence between the prediction \u02c6yi and the target yi.\\nThe second term \u2126 penalizes the complexity of the model\\n(i.e., the regression tree functions). The additional regular-\\nization term helps to smooth the \ufb01nal learnt weights to avoid\\nover-\ufb01tting. Intuitively, the regularized objective will tend\\nto select a model employing simple and predictive functions.\\nA similar regularization technique has been used in Regu-\\nlarized greedy forest (RGF) [25] model. Our objective and\\nthe corresponding learning algorithm is simpler than RGF\\nand easier to parallelize. When the regularization parame-\\nter is set to zero, the objective falls back to the traditional\\ngradient tree boosting.\\n2.2 Gradient Tree Boosting\\nThe tree ensemble model in Eq. (2) includes functions as\\nparameters and cannot be optimized using traditional opti-\\nmization methods in Euclidean space. Instead, the model\\nis trained in an additive manner. Formally, let \u02c6 y(t)\\ni be the\\nprediction of the i-th instance at the t-th iteration, we will\\nneed to add ft to minimize the following objective.\\nL(t) =\\nn\u2211\\ni=1\\nl(yi, \u02c6yi\\n(t\u22121) + ft(xi)) + \u2126(ft)\\nThis means we greedily add the ft that most improves our\\nmodel according to Eq. (2). Second-order approximation\\ncan be used to quickly optimize the objective in the general\\nsetting [12].\\nL(t) \u2243\\nn\u2211\\ni=1\\n[l(yi,\u02c6y(t\u22121)) + gift(xi) + 1\\n2hif2\\nt(xi)] + \u2126(ft)\\nwhere gi = \u2202\u02c6y(t\u22121) l(yi,\u02c6y(t\u22121)) and hi = \u22022\\n\u02c6y(t\u22121) l(yi,\u02c6y(t\u22121))\\nare \ufb01rst and second order gradient statistics on the loss func-\\ntion. We can remove the constant terms to obtain the fol-\\nlowing simpli\ufb01ed objective at step t.\\n\u02dcL(t) =\\nn\u2211\\ni=1\\n[gift(xi) + 1\\n2hif2\\nt(xi)] + \u2126(ft) (3)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Structure Score Calculation. We only\\nneed to sum up the gradient and second order gra-\\ndient statistics on each leaf, then apply the scoring\\nformula to get the quality score.\\nDe\ufb01ne Ij = {i|q(xi) = j}as the instance set of leaf j. We\\ncan rewrite Eq (3) by expanding \u2126 as follows\\n\u02dcL(t) =\\nn\u2211\\ni=1\\n[gift(xi) + 1\\n2hif2\\nt(xi)] + \u03b3T + 1\\n2\u03bb\\nT\u2211\\nj=1\\nw2\\nj\\n=\\nT\u2211\\nj=1\\n[(\\n\u2211\\ni\u2208Ij\\ngi)wj + 1\\n2(\\n\u2211\\ni\u2208Ij\\nhi + \u03bb)w2\\nj] + \u03b3T\\n(4)\\nFor a \ufb01xed structure q(x), we can compute the optimal\\nweight w\u2217\\nj of leaf j by\\nw\u2217\\nj = \u2212\\n\u2211\\ni\u2208Ij\\ngi\\n\u2211\\ni\u2208Ij\\nhi + \u03bb, (5)\\nand calculate the corresponding optimal value by\\n\u02dcL(t)(q) = \u22121\\n2\\nT\u2211\\nj=1\\n(\u2211\\ni\u2208Ij\\ngi)2\\n\u2211\\ni\u2208Ij\\nhi + \u03bb + \u03b3T. (6)\\nEq (6) can be used as a scoring function to measure the\\nquality of a tree structure q. This score is like the impurity\\nscore for evaluating decision trees, except that it is derived\\nfor a wider range of objective functions. Fig. 2 illustrates\\nhow this score can be calculated.\\nNormally it is impossible to enumerate all the possible\\ntree structures q. A greedy algorithm that starts from a\\nsingle leaf and iteratively adds branches to the tree is used\\ninstead. Assume that IL and IR are the instance sets of left\\nand right nodes after the split. Lettting I = IL \u222aIR, then\\nthe loss reduction after the split is given by\\nLsplit = 1\\n2\\n[\\n(\u2211\\ni\u2208IL\\ngi)2\\n\u2211\\ni\u2208IL\\nhi + \u03bb +\\n(\u2211\\ni\u2208IR\\ngi)2\\n\u2211\\ni\u2208IR\\nhi + \u03bb \u2212 (\u2211\\ni\u2208I gi)2\\n\u2211\\ni\u2208I hi + \u03bb\\n]\\n\u2212\u03b3\\n(7)\\nThis formula is usually used in practice for evaluating the\\nsplit candidates.\\n2.3 Shrinkage and Column Subsampling\\nBesides the regularized objective mentioned in Sec. 2.1,\\ntwo additional techniques are used to further prevent over-\\n\ufb01tting. The \ufb01rst technique is shrinkage introduced by Fried-\\nman [11]. Shrinkage scales newly added weights by a factor\\n\u03b7 after each step of tree boosting. Similar to a learning rate\\nin tochastic optimization, shrinkage reduces the in\ufb02uence of\\neach individual tree and leaves space for future trees to im-\\nprove the model. The second technique is column (feature)\\nsubsampling. This technique is used in RandomForest [4,\\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\\nInput: I, instance set of current node\\nInput: d, feature dimension\\ngain\u21900\\nG\u2190\u2211\\ni\u2208I gi, H \u2190\u2211\\ni\u2208I hi\\nfor k= 1 to m do\\nGL \u21900, HL \u21900\\nfor j in sorted(I, by xjk) do\\nGL \u2190GL + gj, HL \u2190HL + hj\\nGR \u2190G\u2212GL, HR \u2190H\u2212HL\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\nend\\nOutput: Split with max score\\nAlgorithm 2: Approximate Algorithm for Split Finding\\nfor k= 1 to m do\\nPropose Sk = {sk1,sk2,\u00b7\u00b7\u00b7skl}by percentiles on feature k.\\nProposal can be done per tree (global), or per split(local).\\nend\\nfor k= 1 to m do\\nGkv \u2190= \u2211\\nj\u2208{j|sk,v\u2265xjk>sk,v\u22121}gj\\nHkv \u2190= \u2211\\nj\u2208{j|sk,v\u2265xjk>sk,v\u22121}hj\\nend\\nFollow same step as in previous section to \ufb01nd max\\nscore only among proposed splits.\\n13], It is implemented in a commercial software TreeNet 4\\nfor gradient boosting, but is not implemented in existing\\nopensource packages. According to user feedback, using col-\\numn sub-sampling prevents over-\ufb01tting even more so than\\nthe traditional row sub-sampling (which is also supported).\\nThe usage of column sub-samples also speeds up computa-\\ntions of the parallel algorithm described later.\\n3. SPLIT FINDING ALGORITHMS\\n3.1 Basic Exact Greedy Algorithm\\nOne of the key problems in tree learning is to \ufb01nd the\\nbest split as indicated by Eq (7). In order to do so, a split\\n\ufb01nding algorithm enumerates over all the possible splits on\\nall the features. We call this the exact greedy algorithm.\\nMost existing single machine tree boosting implementations,\\nsuch as scikit-learn [20], R\u2019s gbm [21] as well as the single\\nmachine version of XGBoost support the exact greedy algo-\\nrithm. The exact greedy algorithm is shown in Alg. 1. It\\nis computationally demanding to enumerate all the possible\\nsplits for continuous features. In order to do so e\ufb03ciently,\\nthe algorithm must \ufb01rst sort the data according to feature\\nvalues and visit the data in sorted order to accumulate the\\ngradient statistics for the structure score in Eq (7).\\n3.2 Approximate Algorithm\\nThe exact greedy algorithm is very powerful since it enu-\\nmerates over all possible splitting points greedily. However,\\nit is impossible to e\ufb03ciently do so when the data does not \ufb01t\\nentirely into memory. Same problem also arises in the dis-\\n4https://www.salford-systems.com/products/treenet'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='0 10 20 30 40 50 60 70 80 90\\nNumber of Iterations\\n0.75\\n0.76\\n0.77\\n0.78\\n0.79\\n0.80\\n0.81\\n0.82\\n0.83\\nTest AUC\\nexact greedy\\nglobal eps=0.3\\nlocal eps=0.3\\nglobal eps=0.05\\nFigure 3: Comparison of test AUC convergence on\\nHiggs 10M dataset. The eps parameter corresponds\\nto the accuracy of the approximate sketch. This\\nroughly translates to 1 / eps buckets in the proposal.\\nWe \ufb01nd that local proposals require fewer buckets,\\nbecause it re\ufb01ne split candidates.\\ntributed setting. To support e\ufb00ective gradient tree boosting\\nin these two settings, an approximate algorithm is needed.\\nWe summarize an approximate framework, which resem-\\nbles the ideas proposed in past literatures [17, 2, 22], in\\nAlg. 2. To summarize, the algorithm \ufb01rst proposes can-\\ndidate splitting points according to percentiles of feature\\ndistribution (a speci\ufb01c criteria will be given in Sec. 3.3).\\nThe algorithm then maps the continuous features into buck-\\nets split by these candidate points, aggregates the statistics\\nand \ufb01nds the best solution among proposals based on the\\naggregated statistics.\\nThere are two variants of the algorithm, depending on\\nwhen the proposal is given. The global variant proposes all\\nthe candidate splits during the initial phase of tree construc-\\ntion, and uses the same proposals for split \ufb01nding at all lev-\\nels. The local variant re-proposes after each split. The global\\nmethod requires less proposal steps than the local method.\\nHowever, usually more candidate points are needed for the\\nglobal proposal because candidates are not re\ufb01ned after each\\nsplit. The local proposal re\ufb01nes the candidates after splits,\\nand can potentially be more appropriate for deeper trees. A\\ncomparison of di\ufb00erent algorithms on a Higgs boson dataset\\nis given by Fig. 3. We \ufb01nd that the local proposal indeed\\nrequires fewer candidates. The global proposal can be as\\naccurate as the local one given enough candidates.\\nMost existing approximate algorithms for distributed tree\\nlearning also follow this framework. Notably, it is also possi-\\nble to directly construct approximate histograms of gradient\\nstatistics [22]. It is also possible to use other variants of bin-\\nning strategies instead of quantile [17]. Quantile strategy\\nbene\ufb01t from being distributable and recomputable, which\\nwe will detail in next subsection. From Fig. 3, we also \ufb01nd\\nthat the quantile strategy can get the same accuracy as exact\\ngreedy given reasonable approximation level.\\nOur system e\ufb03ciently supports exact greedy for the single\\nmachine setting, as well as approximate algorithm with both\\nlocal and global proposal methods for all settings. Users can\\nfreely choose between the methods according to their needs.\\n3.3 Weighted Quantile Sketch\\nOne important step in the approximate algorithm is to\\npropose candidate split points. Usually percentiles of a fea-\\nFigure 4: Tree structure with default directions. An\\nexample will be classi\ufb01ed into the default direction\\nwhen the feature needed for the split is missing.\\nture are used to make candidates distribute evenly on the\\ndata. Formally, let multi-setDk = {(x1k,h1),(x2k,h2) \u00b7\u00b7\u00b7(xnk,hn)}\\nrepresent the k-th feature values and second order gradient\\nstatistics of each training instances. We can de\ufb01ne a rank\\nfunctions rk : R \u2192[0,+\u221e) as\\nrk(z) = 1\u2211\\n(x,h)\u2208Dk\\nh\\n\u2211\\n(x,h)\u2208Dk,x<z\\nh, (8)\\nwhich represents the proportion of instances whose feature\\nvalue k is smaller than z. The goal is to \ufb01nd candidate split\\npoints {sk1,sk2,\u00b7\u00b7\u00b7skl}, such that\\n|rk(sk,j) \u2212rk(sk,j+1)|<\u03f5, s k1 = min\\ni\\nxik,skl = max\\ni\\nxik.\\n(9)\\nHere \u03f5 is an approximation factor. Intuitively, this means\\nthat there is roughly 1 /\u03f5 candidate points. Here each data\\npoint is weighted byhi. To see why hi represents the weight,\\nwe can rewrite Eq (3) as\\nn\u2211\\ni=1\\n1\\n2hi(ft(xi) \u2212gi/hi)2 + \u2126(ft) + constant,\\nwhich is exactly weighted squared loss with labels gi/hi\\nand weights hi. For large datasets, it is non-trivial to \ufb01nd\\ncandidate splits that satisfy the criteria. When every in-\\nstance has equal weights, an existing algorithm called quan-\\ntile sketch [14, 24] solves the problem. However, there is no\\nexisting quantile sketch for the weighted datasets. There-\\nfore, most existing approximate algorithms either resorted\\nto sorting on a random subset of data which have a chance of\\nfailure or heuristics that do not have theoretical guarantee.\\nTo solve this problem, we introduced a novel distributed\\nweighted quantile sketch algorithm that can handle weighted\\ndata with a provable theoretical guarantee. The general idea\\nis to propose a data structure that supportsmerge and prune\\noperations, with each operation proven to maintain a certain\\naccuracy level. A detailed description of the algorithm as\\nwell as proofs are given in the appendix.\\n3.4 Sparsity-aware Split Finding\\nIn many real-world problems, it is quite common for the\\ninput x to be sparse. There are multiple possible causes\\nfor sparsity: 1) presence of missing values in the data; 2)\\nfrequent zero entries in the statistics; and, 3) artifacts of\\nfeature engineering such as one-hot encoding. It is impor-\\ntant to make the algorithm aware of the sparsity pattern in\\nthe data. In order to do so, we propose to add a default\\ndirection in each tree node, which is shown in Fig. 4. When\\na value is missing in the sparse matrix x, the instance is\\nclassi\ufb01ed into the default direction. There are two choices'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Figure 6: Block structure for parallel learning. Each column in a block is sorted by the corresponding feature\\nvalue. A linear scan over one column in the block is su\ufb03cient to enumerate all the split points.\\nAlgorithm 3: Sparsity-aware Split Finding\\nInput: I, instance set of current node\\nInput: Ik = {i\u2208I|xik \u0338= missing}\\nInput: d, feature dimension\\nAlso applies to the approximate setting, only collect\\nstatistics of non-missing entries into buckets\\ngain\u21900\\nG\u2190\u2211\\ni\u2208I,gi,H \u2190\u2211\\ni\u2208I hi\\nfor k= 1 to m do\\n// enumerate missing value goto right\\nGL \u21900, HL \u21900\\nfor j in sorted(Ik, ascent order byxjk) do\\nGL \u2190GL + gj, HL \u2190HL + hj\\nGR \u2190G\u2212GL, HR \u2190H\u2212HL\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\n// enumerate missing value goto left\\nGR \u21900, HR \u21900\\nfor j in sorted(Ik, descent order byxjk) do\\nGR \u2190GR + gj, HR \u2190HR + hj\\nGL \u2190G\u2212GR, HL \u2190H\u2212HR\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\nend\\nOutput: Split and default directions with max gain\\nof default direction in each branch. The optimal default di-\\nrections are learnt from the data. The algorithm is shown in\\nAlg. 3. The key improvement is to only visit the non-missing\\nentries Ik. The presented algorithm treats the non-presence\\nas a missing value and learns the best direction to handle\\nmissing values. The same algorithm can also be applied\\nwhen the non-presence corresponds to a user speci\ufb01ed value\\nby limiting the enumeration only to consistent solutions.\\nTo the best of our knowledge, most existing tree learn-\\ning algorithms are either only optimized for dense data, or\\nneed speci\ufb01c procedures to handle limited cases such as cat-\\negorical encoding. XGBoost handles all sparsity patterns in\\na uni\ufb01ed way. More importantly, our method exploits the\\nsparsity to make computation complexity linear to number\\nof non-missing entries in the input. Fig. 5 shows the com-\\nparison of sparsity aware and a naive implementation on an\\nAllstate-10K dataset (description of dataset given in Sec. 6).\\nWe \ufb01nd that the sparsity aware algorithm runs 50 times\\nfaster than the naive version. This con\ufb01rms the importance\\nof the sparsity aware algorithm.\\n1 2 4 8 16\\nNumber of Threads\\n0.03125\\n0.0625\\n0.125\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\n16\\n32\\nTime per Tree(sec) Sparsity aware algorithm\\nBasic algorithm\\nFigure 5: Impact of the sparsity aware algorithm\\non Allstate-10K. The dataset is sparse mainly due\\nto one-hot encoding. The sparsity aware algorithm\\nis more than 50 times faster than the naive version\\nthat does not take sparsity into consideration.\\n4. SYSTEM DESIGN\\n4.1 Column Block for Parallel Learning\\nThe most time consuming part of tree learning is to get\\nthe data into sorted order. In order to reduce the cost of\\nsorting, we propose to store the data in in-memory units,\\nwhich we called block. Data in each block is stored in the\\ncompressed column (CSC) format, with each column sorted\\nby the corresponding feature value. This input data layout\\nonly needs to be computed once before training, and can be\\nreused in later iterations.\\nIn the exact greedy algorithm, we store the entire dataset\\nin a single block and run the split search algorithm by lin-\\nearly scanning over the pre-sorted entries. We do the split\\n\ufb01nding of all leaves collectively, so one scan over the block\\nwill collect the statistics of the split candidates in all leaf\\nbranches. Fig. 6 shows how we transform a dataset into the\\nformat and \ufb01nd the optimal split using the block structure.\\nThe block structure also helps when using the approxi-\\nmate algorithms. Multiple blocks can be used in this case,\\nwith each block corresponding to subset of rows in the dataset.\\nDi\ufb00erent blocks can be distributed across machines, or stored\\non disk in the out-of-core setting. Using the sorted struc-\\nture, the quantile \ufb01nding step becomes a linear scan over\\nthe sorted columns. This is especially valuable for local pro-\\nposal algorithms, where candidates are generated frequently\\nat each branch. The binary search in histogram aggregation\\nalso becomes a linear time merge style algorithm.\\nCollecting statistics for each column can be parallelized,\\ngiving us a parallel algorithm for split \ufb01nding. Importantly,\\nthe column block structure also supports column subsam-\\npling, as it is easy to select a subset of columns in a block.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='1 2 4 8 16\\nNumber of Threads\\n8\\n16\\n32\\n64\\n128Time per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm\\n(a) Allstate 10M\\n1 2 4 8 16\\nNumber of Threads\\n8\\n16\\n32\\n64\\n128\\n256Time per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (b) Higgs 10M\\n1 2 4 8 16\\nNumber of Threads\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\nTime per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (c) Allstate 1M\\n1 2 4 8 16\\nNumber of Threads\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\nTime per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (d) Higgs 1M\\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We \ufb01nd that the cache-miss e\ufb00ect\\nimpacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves\\nthe performance by factor of two when the dataset is large.\\nFigure 8: Short range data dependency pattern\\nthat can cause stall due to cache miss.\\nTime Complexity AnalysisLet dbe the maximum depth\\nof the tree and K be total number of trees. For the ex-\\nact greedy algorithm, the time complexity of original spase\\naware algorithm is O(Kd\u2225x\u22250 log n). Here we use \u2225x\u22250 to\\ndenote number of non-missing entries in the training data.\\nOn the other hand, tree boosting on the block structure only\\ncost O(Kd\u2225x\u22250 + \u2225x\u22250 log n). Here O(\u2225x\u22250 log n) is the one\\ntime preprocessing cost that can be amortized. This analysis\\nshows that the block structure helps to save an additional\\nlog n factor, which is signi\ufb01cant when n is large. For the\\napproximate algorithm, the time complexity of original al-\\ngorithm with binary search is O(Kd\u2225x\u22250 log q). Here q is\\nthe number of proposal candidates in the dataset. While q\\nis usually between 32 and 100, the log factor still introduces\\noverhead. Using the block structure, we can reduce the time\\nto O(Kd\u2225x\u22250 + \u2225x\u22250 log B), where B is the maximum num-\\nber of rows in each block. Again we can save the additional\\nlog q factor in computation.\\n4.2 Cache-aware Access\\nWhile the proposed block structure helps optimize the\\ncomputation complexity of split \ufb01nding, the new algorithm\\nrequires indirect fetches of gradient statistics by row index,\\nsince these values are accessed in order of feature. This is\\na non-continuous memory access. A naive implementation\\nof split enumeration introduces immediate read/write de-\\npendency between the accumulation and the non-continuous\\nmemory fetch operation (see Fig. 8). This slows down split\\n\ufb01nding when the gradient statistics do not \ufb01t into CPU cache\\nand cache miss occur.\\nFor the exact greedy algorithm, we can alleviate the prob-\\nlem by a cache-aware prefetching algorithm. Speci\ufb01cally,\\nwe allocate an internal bu\ufb00er in each thread, fetch the gra-\\ndient statistics into it, and then perform accumulation in\\na mini-batch manner. This prefetching changes the direct\\nread/write dependency to a longer dependency and helps to\\nreduce the runtime overhead when number of rows in the\\nis large. Figure 7 gives the comparison of cache-aware vs.\\n1 2 4 8 16\\nNumber of Threads\\n4\\n8\\n16\\n32\\n64\\n128Time per Tree(sec)\\nblock size=2^12\\nblock size=2^16\\nblock size=2^20\\nblock size=2^24\\n(a) Allstate 10M\\n1 2 4 8 16\\nNumber of Threads\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512Time per Tree(sec)\\nblock size=2^12\\nblock size=2^16\\nblock size=2^20\\nblock size=2^24\\n(b) Higgs 10M\\nFigure 9: The impact of block size in the approxi-\\nmate algorithm. We \ufb01nd that overly small blocks re-\\nsults in ine\ufb03cient parallelization, while overly large\\nblocks also slows down training due to cache misses.\\nnon cache-aware algorithm on the the Higgs and the All-\\nstate dataset. We \ufb01nd that cache-aware implementation of\\nthe exact greedy algorithm runs twice as fast as the naive\\nversion when the dataset is large.\\nFor approximate algorithms, we solve the problem by choos-\\ning a correct block size. We de\ufb01ne the block size to be max-\\nimum number of examples in contained in a block, as this\\nre\ufb02ects the cache storage cost of gradient statistics. Choos-\\ning an overly small block size results in small workload for\\neach thread and leads to ine\ufb03cient parallelization. On the\\nother hand, overly large blocks result in cache misses, as the\\ngradient statistics do not \ufb01t into the CPU cache. A good\\nchoice of block size balances these two factors. We compared\\nvarious choices of block size on two data sets. The results\\nare given in Fig. 9. This result validates our discussion and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison of major tree boosting systems.\\nSystem exact\\ngreedy\\napproximate\\nglobal\\napproximate\\nlocal out-of-core sparsity\\naware parallel\\nXGBoost yes yes yes yes yes yes\\npGBRT no no yes no no yes\\nSpark MLLib no yes no no partially yes\\nH2O no yes no no partially yes\\nscikit-learn yes no no no no no\\nR GBM yes no no no partially no\\nshows that choosing 2 16 examples per block balances the\\ncache property and parallelization.\\n4.3 Blocks for Out-of-core Computation\\nOne goal of our system is to fully utilize a machine\u2019s re-\\nsources to achieve scalable learning. Besides processors and\\nmemory, it is important to utilize disk space to handle data\\nthat does not \ufb01t into main memory. To enable out-of-core\\ncomputation, we divide the data into multiple blocks and\\nstore each block on disk. During computation, it is impor-\\ntant to use an independent thread to pre-fetch the block into\\na main memory bu\ufb00er, so computation can happen in con-\\ncurrence with disk reading. However, this does not entirely\\nsolve the problem since the disk reading takes most of the\\ncomputation time. It is important to reduce the overhead\\nand increase the throughput of disk IO. We mainly use two\\ntechniques to improve the out-of-core computation.\\nBlock Compression The \ufb01rst technique we use is block\\ncompression. The block is compressed by columns, and de-\\ncompressed on the \ufb02y by an independent thread when load-\\ning into main memory. This helps to trade some of the\\ncomputation in decompression with the disk reading cost.\\nWe use a general purpose compression algorithm for com-\\npressing the features values. For the row index, we substract\\nthe row index by the begining index of the block and use a\\n16bit integer to store each o\ufb00set. This requires 216 examples\\nper block, which is con\ufb01rmed to be a good setting. In most\\nof the dataset we tested, we achieve roughly a 26% to 29%\\ncompression ratio.\\nBlock Sharding The second technique is to shard the data\\nonto multiple disks in an alternative manner. A pre-fetcher\\nthread is assigned to each disk and fetches the data into an\\nin-memory bu\ufb00er. The training thread then alternatively\\nreads the data from each bu\ufb00er. This helps to increase the\\nthroughput of disk reading when multiple disks are available.\\n5. RELATED WORKS\\nOur system implements gradient boosting [10], which per-\\nforms additive optimization in functional space. Gradient\\ntree boosting has been successfully used in classi\ufb01cation [12],\\nlearning to rank [5], structured prediction [8] as well as other\\n\ufb01elds. XGBoost incorporates a regularized model to prevent\\nover\ufb01tting. This this resembles previous work on regularized\\ngreedy forest [25], but simpli\ufb01es the objective and algorithm\\nfor parallelization. Column sampling is a simple but e\ufb00ective\\ntechnique borrowed from RandomForest [4]. While sparsity-\\naware learning is essential in other types of models such as\\nlinear models [9], few works on tree learning have considered\\nthis topic in a principled way. The algorithm proposed in\\nthis paper is the \ufb01rst uni\ufb01ed approach to handle all kinds of\\nsparsity patterns.\\nThere are several existing works on parallelizing tree learn-\\ning [22, 19]. Most of these algorithms fall into the ap-\\nproximate framework described in this paper. Notably, it\\nis also possible to partition data by columns [23] and ap-\\nply the exact greedy algorithm. This is also supported in\\nour framework, and the techniques such as cache-aware pre-\\nfecthing can be used to bene\ufb01t this type of algorithm. While\\nmost existing works focus on the algorithmic aspect of par-\\nallelization, our work improves in two unexplored system di-\\nrections: out-of-core computation and cache-aware learning.\\nThis gives us insights on how the system and the algorithm\\ncan be jointly optimized and provides an end-to-end system\\nthat can handle large scale problems with very limited com-\\nputing resources. We also summarize the comparison be-\\ntween our system and existing opensource implementations\\nin Table 1.\\nQuantile summary (without weights) is a classical prob-\\nlem in the database community [14, 24]. However, the ap-\\nproximate tree boosting algorithm reveals a more general\\nproblem \u2013 \ufb01nding quantiles on weighted data. To the best\\nof our knowledge, the weighted quantile sketch proposed in\\nthis paper is the \ufb01rst method to solve this problem. The\\nweighted quantile summary is also not speci\ufb01c to the tree\\nlearning and can bene\ufb01t other applications in data science\\nand machine learning in the future.\\n6. END TO END EV ALUATIONS\\n6.1 System Implementation\\nWe implemented XGBoost as an open source package 5.\\nThe package is portable and reusable. It supports various\\nweighted classi\ufb01cation and rank objective functions, as well\\nas user de\ufb01ned objective function. It is available in popular\\nlanguages such as python, R, Julia and integrates naturally\\nwith language native data science pipelines such as scikit-\\nlearn. The distributed version is built on top of the rabit\\nlibrary6 for allreduce. The portability of XGBoost makes it\\navailable in many ecosystems, instead of only being tied to\\na speci\ufb01c platform. The distributed XGBoost runs natively\\non Hadoop, MPI Sun Grid engine. Recently, we also enable\\ndistributed XGBoost on jvm bigdata stacks such as Flink\\nand Spark. The distributed version has also been integrated\\ninto cloud platform Tianchi 7 of Alibaba. We believe that\\nthere will be more integrations in the future.\\n6.2 Dataset and Setup\\n5https://github.com/dmlc/xgboost\\n6https://github.com/dmlc/rabit\\n7https://tianchi.aliyun.com'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Table 2: Dataset used in the Experiments.\\nDataset n m Task\\nAllstate 10 M 4227 Insurance claim classi\ufb01cation\\nHiggs Boson 10 M 28 Event classi\ufb01cation\\nYahoo LTRC 473K 700 Learning to Rank\\nCriteo 1.7 B 67 Click through rate prediction\\nWe used four datasets in our experiments. A summary of\\nthese datasets is given in Table 2. In some of the experi-\\nments, we use a randomly selected subset of the data either\\ndue to slow baselines or to demonstrate the performance of\\nthe algorithm with varying dataset size. We use a su\ufb03x to\\ndenote the size in these cases. For example Allstate-10K\\nmeans a subset of the Allstate dataset with 10K instances.\\nThe \ufb01rst dataset we use is the Allstate insurance claim\\ndataset8. The task is to predict the likelihood and cost of\\nan insurance claim given di\ufb00erent risk factors. In the exper-\\niment, we simpli\ufb01ed the task to only predict the likelihood\\nof an insurance claim. This dataset is used to evaluate the\\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\\nsparse features in this data come from one-hot encoding. We\\nrandomly select 10M instances as training set and use the\\nrest as evaluation set.\\nThe second dataset is the Higgs boson dataset9 from high\\nenergy physics. The data was produced using Monte Carlo\\nsimulations of physics events. It contains 21 kinematic prop-\\nerties measured by the particle detectors in the accelerator.\\nIt also contains seven additional derived physics quantities\\nof the particles. The task is to classify whether an event\\ncorresponds to the Higgs boson. We randomly select 10M\\ninstances as training set and use the rest as evaluation set.\\nThe third dataset is the Yahoo! learning to rank challenge\\ndataset [6], which is one of the most commonly used bench-\\nmarks in learning to rank algorithms. The dataset contains\\n20K web search queries, with each query corresponding to a\\nlist of around 22 documents. The task is to rank the docu-\\nments according to relevance of the query. We use the o\ufb03cial\\ntrain test split in our experiment.\\nThe last dataset is the criteo terabyte click log dataset 10.\\nWe use this dataset to evaluate the scaling property of the\\nsystem in the out-of-core and the distributed settings. The\\ndata contains 13 integer features and 26 ID features of user,\\nitem and advertiser information. Since a tree based model\\nis better at handling continuous features, we preprocess the\\ndata by calculating the statistics of average CTR and count\\nof ID features on the \ufb01rst ten days, replacing the ID fea-\\ntures by the corresponding count statistics during the next\\nten days for training. The training set after preprocessing\\ncontains 1.7 billion instances with 67 features (13 integer, 26\\naverage CTR statistics and 26 counts). The entire dataset\\nis more than one terabyte in LibSVM format.\\nWe use the \ufb01rst three datasets for the single machine par-\\nallel setting, and the last dataset for the distributed and\\nout-of-core settings. All the single machine experiments are\\nconducted on a Dell PowerEdge R420 with two eight-core\\nIntel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If\\nnot speci\ufb01ed, all the experiments are run using all the avail-\\n8https://www.kaggle.com/c/ClaimPredictionChallenge\\n9https://archive.ics.uci.edu/ml/datasets/HIGGS\\n10http://labs.criteo.com/downloads/download-terabyte-\\nclick-logs/\\nTable 3: Comparison of Exact Greedy Methods with\\n500 trees on Higgs-1M data.\\nMethod Time per Tree (sec) Test AUC\\nXGBoost 0.6841 0.8304\\nXGBoost (colsample=0.5) 0.6401 0.8245\\nscikit-learn 28.51 0.8302\\nR.gbm 1.032 0.6224\\n1 2 4 8 16\\nNumber of Threads\\n0.5\\n1\\n2\\n4\\n8\\n16\\n32Time per Tree(sec)\\nXGBoost\\npGBRT\\nFigure 10: Comparison between XGBoost and pG-\\nBRT on Yahoo LTRC dataset.\\nTable 4: Comparison of Learning to Rank with 500\\ntrees on Yahoo! LTRC Dataset\\nMethod Time per Tree (sec) NDCG@10\\nXGBoost 0.826 0.7892\\nXGBoost (colsample=0.5) 0.506 0.7913\\npGBRT [22] 2.576 0.7915\\nable cores in the machine. The machine settings of the dis-\\ntributed and the out-of-core experiments will be described in\\nthe corresponding section. In all the experiments, we boost\\ntrees with a common setting of maximum depth equals 8,\\nshrinkage equals 0.1 and no column subsampling unless ex-\\nplicitly speci\ufb01ed. We can \ufb01nd similar results when we use\\nother settings of maximum depth.\\n6.3 Classi\ufb01cation\\nIn this section, we evaluate the performance of XGBoost\\non a single machine using the exact greedy algorithm on\\nHiggs-1M data, by comparing it against two other commonly\\nused exact greedy tree boosting implementations. Since\\nscikit-learn only handles non-sparse input, we choose the\\ndense Higgs dataset for a fair comparison. We use the 1M\\nsubset to make scikit-learn \ufb01nish running in reasonable time.\\nAmong the methods in comparison, R\u2019s GBM uses a greedy\\napproach that only expands one branch of a tree, which\\nmakes it faster but can result in lower accuracy, while both\\nscikit-learn and XGBoost learn a full tree. The results are\\nshown in Table 3. Both XGBoost and scikit-learn give better\\nperformance than R\u2019s GBM, while XGBoost runs more than\\n10x faster than scikit-learn. In this experiment, we also \ufb01nd\\ncolumn subsamples gives slightly worse performance than\\nusing all the features. This could due to the fact that there\\nare few important features in this dataset and we can bene\ufb01t\\nfrom greedily select from all the features.\\n6.4 Learning to Rank\\nWe next evaluate the performance of XGBoost on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='128 256 512 1024 2048\\nNumber of Training Examples (million)\\n128\\n256\\n512\\n1024\\n2048\\n4096Time per Tree(sec)\\nBasic algorithm\\nBlock compression\\nCompression+shard\\nOut of system file cache\\nstart from this point\\nFigure 11: Comparison of out-of-core methods on\\ndi\ufb00erent subsets of criteo data. The missing data\\npoints are due to out of disk space. We can \ufb01nd\\nthat basic algorithm can only handle 200M exam-\\nples. Adding compression gives 3x speedup, and\\nsharding into two disks gives another 2x speedup.\\nThe system runs out of \ufb01le cache start from 400M\\nexamples. The algorithm really has to rely on disk\\nafter this point. The compression+shard method\\nhas a less dramatic slowdown when running out of\\n\ufb01le cache, and exhibits a linear trend afterwards.\\nlearning to rank problem. We compare against pGBRT [22],\\nthe best previously pubished system on this task. XGBoost\\nruns exact greedy algorithm, while pGBRT only support an\\napproximate algorithm. The results are shown in Table 4\\nand Fig. 10. We \ufb01nd that XGBoost runs faster. Interest-\\ningly, subsampling columns not only reduces running time,\\nand but also gives a bit higher performance for this prob-\\nlem. This could due to the fact that the subsampling helps\\nprevent over\ufb01tting, which is observed by many of the users.\\n6.5 Out-of-core Experiment\\nWe also evaluate our system in the out-of-core setting on\\nthe criteo data. We conducted the experiment on one AWS\\nc3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB\\nRAM). The results are shown in Figure 11. We can \ufb01nd\\nthat compression helps to speed up computation by factor of\\nthree, and sharding into two disks further gives 2x speedup.\\nFor this type of experiment, it is important to use a very\\nlarge dataset to drain the system \ufb01le cache for a real out-\\nof-core setting. This is indeed our setup. We can observe a\\ntransition point when the system runs out of \ufb01le cache. Note\\nthat the transition in the \ufb01nal method is less dramatic. This\\nis due to larger disk throughput and better utilization of\\ncomputation resources. Our \ufb01nal method is able to process\\n1.7 billion examples on a single machine.\\n6.6 Distributed Experiment\\nFinally, we evaluate the system in the distributed setting.\\nWe set up a YARN cluster on EC2 with m3.2xlarge ma-\\nchines, which is a very common choice for clusters. Each\\nmachine contains 8 virtual cores, 30GB of RAM and two\\n80GB SSD local disks. The dataset is stored on AWS S3\\ninstead of HDFS to avoid purchasing persistent storage.\\nWe \ufb01rst compare our system against two production-level\\ndistributed systems: Spark MLLib [18] and H2O 11. We use\\n11www.h2o.ai\\n128 256 512 1024 2048\\nNumber of Training Examples (million)\\n128\\n256\\n512\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768Total Running Time (sec)\\nSpark MLLib\\nH2O\\nXGBoost\\n(a) End-to-end time cost include data loading\\n128 256 512 1024 2048\\nNumber of Training Examples (million)\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096Time per Iteration (sec)\\nSpark MLLib\\nH2O\\nXGBoost\\n(b) Per iteration cost exclude data loading\\nFigure 12: Comparison of di\ufb00erent distributed sys-\\ntems on 32 EC2 nodes for 10 iterations on di\ufb00erent\\nsubset of criteo data. XGBoost runs more 10x than\\nspark per iteration and 2.2x as H2O\u2019s optimized ver-\\nsion (However, H2O is slow in loading the data, get-\\nting worse end-to-end time). Note that spark su\ufb00ers\\nfrom drastic slow down when running out of mem-\\nory. XGBoost runs faster and scales smoothly to\\nthe full 1.7 billion examples with given resources by\\nutilizing out-of-core computation.\\n32 m3.2xlarge machines and test the performance of the sys-\\ntems with various input size. Both of the baseline systems\\nare in-memory analytics frameworks that need to store the\\ndata in RAM, while XGBoost can switch to out-of-core set-\\nting when it runs out of memory. The results are shown\\nin Fig. 12. We can \ufb01nd that XGBoost runs faster than the\\nbaseline systems. More importantly, it is able to take ad-\\nvantage of out-of-core computing and smoothly scale to all\\n1.7 billion examples with the given limited computing re-\\nsources. The baseline systems are only able to handle sub-\\nset of the data with the given resources. This experiment\\nshows the advantage to bring all the system improvement\\ntogether and solve a real-world scale problem. We also eval-\\nuate the scaling property of XGBoost by varying the number\\nof machines. The results are shown in Fig. 13. We can \ufb01nd\\nXGBoost\u2019s performance scales linearly as we add more ma-\\nchines. Importantly, XGBoost is able to handle the entire\\n1.7 billion data with only four machines. This shows the\\nsystem\u2019s potential to handle even larger data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='4 8 16 32\\nNumber of Machines\\n128\\n256\\n512\\n1024\\n2048Time per Iteration (sec)\\nFigure 13: Scaling of XGBoost with di\ufb00erent num-\\nber of machines on criteo full 1.7 billion dataset.\\nUsing more machines results in more \ufb01le cache and\\nmakes the system run faster, causing the trend\\nto be slightly super linear. XGBoost can process\\nthe entire dataset using as little as four machines,\\nand scales smoothly by utilizing more available re-\\nsources.\\n7. CONCLUSION\\nIn this paper, we described the lessons we learnt when\\nbuilding XGBoost, a scalable tree boosting system that is\\nwidely used by data scientists and provides state-of-the-art\\nresults on many problems. We proposed a novel sparsity\\naware algorithm for handling sparse data and a theoretically\\njusti\ufb01ed weighted quantile sketch for approximate learning.\\nOur experience shows that cache access patterns, data com-\\npression and sharding are essential elements for building a\\nscalable end-to-end system for tree boosting. These lessons\\ncan be applied to other machine learning systems as well.\\nBy combining these insights, XGBoost is able to solve real-\\nworld scale problems using a minimal amount of resources.\\nAcknowledgments\\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro,\\nSameer Singh, Arvind Krishnamurthy for their valuable feedback.\\nWe also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan\\nTang, Hongliang Liu, Qiang Kou, Nan Zhu and all other con-\\ntributors in the XGBoost community. This work was supported\\nin part by ONR (PECASE) N000141010672, NSF IIS 1258741\\nand the TerraSwarm Research Center sponsored by MARCO and\\nDARPA.\\n8. REFERENCES\\n[1] R. Bekkerman. The present and the future of the kdd cup\\ncompetition: an outsider\u2019s perspective.\\n[2] R. Bekkerman, M. Bilenko, and J. Langford. Scaling Up\\nMachine Learning: Parallel and Distributed Approaches .\\nCambridge University Press, New York, NY, USA, 2011.\\n[3] J. Bennett and S. Lanning. The net\ufb02ix prize. In\\nProceedings of the KDD Cup Workshop 2007 , pages 3\u20136,\\nNew York, Aug. 2007.\\n[4] L. Breiman. Random forests. Maching Learning,\\n45(1):5\u201332, Oct. 2001.\\n[5] C. Burges. From ranknet to lambdarank to lambdamart:\\nAn overview. Learning, 11:23\u2013581, 2010.\\n[6] O. Chapelle and Y. Chang. Yahoo! Learning to Rank\\nChallenge Overview. Journal of Machine Learning\\nResearch - W & CP , 14:1\u201324, 2011.\\n[7] T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\\nmatrix factorization using gradient boosting. In Proceeding\\nof 30th International Conference on Machine Learning\\n(ICML\u201913), volume 1, pages 436\u2013444, 2013.\\n[8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. E\ufb03cient\\nsecond-order gradient boosting for conditional random\\n\ufb01elds. In Proceeding of 18th Arti\ufb01cial Intelligence and\\nStatistics Conference (AISTATS\u201915), volume 1, 2015.\\n[9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and\\nC.-J. Lin. LIBLINEAR: A library for large linear\\nclassi\ufb01cation. Journal of Machine Learning Research ,\\n9:1871\u20131874, 2008.\\n[10] J. Friedman. Greedy function approximation: a gradient\\nboosting machine. Annals of Statistics , 29(5):1189\u20131232,\\n2001.\\n[11] J. Friedman. Stochastic gradient boosting. Computational\\nStatistics & Data Analysis , 38(4):367\u2013378, 2002.\\n[12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic\\nregression: a statistical view of boosting. Annals of\\nStatistics, 28(2):337\u2013407, 2000.\\n[13] J. H. Friedman and B. E. Popescu. Importance sampled\\nlearning ensembles, 2003.\\n[14] M. Greenwald and S. Khanna. Space-e\ufb03cient online\\ncomputation of quantile summaries. In Proceedings of the\\n2001 ACM SIGMOD International Conference on\\nManagement of Data , pages 58\u201366, 2001.\\n[15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi,\\nA. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela.\\nPractical lessons from predicting clicks on ads at facebook.\\nIn Proceedings of the Eighth International Workshop on\\nData Mining for Online Advertising , ADKDD\u201914, 2014.\\n[16] P. Li. Robust Logitboost and adaptive base class (ABC)\\nLogitboost. In Proceedings of the Twenty-Sixth Conference\\nAnnual Conference on Uncertainty in Arti\ufb01cial Intelligence\\n(UAI\u201910), pages 302\u2013311, 2010.\\n[17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank\\nusing multiple classi\ufb01cation and gradient boosting. In\\nAdvances in Neural Information Processing Systems 20 ,\\npages 897\u2013904. 2008.\\n[18] X. Meng, J. Bradley, B. Yavuz, E. Sparks,\\nS. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde,\\nS. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,\\nM. Zaharia, and A. Talwalkar. MLlib: Machine learning in\\napache spark. Journal of Machine Learning Research ,\\n17(34):1\u20137, 2016.\\n[19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo.\\nPlanet: Massively parallel learning of tree ensembles with\\nmapreduce. Proceeding of VLDB Endowment,\\n2(2):1426\u20131437, Aug. 2009.\\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research, 12:2825\u20132830, 2011.\\n[21] G. Ridgeway. Generalized Boosted Models: A guide to the\\ngbm package.\\n[22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin.\\nParallel boosted regression trees for web search ranking. In\\nProceedings of the 20th international conference on World\\nwide web, pages 387\u2013396. ACM, 2011.\\n[23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic\\ngradient boosted distributed decision trees. In Proceedings\\nof the 18th ACM Conference on Information and\\nKnowledge Management, CIKM \u201909.\\n[24] Q. Zhang and W. Wang. A fast algorithm for approximate\\nquantiles in high speed data streams. In Proceedings of the\\n19th International Conference on Scienti\ufb01c and Statistical\\nDatabase Management, 2007.\\n[25] T. Zhang and R. Johnson. Learning nonlinear functions\\nusing regularized greedy forest. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 36(5), 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='APPENDIX\\nA. WEIGHTED QUANTILE SKETCH\\nIn this section, we introduce the weighted quantile sketch algo-\\nrithm. Approximate answer of quantile queries is for many real-\\nworld applications. One classical approach to this problem is GK\\nalgorithm [14] and extensions based on the GK framework [24].\\nThe main component of these algorithms is a data structure called\\nquantile summary, that is able to answer quantile queries with\\nrelative accuracy of \u03f5. Two operations are de\ufb01ned for a quantile\\nsummary:\\n\u2022 A merge operation that combines two summaries with ap-\\nproximation error \u03f51 and \u03f52 together and create a merged\\nsummary with approximation error max( \u03f51,\u03f52).\\n\u2022 A prune operation that reduces the number of elements in\\nthe summary to b+1 and changes approximation error from\\n\u03f5 to \u03f5+ 1\\nb.\\nA quantile summary with merge and prune operations forms basic\\nbuilding blocks of the distributed and streaming quantile comput-\\ning algorithms [24].\\nIn order to use quantile computation for approximate tree boost-\\ning, we need to \ufb01nd quantiles on weighted data. This more gen-\\neral problem is not supported by any of the existing algorithm. In\\nthis section, we describe a non-trivial weighted quantile summary\\nstructure to solve this problem. Importantly, the new algorithm\\ncontains merge and prune operations with the same guarantee as\\nGK summary. This allows our summary to be plugged into all\\nthe frameworks used GK summary as building block and answer\\nquantile queries over weighted data e\ufb03ciently.\\nA.1 Formalization and De\ufb01nitions\\nGiven an input multi-set D= {(x1,w1),(x2,w2) \u00b7\u00b7\u00b7 (xn,wn)}\\nsuch that wi \u2208[0,+\u221e),xi \u2208X . Each xi corresponds to a po-\\nsition of the point and wi is the weight of the point. Assume\\nwe have a total order < de\ufb01ned on X. Let us de\ufb01ne two rank\\nfunctions r\u2212\\nD,r+\\nD: X\u2192 [0,+\u221e)\\nr\u2212\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x<y\\nw (10)\\nr+\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x\u2264y\\nw (11)\\nWe should note that since Dis de\ufb01ned to be a multiset of the\\npoints. It can contain multiple record with exactly same position\\nx and weight w. We also de\ufb01ne another weight function \u03c9D :\\nX\u2192 [0,+\u221e) as\\n\u03c9D(y) = r+\\nD(y) \u2212r\u2212\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x=y\\nw. (12)\\nFinally, we also de\ufb01ne the weight of multi-set Dto be the sum of\\nweights of all the points in the set\\n\u03c9(D) =\\n\u2211\\n(x,w)\u2208D\\nw (13)\\nOur task is given a series of input D, to estimate r+(y) and r\u2212(y)\\nfor y\u2208X as well as \ufb01nding points with speci\ufb01c rank. Given these\\nnotations, we de\ufb01ne quantile summary of weighted examples as\\nfollows:\\nDefinition A.1. Quantile Summary of Weighted Data\\nA quantile summary for Dis de\ufb01ned to be tuple Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D),\\nwhere S = {x1,x2,\u00b7\u00b7\u00b7 ,xk}is selected from the points in D(i.e.\\nxi \u2208{x|(x,w) \u2208D}) with the following properties:\\n1) xi <xi+1 for all i, and x1 and xk are minimum and max-\\nimum point in D:\\nx1 = min\\n(x,w)\u2208D\\nx, xk = max\\n(x,w)\u2208D\\nx\\n2) \u02dcr+\\nD, \u02dcr\u2212\\nD and \u02dc\u03c9D are functions in S \u2192[0,+\u221e), that satis\ufb01es\\n\u02dcr\u2212\\nD(xi) \u2264r\u2212\\nD(xi), \u02dcr+\\nD(xi) \u2265r+\\nD(xi), \u02dc\u03c9D(xi) \u2264\u03c9D(xi), (14)\\nthe equality sign holds for maximum and minimum point ( \u02dcr\u2212\\nD(xi) =\\nr\u2212\\nD(xi), \u02dcr+\\nD(xi) = r+\\nD(xi) and \u02dc\u03c9D(xi) = \u03c9D(xi) for i\u2208{1,k}).\\nFinally, the function value must also satisfy the following con-\\nstraints\\n\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) \u2264\u02dcr\u2212\\nD(xi+1), \u02dcr+\\nD(xi) \u2264\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n(15)\\nSince these functions are only de\ufb01ned onS, it is su\ufb03ce to use 4k\\nrecord to store the summary. Speci\ufb01cally, we need to remember\\neach xi and the corresponding function values of each xi.\\nDefinition A.2. Extension of Function Domains\\nGiven a quantile summary Q(D) = ( S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) de\ufb01ned in\\nDe\ufb01nition A.1, the domain of \u02dcr+\\nD, \u02dcr\u2212\\nD and \u02dc\u03c9D were de\ufb01ned only\\nin S. We extend the de\ufb01nition of these functions to X\u2192 [0,+\u221e)\\nas follows\\nWhen y <x1:\\n\u02dcr\u2212\\nD(y) = 0, \u02dcr+\\nD(y) = 0, \u02dc\u03c9D(y) = 0 (16)\\nWhen y >xk:\\n\u02dcr\u2212\\nD(y) = \u02dcr+\\nD(xk), \u02dcr+\\nD(y) = \u02dcr+\\nD(xk), \u02dc\u03c9D(y) = 0 (17)\\nWhen y\u2208(xi,xi+1) for some i:\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi),\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1),\\n\u02dc\u03c9D(y) = 0\\n(18)\\nLemma A.1. Extended Constraint\\nThe extended de\ufb01nition of \u02dcr\u2212\\nD, \u02dcr+\\nD, \u02dc\u03c9D satis\ufb01es the following\\nconstraints\\n\u02dcr\u2212\\nD(y) \u2264r\u2212\\nD(y), \u02dcr+\\nD(y) \u2265r+\\nD(y), \u02dc\u03c9D(y) \u2264\u03c9D(y) (19)\\n\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y) \u2264\u02dcr\u2212\\nD(x), \u02dcr+\\nD(y) \u2264\u02dcr+\\nD(x) \u2212\u02dc\u03c9D(x), for all y <x\\n(20)\\nProof. The only non-trivial part is to prove the case when\\ny\u2208(xi,xi+1):\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) \u2264r\u2212\\nD(xi) + \u03c9D(xi) \u2264r\u2212\\nD(y)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2265r+\\nD(xi+1) \u2212\u03c9D(xi+1) \u2265r+\\nD(y)\\nThis proves Eq. (19). Furthermore, we can verify that\\n\u02dcr+\\nD(xi) \u2264\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) = \u02dcr+\\nD(y) \u2212\u02dc\u03c9D(y)\\n\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + 0 \u2264\u02dcr\u2212\\nD(xi+1)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\nUsing these facts and transitivity of < relation, we can prove\\nEq. (20)\\nWe should note that the extension is based on the ground case\\nde\ufb01ned in S, and we do not require extra space to store the sum-\\nmary in order to use the extended de\ufb01nition. We are now ready\\nto introduce the de\ufb01nition of \u03f5-approximate quantile summary.\\nDefinition A.3. \u03f5-Approximate Quantile Summary\\nGiven a quantile summary Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D), we call it is\\n\u03f5-approximate summary if for any y\u2208X\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y) \u2264\u03f5\u03c9(D) (21)\\nWe use this de\ufb01nition since we know that r\u2212(y) \u2208[\u02dcr\u2212\\nD(y),\u02dcr+\\nD(y)\u2212\\n\u02dc\u03c9D(y)] and r+(y) \u2208[\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y),\u02dcr+\\nD(y)]. Eq. (21) means the\\nwe can get estimation of r+(y) and r\u2212(y) by error of at most\\n\u03f5\u03c9(D).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Lemma A.2. Quantile summary Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) is an\\n\u03f5-approximate summary if and only if the following two condition\\nholds\\n\u02dcr+\\nD(xi) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi) \u2264\u03f5\u03c9(D) (22)\\n\u02dcr+\\nD(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dc\u03c9D(xi) \u2264\u03f5\u03c9(D) (23)\\nProof. The key is again consider y\u2208(xi,xi+1)\\n\u02dcr+\\nD(y)\u2212\u02dcr\u2212\\nD(y)\u2212\u02dc\u03c9D(y) = [\u02dcr+\\nD(xi+1)\u2212\u02dc\u03c9D(xi+1)]\u2212[\u02dcr+\\nD(xi)+\u02dc\u03c9D(xi)]\u22120\\nThis means the condition in Eq. (23) plus Eq. (22) can give us\\nEq. (21)\\nProperty of Extended FunctionIn this section, we have in-\\ntroduced the extension of function \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D to X \u2192[0,+\u221e).\\nThe key theme discussed in this section is the relation of con-\\nstraints on the original function and constraints on the extended\\nfunction. Lemma A.1 and A.2 show that the constraints on the\\noriginal function can lead to in more general constraints on the\\nextended function. This is a very useful property which will be\\nused in the proofs in later sections.\\nA.2 Construction of Initial Summary\\nGiven a small multi-set D= {(x1,w1),(x2,w2),\u00b7\u00b7\u00b7 ,(xn,wn)},\\nwe can construct initial summaryQ(D) = {S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D}, with S\\nto the set of all values in D(S = {x|(x,w) \u2208D}), and \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D\\nde\ufb01ned to be\\n\u02dcr+\\nD(x) = r+\\nD(x), \u02dcr\u2212\\nD(x) = r\u2212\\nD(x), \u02dc\u03c9D(x) = \u03c9D(x) for x\u2208S\\n(24)\\nThe constructed summary is 0-approximate summary, since it can\\nanswer all the queries accurately. The constructed summary can\\nbe feed into future operations described in the latter sections.\\nA.3 Merge Operation\\nIn this section, we de\ufb01ne how we can merge the two summaries\\ntogether. Assume we have Q(D1) = ( S1,\u02dcr+\\nD1 ,\u02dcr\u2212\\nD1 ,\u02dc\u03c9D1 ) and\\nQ(D2) = ( S2,\u02dcr+\\nD1 ,\u02dcr\u2212\\nD2 ,\u02dc\u03c9D2 ) quantile summary of two dataset\\nD1 and D2. Let D= D1 \u222aD2, and de\ufb01ne the merged summary\\nQ(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) as follows.\\nS = {x1,x2 \u00b7\u00b7\u00b7 ,xk},xi \u2208S1 or xi \u2208S2 (25)\\nThe points in S are combination of points in S1 and S2. And the\\nfunction \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9Dare de\ufb01ned to be\\n\u02dcr\u2212\\nD(xi) = \u02dcr\u2212\\nD1 (xi) + \u02dcr\u2212\\nD2 (xi) (26)\\n\u02dcr+\\nD(xi) = \u02dcr+\\nD1 (xi) + \u02dcr+\\nD2 (xi) (27)\\n\u02dc\u03c9D(xi) = \u02dc\u03c9D1 (xi) + \u02dc\u03c9D2 (xi) (28)\\nHere we use functions de\ufb01ned on S \u2192[0,+\u221e) on the left sides of\\nequalities and use the extended function de\ufb01nitions on the right\\nsides.\\nDue to additive nature of r+, r\u2212and \u03c9, which can be formally\\nwritten as\\nr\u2212\\nD(y) =r\u2212\\nD1 (y) + r\u2212\\nD2 (y),\\nr+\\nD(y) =r+\\nD1 (y) + r+\\nD2 (y),\\n\u03c9D(y) =\u03c9D1 (y) + \u03c9D2 (y),\\n(29)\\nand the extended constraint property in Lemma A.1, we can verify\\nthat Q(D) satis\ufb01es all the constraints in De\ufb01nition A.1. Therefore\\nit is a valid quantile summary.\\nLemma A.3. The combined quantile summary satis\ufb01es\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y) (30)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD1 (y) + \u02dcr+\\nD2 (y) (31)\\n\u02dc\u03c9D(y) = \u02dc\u03c9D1 (y) + \u02dc\u03c9D2 (y) (32)\\nfor all y\u2208X\\nAlgorithm 4: Query Function g(Q,d)\\nInput: d: 0 \u2264d\u2264\u03c9(D)\\nInput: Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) where\\nS = x1,x2,\u00b7\u00b7\u00b7 ,xk\\nif d< 1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)] then return x1 ;\\nif d\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)] then return xk ;\\nFind i such that\\n1\\n2 [\u02dcr\u2212\\nD(xi) + \u02dcr+\\nD(xi)] \u2264d< 1\\n2 [\u02dcr\u2212\\nD(xi+1) + \u02dcr+\\nD(xi+1)]\\nif 2d< \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) then\\nreturn xi\\nelse\\nreturn xi+1\\nend\\nThis can be obtained by straight-forward application of De\ufb01ni-\\ntion A.2.\\nTheorem A.1. If Q(D1) is \u03f51-approximate summary, and Q(D2)\\nis \u03f52-approximate summary. Then the merged summary Q(D) is\\nmax(\u03f51,\u03f52)-approximate summary.\\nProof. For any y\u2208X, we have\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y)\\n=[\u02dcr+\\nD1 (y) + \u02dcr+\\nD2 (y)] \u2212[\u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y)] \u2212[\u02dc\u03c9D1 (y) + \u02dc\u03c9D2 (y)]\\n\u2264\u03f51\u03c9(D1) + \u03f52\u03c9(D2) \u2264max(\u03f51,\u03f52)\u03c9(D1 \u222aD2)\\nHere the \ufb01rst inequality is due to Lemma A.3.\\nA.4 Prune Operation\\nBefore we start discussing the prune operation, we \ufb01rst in-\\ntroduce a query function g(Q,d). The de\ufb01nition of function is\\nshown in Algorithm 4. For a given rank d, the function returns\\na x whose rank is close to d. This property is formally described\\nin the following Lemma.\\nLemma A.4. For a given \u03f5-approximate summary Q(D) =\\n(S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D), x\u2217= g(Q,d) satis\ufb01es the following property\\nd\u2265\u02dcr+\\nD(x\u2217) \u2212\u02dc\u03c9D(x\u2217) \u2212\u03f5\\n2 \u03c9(D)\\nd\u2264\u02dcr\u2212\\nD(x\u2217) + \u02dc\u03c9D(x\u2217) + \u03f5\\n2 \u03c9(D)\\n(33)\\nProof. We need to discuss four possible cases\\n\u2022 d <1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)] and x\u2217= x1. Note that the rank\\ninformation for x1 is accurate (\u02dc\u03c9D(x1) = \u02dcr+\\nD(x1) = \u03c9(x1),\\n\u02dcr\u2212\\nD(x1) = 0), we have\\nd\u22650 \u2212\u03f5\\n2 \u03c9(D) = \u02dcr+\\nD(x1) \u2212\u02dc\u03c9D(x1) \u2212\u03f5\\n2 \u03c9(D)\\nd< 1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)]\\n\u2264\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)\\n= \u02dcr\u2212\\nD(x1) + \u02dc\u03c9+\\nD(x1)\\n\u2022 d\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)] and x\u2217= xk, then\\nd\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)]\\n= \u02dcr+\\nD(xk) \u22121\\n2 [\u02dcr+\\nD(xk) \u2212\u02dcr\u2212\\nD(xk)]\\n= \u02dcr+\\nD(xk) \u22121\\n2 \u02dc\u03c9D(xk)\\nd<\u03c9 (D) + \u03f5\\n2 \u03c9(D) = \u02dcr\u2212\\nD(xk) + \u02dc\u03c9D(xk) + \u03f5\\n2 \u03c9(D)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='\u2022 x\u2217= xi in the general case, then\\n2d< \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n= 2[\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi)] + [\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi)]\\n\u22642[\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi)] + \u03f5\u03c9(D)\\n2d\u2265\u02dcr\u2212\\nD(xi) + \u02dcr+\\nD(xi)\\n= 2[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi)] \u2212[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi) \u2212\u02dcr\u2212\\nD(xi)] + \u02dc\u03c9D(xi)\\n\u22652[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi)] \u2212\u03f5\u03c9(D) + 0\\n\u2022 x\u2217= xi+1 in the general case\\n2d\u2265\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n= 2[\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)]\\n\u2212[\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi)]\\n\u22652[\u02dcr+\\nD(xi+1) + \u02dc\u03c9D(xi+1)] \u2212\u03f5\u03c9(D)\\n2d\u2264\u02dcr\u2212\\nD(xi+1) + \u02dcr+\\nD(xi+1)\\n= 2[\u02dcr\u2212\\nD(xi+1) + \u02dc\u03c9D(xi+1)]\\n+ [\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi+1)] \u2212\u02dc\u03c9D(xi+1)\\n\u22642[\u02dcr\u2212\\nD(xi+1) + \u02dc\u03c9D(xi+1)] + \u03f5\u03c9(D) \u22120\\nNow we are ready to introduce the prune operation. Given a\\nquantile summaryQ(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) with S = {x1,x2,\u00b7\u00b7\u00b7 ,xk}\\nelements, and a memory budget b. The prune operation creates\\nanother summary Q\u2032(D) = (S\u2032,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) with S\u2032= {x\u2032\\n1,x\u2032\\n2,\u00b7\u00b7\u00b7 ,x\u2032\\nb+1},\\nwhere x\u2032\\ni are selected by query the original summary such that\\nx\u2032\\ni = g\\n(\\nQ,i\u22121\\nb \u03c9(D)\\n)\\n.\\nThe de\ufb01nition of \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D in Q\u2032 is copied from original sum-\\nmary Q, by restricting input domain from S to S\u2032. There could\\nbe duplicated entries in the S\u2032. These duplicated entries can be\\nsafely removed to further reduce the memory cost. Since all the\\nelements in Q\u2032comes from Q, we can verify that Q\u2032satis\ufb01es all\\nthe constraints in De\ufb01nition A.1 and is a valid quantile summary.\\nTheorem A.2. Let Q\u2032(D) be the summary pruned from an\\n\u03f5-approximate quantile summary Q(D) with b memory budget.\\nThen Q\u2032(D) is a (\u03f5+ 1\\nb)-approximate summary.\\nProof. We only need to prove the property in Eq. (23) forQ\u2032.\\nUsing Lemma A.4, we have\\ni\u22121\\nb \u03c9(D) + \u03f5\\n2 \u03c9(D) \u2265\u02dcr+\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\ni\u22121\\nb \u03c9(D) \u2212\u03f5\\n2 \u03c9(D) \u2264\u02dcr\u2212\\nD(x\u2032\\ni) + \u02dc\u03c9D(x\u2032\\ni)\\nCombining these inequalities gives\\n\u02dcr+\\nD(x\u2032\\ni+1) \u2212\u02dc\u03c9D(x\u2032\\ni+1) \u2212\u02dcr\u2212\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\n\u2264[ i\\nb\u03c9(D) + \u03f5\\n2 \u03c9(D)] \u2212[ i\u22121\\nb \u03c9(D) \u2212\u03f5\\n2 \u03c9(D)] = ( 1\\nb + \u03f5)\u03c9(D)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings and Their Use In\\nSentence Classi\ufb01cation Tasks\\nAmit Mandelbaum Adi Shalev\\nHebrew University of Jerusalm\\namit.mandelbaum@mail.huji.ac.il bitan.adi@gmail.com\\nOctober 27, 2016\\nAbstract\\nThis paper has two parts. In the \ufb01rst part we discuss word embeddings. We discuss the need for them, some\\nof the methods to create them, and some of their interesting properties. We also compare them to image\\nembeddings and see how word embedding and image embedding can be combined to perform different tasks.\\nIn the second part we implement a convolutional neural network trained on top of pre-trained word vectors.\\nThe network is used for several sentence-level classi\ufb01cation tasks, and achieves state-of-art (or comparable)\\nresults, demonstrating the great power of pre-trainted word embeddings over random ones.\\nI. I ntroduction\\nT\\nhere are some de\ufb01nitions for what Word\\nEmbeddings are, but in the most general\\nnotion, word embeddings are the numer-\\nical representation of words, usually in a shape\\nof a vector in \u211cd. Being more speci\ufb01c, word\\nembeddings are unsupervisedly learned word\\nrepresentation vectors whose relative similari-\\nties correlate with semantic similarity. In com-\\nputational linguistics they are often referred as\\ndistributional semantic model or distributed repre-\\nsentations.\\nThe theoretical foundations of word embed-\\ndings can be traced back to the early 1950\u2019s\\nand in particular in the works of Zellig Harris,\\nJohn Firth, and Ludwig Wittgenstein. The ear-\\nliest attempts at using feature representations\\nto quantify (semantic) similarity used hand-\\ncrafted features. A good example is the work\\non semantic differentials [Osgood, 1964]. The\\nearly 1990\u2019s saw the rise of automatically gen-\\nerated contextual features, and the rise of Deep\\nLearning methods for Natural Language Pro-\\ncessing (NLP) in the early 2010\u2019s helped to in-\\ncrease their popularity, to the point that, these\\ndays, word embeddings are the most popular\\nresearch area in NLP 1.\\nThis work will be divided into two parts.\\nIn the \ufb01rst part we will discuss the need for\\nword embeddings, some of the methods to\\ncreate them, and some interesting features of\\nthose embeddings. We also compare them to\\nimage embeddings (usually referred as image\\nfeatures) and see how word embedding and\\nimage embedding can be combined to perform\\ndifferent tasks.\\nIn the second part of this paper we will\\npresent our implementation of Convolutional\\nNeural Networks for Sentence Classi\ufb01cation\\n[Kim ,2014]. This work which became very\\npopular is a very good demonstration of the\\npower of pre-trained word embeddings. Us-\\ning a relatively simple model, the authors were\\nable to achieve state-of-art (or comparable) re-\\nsults, for several sentence-level classi\ufb01cation\\ntasks. In this part we will present the model,\\ndiscuss the results and compare them to those\\nof the original article. We will also extend and\\ntest the model on some datasets that were not\\nused in the original article. Finally, we will\\n1In 2015 the dominating subject at EMNLP (\"Empiri-\\ncal Methods in NLP\") conference was word embeddings,\\nsource: http://sebastianruder.com/word-embeddings-1/\\n1\\narXiv:1610.08229v1  [cs.LG]  26 Oct 2016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\npropose some extensions for the model which\\nmight be a good proposition for future work.\\nII. W ord Embeddings\\ni. Motivation\\nIt is obvious that every mathematical system\\nor algorithm needs some sort of numeric input\\nto work with. However, while images and\\naudio naturally come in the form of rich, high-\\ndimensional vectors (i.e. pixel intensity for\\nimages and power spectral density coef\ufb01cients\\nfor audio data), words are treated as discrete\\natomic symbols.\\nThe naive way of converting words to vectors\\nmight assign each word a one-hot vector in\\n\u211c|V| where |V| being vocabulary size. This\\nvector will be all zeros except one unique index\\nfor each word. Representing words in this way\\nleads to substantial data sparsity and usually\\nmeans that we may need more data in order to\\nsuccessfully train statistical models.\\nFigure 1: Density of different data sources.\\nWhat mentioned above raise the need for\\ncontinuous, vector space representations of\\nwords that contain data that can be leveraged\\nby models. To be more speci\ufb01c we want seman-\\ntically similar words to be mapped to nearby\\npoints, thus making the representation carry\\nuseful information about the word actual mean-\\ning.\\nii. Word Embeddings Methods\\nWord embeddings models can be divided into\\nmain categories:\\n\u2022Count-based methods\\n\u2022Predictive methods\\nModels in both categories share, in at least\\nsome way, the assumption that words that ap-\\npear in the same contexts share semantic mean-\\ning.\\nOne of the most in\ufb02uential early works\\nin count-based methods is the LSI/LSA\\n(Latent Semantic Indexing/Analysis)\\n[Deerwester et al., 1990] method. This\\nmethod is based on the Firth\u2019s hypothesisfrom\\n1957 [Firth, 1957] that the meaning of a word\\nis de\ufb01ned \"by the company it keeps\". This\\nhypothesis leads to a very simple albeit a very\\nhigh-dimensional word embedding. Formally,\\neach word can be represented as a vector in \u211cN\\nwhere N is the unique number of words in a\\ngiven dictionary (in practice N=100,000). Then,\\nby taking a very large corpus (e.g. Wikipedia),\\nlet Count 5(w1, w2) be the number of times\\nw1 and w2 occur within a distance 5 of each\\nother in the corpus. Then the word embedding\\nfor a word w is a vector of dimension N,\\nwith one coordinate for each dictionary word.\\nThe coordinate corresponding to word w2 is\\nCount 5(w, w2).\\nThe problem with the resulting embedding\\nis that it uses extremely high-dimensional vec-\\ntors. In the LSA article, is was empirically\\ndiscovered that these embeddings can be re-\\nduced to vectors R300 by doing a rank-300 SVD\\non the NxN original embeddings matrix.\\nThis method was later re\ufb01ned with reweight-\\ning heuristics, such as taking the loga-\\nrithm, or Pointwise Mutual Information (PMI)\\n[Kenneth et al., 1990] on the count, which is a\\nvery popular method.\\nThe second family of methods, sometimes\\nalso referred as neural probabilistic language mod-\\nels, had theoretical and some practical appear-\\nance as early as 1986 [ Hinton, 1986], but \ufb01rst\\nto show the utility of pre-trained word em-\\nbeddings were arguably Collobert and Weston\\nin 2008 [ Collobert and Weston, 2008]. Unlike\\ncount-based models, predictive models try to\\npredict a word from its neighbors in terms of\\nlearned small, dense embedding vectors.\\nTwo of the most popular methods which\\nappeared recently are the Glove (Global\\nVectors for Word Representation) method\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\n[Pennington et. al., 2014 ], which is an unsuper-\\nvised learning method, although not predictive\\nin the common sense, and Word2Vec, a family\\nof energy based predictive models, presented\\nby [Mikolov et. al., 2013 ]. As Word2Vec is the\\nembedding method used in our work it shall\\nbe brie\ufb02y discussed here.\\niii. Word2Vec\\nWord2vec is a particularly computationally-\\nef\ufb01cient predictive model for learning word\\nembeddings from raw text. It comes in two\\n\ufb02avors, the Continuous Bag-of-Words model\\n(CBOW) and the Skip-Gram model. Algo-\\nrithmically, these models are similar, except\\nthat CBOW predicts target words (e.g. \u2019mat\u2019)\\nfrom source context words (\u2019the cat sits on\\nthe\u2019), while the skip-gram does the inverse\\nand predicts source context-words from the\\ntarget words. In the skip-gram model (see \ufb01g-\\nFigure 2: The Skip-gram model architecture\\nure 2) a neural network is trained over a large\\ncorpus in where the training objective is to\\nlearn word vector representations that are good\\nat predicting the nearby words. The method\\nis also using a simpli\ufb01ed version of NCE\\n[Gutmann and Hyv\u00c3d\u2019rinen, 2012] called Neg-\\native sampling where the objective function is\\nde\ufb01ned as follows:\\nlog \u03c3(v\u2032T\\nwO vwI ) +\\nk\\n\u2211\\ni=1\\nEwi\u223cPn (w)[\u03c3(\u2212v\u2032T\\nwi vwI )]\\n(1)\\nwhere v\u2032\\nw and vw are the \"input\" and \"output\"\\nvector representations of w, \u03c3 is the sigmoid\\nfunction but can also be seen as the network\\nparameters function, and Pn is some noise prob-\\nability used to sample random words. In the\\narticle they recommend k to be between 5 to 20,\\nwhile the context of predicted words should be\\n5 or 10. This above objective is later put in the\\nSkip-Gram objective (equtaion 2) to produce\\noptimal word embeddings.\\n1\\nT\\nT\\n\u2211\\nt=1\\n\u2211\\n\u2212c\u2264j\u2264c,j\u0338=0\\nlogp (wt+j|wt) (2)\\nThis objective enables the model to differentiate\\ndata from noise by means of logistic regression,\\nthus learning high-quality vector representa-\\ntions.\\nThe CBOW does exactly the same but the di-\\nrection is inverted. In other words the CBOW\\ntrains a binary logistic classi\ufb01er where, given a\\nwindow of context words, gives a higher proba-\\nbility to \"correct\" if the next word is correct and\\na higher probability to \"incorrect\" if the next\\nword is a random sampled one. Notice that\\nCBOW smoothes over a lot of the distributional\\ninformation (by treating an entire context as\\none observation). For the most part, this turns\\nout to be a useful thing for smaller datasets.\\nHowever, skip-gram treats each context-target\\npair as a new observation, and this tends to do\\nbetter when we have larger datasets.\\nFinally the vector we used in our work had\\na dimension of 300. The Network was trained\\non the Google News dataset which contains 30\\nbillion training words, with negative sampling\\nas mentioned above. These embeddings can be\\nfound online2.\\nA lot of follow-up work was done on the\\nWord2Vec method. One interesting work was\\n2code.google.com/p/word2vec\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 3: Left: Word2Vec t-SNE [Maaten and Hinton, 2008] visualization of our implementation, using text8 dataset\\nand a window size of 5. Only 400 words are visualized. Right: Zooming in of the rectangle in the left \ufb01gure.\\ndone by [ Goldberg and Levy, 2014] where ex-\\nperiments and theory were used to suggest that\\nthese newer methods are related to the older\\nPMI based models, but with new hyperparam-\\neters and/or term reweightings. In this project\\nappendix you can \ufb01nd a simpli\ufb01ed version\\nof Word2Vec we implemented in TensorFlow\\narchitecture using the text8 dataset 3 and the\\nSkip-Gram model. See \ufb01gure 3 for visualized\\nresults.\\niv. Word Embeddings Properties\\nSimilarity: The simplest property of embed-\\ndings obtained by all the methods described\\nabove is that similar words tend to have sim-\\nilar vectors. More formally, the similarity be-\\ntween two words (as rated by humans on a\\n[-1,1] scale) correlates with the cosine similar-\\nity between those words\u2019 vectors. The fact that\\nFigure 4: What words have embeddings closest to a\\ngiven word? From [Collobert et al., 2011]\\nwords embedding are related to their context-\\nwords stand behind the similarity property\\n3http://mattmahoney.net/dc/textdata\\nas naturally, similar words tend to appear in\\nsimilar context. This, however creates the\\nproblem that antonyms (e.g. cold and hot\\netc.) also appear with the same context while\\nthey are, by de\ufb01nition, have opposite mean-\\ning. In [ Mikolov et. al., 2013 ] the score of the\\n(accept,reject) pair is 0.73, and the score of\\n(long,short) is 0.71.\\nThe problem of antonyms was tackled di-\\nrectly by [ Schwartz et al., 2015]. In this arti-\\ncle, the authors introduce a symmetric pattern\\nbased approach to word representation which\\nis particularly suitable for capturing word sim-\\nilarity. Symmetric patterns are a special type\\nof patterns that contain exactly two wildcards\\nand that tend to be instantiated by wildcard\\npairs such that each member of the pair can\\ntake the X or the Y position. For example, the\\nsymmetry of the pattern \"X or Y\" is exempli-\\n\ufb01ed by the semantically plausible expressions\\n\"cats or dogs\" and \"dogs or cats\". Speci\ufb01cally\\nit was found that two patterns are particularly\\nindicative of antonymy - \"from X to Y\" and\\n\"either X or Y\".\\nUsing their model the authors were able\\nto achieve a \u03c1 score of 0.56 on the simlex999\\ndataset [ Hill et al., 2016], improving state-of-\\nthe-art word2vec skip-gram model results by\\nas much as 5.5-16.7%. Furthermore, the au-\\nthors demonstrated the adaptability of their\\nmodel to antonym judgment speci\ufb01cations.\\nLinear analogy relationships: A more\\ninteresting property of recent embeddings\\n[Mikolov et. al., 2013 ] is that they can solve\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nanalogy relationships via linear algebra. This\\nis despite the fact that those embeddings are\\nbeing produced via nonlinear methods. For\\nexample, vqueen is the most similar answer to\\nthe vking \u2212vmen + vwomen equation. It turns out,\\nthough, that much more sophisticated relation-\\nships are also encoded in this way as we can\\nsee in \ufb01gure 5 below.\\nFigure 5: Relationship pairs in a word embedding. From\\n[Mikolov et. al., 2013]\\nAn interesting theoretical work on non-linear\\nembeddings (especially PMI) was done by\\n[Arora et al., 2015]. In their article they sug-\\ngest that the creation of a textual corpus is\\ndriven by the random walk of a discourse vec-\\ntor ct \u2208\u211cd, which is a unit vector whose direc-\\ntion in space represents what is being talked\\nabout. Each word has a (time-invariant) la-\\ntent vector vw \u2208\u211cd that captures its correla-\\ntions with the discourse vector. Using a word\\nproduction model they predict that words oc-\\ncurring at successive time steps will also tend\\nto have vectors that are close together, thus\\nexplaining why similar words have similar vec-\\ntors.\\nUsing the above model the authors introduce\\nthe \"RELATIONS = DIRECTIONS\" notion for\\nlinear analogies. The authors claim that for\\neach relation R, some direction \u00b5R can be found\\nwhich satis\ufb01es some equation. This leads to\\nthe \ufb01nding that given enough examples of a\\nrelationship R, it is possible to compute \u00b5R us-\\ning SVD and then given a pair of words with a\\nrealtion R and a word c, \ufb01nd the best analogy\\nwith word d by \ufb01nding the pair c and d such\\nthat vc \u2212vd has highest possible projection over\\n\u00b5R. In this way, thay also explain that low di-\\nmension of the vectors has a \"purifying\" effect\\nthat reduces the effect of the over\ufb01tting coming\\nfrom the PMI approximation, thus achieving\\nmuch better results than higher dimensional\\nvectors.\\nv. Word Embeddings Extensions\\nIn this last subsection we will review two inter-\\nesting works that extend the word embedding\\nconcept to phrases and sentences using differ-\\nent approaches.\\nIn [ Mitchell and Lapata, 2008] the authors\\naddress the problem that vector-based mod-\\nels are typically directed at representing words\\nin isolation and methods for constructing rep-\\nresentations for phrases or sentences have re-\\nceived little attention in the literature. The\\nauthors suggests the use of two composition\\noperations, multiplication and addition (and\\ntheir combination). This way the authors are\\nable to combine word embeddings into phrase\\nor sentences embeddings while taking into ac-\\ncount important properties like word order\\nand semantic relationship between words (i.e.\\nsemantic composition types).\\nIn MIL (Multi Instance Transfer Learning)\\n[Kotzias et al., 2014] the authors propose a neu-\\nral network model which learns embedding\\nat increasing level of hierarchy, starting from\\nword embeddings, going to sentences and end-\\ning with entire document embeddings. The au-\\nthors then use transfer learning by pulling the\\nsentence or word embedding that were trained\\nas part of the document embeddings and use\\nthem for sentence or word review classi\ufb01cation\\nor similarity tasks (See \ufb01gure 6 below).\\nIII. W ord Embeddings vs. Image\\nEmbeddings\\ni. Image Embeddings\\nImage embeddings, or image features, were\\nwildly used for most image processing and\\nclassi\ufb01cation tasks until the early 2010\u2019s. The\\nfeatures ranged from simple histograms or\\nedge maps to the more sophisticated and\\nvery popular SIFT [ Lowe, 1999] and HOG\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 6: Deep multi-instance transfer learning\\napproach for review data, taken from\\n[Kotzias et al., 2014]\\n[Dalal and Triggs, 2005]. However, recent\\nyears have seen the rise of Deep Learning\\nfor image classi\ufb01cation, especially since 2012\\nwhen the AlexNet [Krizhevsky et al., 2012] ar-\\nticle was published. As those Convolutional\\nNeural Networks (CNN) operated directly on\\nthe images, it was suggested that these net-\\nworks learn the best image features for the spe-\\nci\ufb01c task that they are trained for, thus obviat-\\ning the need for speci\ufb01c hand-crafted features.\\nFigure 7: The CNN architecture of AlexNet\\nIn recent years though, an extensive research\\nwas done on the nature and usage of the ker-\\nnels and features learned by CNN\u2019s. Exten-\\nsive study of CNN feature layers was done\\nin [Zeiler and Fergus, 2014] where they empir-\\nically con\ufb01rmed that each convolutional layer\\nof the CNN learns a set of \ufb01lters. Their ex-\\nperiments also con\ufb01rm that \ufb01lters complexity\\nand expressive power is rising from layer to\\nlayer (i.e. as the network goes deeper) starting\\nfrom simple edge detectors to complex objects\\ndetectors like eyes, \ufb02owers, faces and more.\\nThe authors also suggest using the pre-trained\\none before last layer as a feature map, or Image\\nEmbeddings input for simpler SVM classi\ufb01ers.\\nAnother popular work was done a bit\\nearlier in [ Yangqing et al., 2014] where they\\nalso used a pre-trained CNN features as a\\nbase for visual recognition tasks. This work\\nwas followed by several works with one of\\nthem being considered the philosophical fa-\\nther of the algorithm we implement later. In\\n[Razavian et al., 2014] the authors used the\\none before last layer of a network similar to\\nAlexNet that was pre-trained on ImageNet\\n[Russakovsky et al., 2015] as image embed-\\ndings. The authors were able to acheive state-\\nof-art results on several recognition tasks, us-\\ning simple classi\ufb01ers like SVM. The result was\\nsurprising due to the fact that the CNN model\\nwas originally optimized for the task of object\\nclassi\ufb01cation in ILSVRC 2013 dataset. Never-\\ntheless, it showed itself to be a strong competi-\\ntor to the more sophisticated and highly tuned\\nstate-of-the-art methods.\\nThese works and others suggested that given\\na large enough database of images, a CNN can\\nlearn an image embedding which captures the\\n\"essence\" of the picture and can be used later\\nas an input to different tasks, similar to what\\nis done with word embeddings.\\nii. Similarities and Differences\\nAs we saw earlier Word embedding and Im-\\nage embeddings are similar in the sense that\\nwhile they are being learned as part of a spe-\\nci\ufb01c task, they can be successfully used later\\nfor a variety of other tasks. Also, in both cases,\\nsimilar images or words will usually have sim-\\nilar embeddings. However Word embeddings\\nand image embeddings differ in some aspects.\\nThe \ufb01rst difference is that while word embed-\\ndings depends mostly on the words surround-\\ning the given word, image embeddings usually\\nrely on the speci\ufb01c image itself. This might ex-\\nplain the fact that linear analogies does not ap-\\npear naturally in images. An interesting work\\nwas done in [Reed et al., 2015] where a neural\\nnetwork is trained to make visual analogies\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nand learn to make them based on appearance,\\nrotation, 3D pose, and various object attributes.\\nAnother difference is that while word em-\\nbeddings are usually low-ranked, image em-\\nbeddings might have same or even higher di-\\nmension then the original image. Those em-\\nbeddings are still useful as they contain a lot\\nof information that is extracted from the image\\nand can be used easily.\\nLastly, we notice that word embeddings are\\ntrained on a speci\ufb01c corpus where the \ufb01nal\\nembedding results come as the form of word-\\nvectors. This limits the embedding to be valid\\nonly for words that were found in the original\\ncorpus while other words will need to be ini-\\ntialized as random vectors (as also done in our\\nwork). In images on the other hand, the em-\\nbeddings come as a pre-trained model where\\nfeatures or embeddings can be pulled for any\\nsort of image by feeding the model with the\\nimage, making image embeddings models a\\nbit more robust (although they might subjected\\nto other constraints like size and image type).\\niii. Joint Word-Image Embeddings\\nTo conclude this part we will review some\\nof the recent work done in the exciting area\\nof joint word-image embeddings. The \ufb01rst\\nimmediate usage of joint word-image embed-\\ndings is image annotations or image label-\\ning. An early notable work was done by\\n[Weston, et al., 2010] where a representation\\nof images and representation of annotations\\nwhere both mapped to a joint feature space by\\nlearning a mapping which optimizes top-of-\\nthe-list ranking measures for images and an-\\nnotations. This method, however, learns linear\\nmappings from image features to the embed-\\nding space, and the available labels were only\\nthose provided in the image training set. It\\ncould thus not generalize to new classes.\\nIn 2014 DeVise (A Deep Visual-Semantic\\nEmbedding Model) model was shown by\\n[Frome et al., 2013]. This work which con-\\ntinued earlier work [ Socher et al., 2013 ], com-\\nbined image embedding and word embedding\\ntrained separately into joint similarity metric\\n(see \ufb01gure 8). This enabled them to give perfor-\\nmance comparable to a state-of-the-art softmax\\nbased model on a \ufb02at object classi\ufb01cation met-\\nric, while simultaneously making more seman-\\ntically reasonable errors. Their model was also\\nable to make correct predictions across thou-\\nsands of previously unseen classes by lever-\\naging semantic knowledge elicited only from\\nun-annotated text.\\nAnother line of works which combines im-\\nage and words embeddings is the image cap-\\ntioning area. In this area the embeddings are\\nusually not combined into a joint space but\\nrather used together to create captions for im-\\nages. In [Karpathy and Fei Fei, 2015 ] an image\\nFigure 8: : (a) Left: a visual object categorization network with a softmax output layer; Right: a skip-gram language\\nmodel; Center: the joint model, which is initialized with parameters pre-trained at the lower layers of the\\nother two models. (b) t-SNE visualization [19] of a subset of the ILSVRC 2012 1K label embeddings learned\\nusing skip-gram. Taken from [Frome et al., 2013]\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 9: Image captiones generated with Deep Visual-Semantic model Taken from [Karpathy and Fei Fei, 2015]\\nfeatures pulled from a pre-trained CNN are\\nfed into a Recurrent Neural Network (RNN)\\nwhich uses word embeddings in order to gen-\\nerate a captioning for the image, based on the\\nimage features and previous words (see \ufb01g-\\nure 9). This sort of combination appears in\\nmost image captioning works or video action\\nrecognition tasks.\\nFinally, a slightly more sophisticated method\\ncombining RNN\u2019s and Fisher Vectors can be\\nfound in [ Lev et al., 2015] where the authors\\nwere able to achieve state-of-art results on both\\nimage captioning and video action recognition\\ntasks, using transfer learning on the embed-\\ndings learned for the image captioning tasks.\\nIV . CNN for Sentence\\nClassification Model\\nIn this section and the following we are going\\nto represent our implementation of The Convo-\\nlutional Neural Networks for Sentence Classi\ufb01-\\ncation model [Kim ,2014] and our results. This\\nmodel gained much popularity since it was\\n\ufb01rst introduced in late 2014, mainly because it\\nprovides a very strong demonstration for the\\npower of pre-trained word embeddings.\\nThe model and results were examined in de-\\ntail in [ Zhang and Wallace, 2015] where they\\ntest many types of con\ufb01gurations for the model,\\nincluding different sizes and number of \ufb01lters,\\ndifferent activation units and different word\\nembbeddings.\\nA partial implementation of the model was\\ndone in Theano framework by the authors 4\\nand another simpli\ufb01ed version of the model\\nwas done in TensorFlow 5. In our work we\\nused small parts of the mentioned codes, how-\\never most of the code had to be re-written and\\nexpanded in order to perform a true implmen-\\ntation of the article\u2019s model.\\ni. Model details\\nThe model architecture, shown in \ufb01gure 10,\\nis a slight variant of the CNN architecture of\\n[Collobert et al., 2011]. Formally, let xi \u2208\u211ck\\nbe the k-dimensional word vector correspond-\\ning to the i-th word in the sentence. Let n be\\nthe length (in number of words) of the longest\\nsentence in the dataset, and let lh be the width\\nof the widest \ufb01lter in the network. Then, the\\ninput to the network is a k \u00d7(n + lh \u22121) ma-\\ntrix, which is a concatenation of the word em-\\nbeddings vectors of each sentences, padded\\nby lh \u22121 zero vectors in the beginning and\\nsome more zero vectors in the end so there are\\nn + lh \u22121 vectors eventually.\\nThe input of the network is convolved with\\n\ufb01lters of different widths (i.e. number of words\\nin the window) and different sizes (i.e. number\\nof features). For example, a feature ci is gen-\\nerated from a window of words xi:i+h\u22121 by a\\n\ufb01lter with width h is:\\nci = f (wxi:i+h\u22121 + b) (3)\\nwhere w are the \ufb01lter weights, b is a bias term\\n4https://github.com/yoonkim/CNN_sentence\\n5https://github.com/dennybritz/cnn-text-\\nclassi\ufb01cation-tf\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 10: Model architecture with two channels for an example sentence. Taken from [Kim ,2014]\\nand f is a non-linear function like ReLU. This\\nprocess is done for all \ufb01lters and for all words\\nto create a number of feature maps for each\\n\ufb01lter. Next, those features maps are then max-\\npooled (so we can deal with different sentence\\nsizes) and \ufb01nally connected to a soft-max clas-\\nsi\ufb01cation layer.\\nFor regularization we employ dropout\\n[Hinton et al., 2014] on the penultimate layer.\\nThis entails randomly (under some probabil-\\nity) setting values in the weight vector to\\n0. In the original article they also employed\\nconstraint on l2 norms of this layer, however\\n[Zhang and Wallace, 2015] found that it had\\nnegligible contribution to results and therefore\\nwas not used here.\\nTraining of the network is done by minimiz-\\ning the corss-entropy loss between predicted\\nlabels (soft-max layer) and correct ones. The\\nparameters to be estimated include the weight\\nvector(s), of the \ufb01lter(s), the bias term in the ac-\\ntivation function, the weight vector of the soft-\\nmax function and (optionally) the word embed-\\ndings. Optimization is performed using SGD\\n[Rumelhart et al., 1988] and back-propagation,\\nwith a small mini-batch size speci\ufb01ed later.\\nV . D atasets\\nWe test our model on various benchmarks.\\nSome of them used in the original article while\\nothers are extension we do to the original work.\\nYou can see the dataset statistics summary in\\ntable 1 below.\\n\u2022 MR: Movie reviews with one sentence\\nper review. Classi\ufb01cation involves\\ndetecting positive/negative reviews\\n[Pang and Lee, 2005]6\\n\u2022 SST-1: Stanford Sentiment Treebank-an\\nextension of MR but with train/dev/test\\nsplits provided and \ufb01ne-grained la-\\nbels (very positive, positive, neutral,\\nnegative, very negative), re-labeled by\\n[Socher et al., 2013] 7\\n\u2022 SST-2: Same as SST-1 but with neutral\\nreviews removed and binary labels.\\n\u2022 Subj: Subjectivity dataset where the task\\nis to classify a sentence as being subjective\\nor objective [Pang and Lee, 2004].\\n\u2022 TREC: TREC question dataset-task\\ninvolves classifying a question into 6 ques-\\ntion types (whether the question is about\\nperson, location, numeric information,\\n6https://www.cs.cornell.edu/people/pabo/movie-\\nreview-data/\\n7http://nlp.stanford.edu/sentiment/ Data is actually\\nprovided at the phrase-level and hence we train the model\\non both phrases and sentences but only score on sentences\\nat test time, as in [ Socher et al., 2013 ] Thus the training set\\nis an order of magnitude larger than listed in table 1.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\netc.) [Li and Roth, 2002] 8.\\n\u2022Irony: [Wallace et al., 2014] This contains\\n16,006 sentences from reddit labeled as\\nironic (or not). The dataset is imbalanced\\n(relatively few sentences are ironic).\\nThus before training, we under-sampled\\nnegative instances to make classes sizes\\nequal. This dataset was not used in\\nthe original article but was tested in\\n[Zhang and Wallace, 2015].\\n\u2022Opi: Opinions dataset, which comprises\\nsentences extracted from user reviews\\non a given topic, e.g. \"sound quality of\\nipod nano\". There are 51 such topics\\nand each topic contains approximately\\n100 sentences. The test is to classify\\nwhich opinion belongs to which topic\\n[Ganesan et al., 2010]. This dataset was\\nnot used in the original article but was\\ntested in [Zhang and Wallace, 2015].\\n\u2022Tweet: Tweets from 10 different authors.\\nClassi\ufb01cation involves classifying which\\nTweet belongs to which author 9. This\\ndataset was not used in the original article.\\n\u2022Polite: Sentnces taken Wikipedia editors\u2019\\nlogs which have 25 ranges of politeness\\n[Danescu-Niculescu-Mizil et al., 2013].\\nWe narrowed it to 2 binary classes (po-\\nlite/inpolite). This dataset was not used\\nin the original article.\\nVI. E xperimental Setup\\ni. Hyperparameters and Training\\nIn our implementation of the model we experi-\\nmented with a lof of different con\ufb01gurations.\\nEventually, since results differences were mi-\\nnor, we decided to use the same architecture\\nand parameters mentioned in the original ar-\\nticle for all experiments, with some changes\\n8http://cogcomp.cs.illinois.edu/Data/QA/QC/\\n9http://www.cs.huji.ac.il/ nogazas/pages/projects.html,\\nThanks to Noga Zaslavsky\\nData c l N |V| |Vpre | Test\\nMR 2 20 10662 18765 16488 CV\\nSST-1 5 18 11855 17836 16262 2210\\nSST-2 2 19 9613 16185 14838 1821\\nSubj 2 23 10000 21323 17913 CV\\nTREC 6 10 5952 9592 9125 500\\nIrony 2 75 1074 6138 5718 CV\\nOpi 51 38 7086 7310 6538 CV\\nTweet 10 39 25552 33438 17023 5964\\nPolite 2 53 4353 10135 7951 CV\\nTable 1: Summary statistics for the datasets after tok-\\nenization. c: Number of target classes. l: Av-\\nerage sentence length. N: Dataset size. |V|:\\nVocabulary size. | Vpre|: Number of words\\npresent in the set of pre-trained word vectors.\\nTest: Test set size (CV means there was no stan-\\ndard train/test split and thus 10-fold CV was\\nused).\\nmentioned below. Below is a list of parame-\\nters and speci\ufb01cations that were used for all\\nexperiments:\\n\u2022 Word embeddings: We used the pre-\\ntrained Word2Vec [ Mikolov et. al., 2013 ]\\nmentioned earlier. Each word embedding\\nis in \u211c300. For words that are not found in\\nWord2Vec we randomly initialized them\\nwith a uniform distribution in the range\\nof [-0.5,0.5].\\n\u2022 Filters: We used \ufb01lters with window sizes\\nof [3,4,5] with 100 features each. For\\nactivation we used ReLU.\\n\u2022 Dropout rate: 0.5.\\n\u2022 Mini-Batch size: 50.\\n\u2022 Optimizer: While AdaDelte optimizer\\n[Zeiler, 2012] was used in the original\\narticle. We decided to use the more recent\\nADAM optimizer [Kingma and Ba, 2014],\\nas it seemed to converge much faster (i.e.\\nneeded less epochs on training) and in\\nsome cases improved the results.\\n\u2022 Learning rate: 0.001. We lower it to\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nMR SST-1 SST-2 Subj TREC\\nModel Orig Ours Orig Ours Orig Ours Orig Ours Orig Ours\\nCNN-rand 76.1 76.4 45.0 41 82.7 80.2 89.6 91.1 91.2 97.6\\nCNN-static 81.0 80.3 45.5 48.1 86.8 85.4 93.0 92.5 92.8 98.2\\nCNN-non-static 81.5 80.5 48.0 47.3 87.2 85.8 93.4 93 93.6 98.6\\nTable 2: Results on datasets that were tested in [Kim ,2014] (Orig above)\\n.\\n0.0005 after 8 epchs and to 0.00005 after\\n16 epochs. Notice that the original article\\ndidn\u2019t mention the learning rate.\\n\u2022Number of epochs: This was also not\\nmentioned in the original article but\\ncan be found in the authors code 10. We\\nused 25 epochs for the static version (see\\nModel Variations below). For non static\\nwe used either 4 (MR, SST1, SST2, Subj),\\n10 (Polite), 16 (Twitter, Opi), or 25 (TREC).\\nFor the random version we used 25 except\\nfor Tweet where we used 10, and MR and\\nSST-1 where we used 4.\\n\u2022l2-loss: We added l2-loss with \u03bb = 0.15\\non the weights and biases of the \ufb01nal layer.\\nAltough this was not done in the original\\nartcle, we found it to slightly improve the\\nresults.\\nAs mentioned earlier, we decided not to use\\nl2 constrains on the norms due to their negligi-\\nble contribution.\\nii. Model Variations\\nWe experiment with several variants of the\\nmodel like in the original article.\\n\u2022CNN-rand: Our baseline model where all\\nwords are randomly initialized and then\\nmodi\ufb01ed during training.\\n10We note here that in the original article they used early\\nstopping with a dev set. However, the early stopping\\nparameters are not mentioned and experiments demanded\\na lot of coding which is behind the scope of this project.\\nWe do assume that the 25 number used in the code might\\nbe close enough to the actual number used in the article.\\n\u2022 CNN-static: model with pre-trained vec-\\ntors from word2vec. All words-including\\nthe unknown ones that are randomly\\ninitialized-are kept static and only the\\nother parameters of the model are learned.\\n\u2022 CNN-non-static: Same as above but the\\npretrained vectors are \ufb01ne-tuned for each\\ntask.\\nThe authors also used a multi-channel model\\nwhere one channel is static and the other is not.\\nHowever, experiments showed that on most\\ndatasets, this did not improve the results. As\\nimplementing this would have required a lot\\nmore coding, we decided to drop it.\\nVII. R esults and discussion\\nIn this section we will compare the results we\\ngot in our implementation to the ones achieved\\nin the original article. Full results can be found\\nin the original article, and we do note that\\nmost of them are state-of-art results, or com-\\nparable. For datasets that were not present in\\nthe original article we shall compare with other\\nachieved results, whether ours or others\u2019.\\nIn table 2 above we can see a comparison\\nbetween our results and the ones in the origi-\\nnal article [Kim ,2014]. We can see that overall,\\nour results are comparable (and sometimes bet-\\nter) to the ones in the original article. We also\\nsee that like in the original article, our base-\\nline model with all randomly initialized words\\n(CNN-rand) does not perform well on its own\\n(on most cases). These results suggest that\\nthe pre-trained vectors are good, \u2019universal\u2019\\nfeature extractors and can be utilized across\\ndatasets. Finetuning the pre-trained vectors\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nModel Opi Irony Tweet Polite\\nRandom 64.8 60.2 89.1 66.2\\nStatic 65.3 60.5 83.8 66.2\\nNon-Static 66.4 62.1 89.2 65.7\\nConvSent 64.9 67.112 - -\\nSVM+TF-IDF - - 92.5 -\\nTable 3: Resutls for datasets that were not used\\nin the original article. Convesent is\\n[Zhang and Wallace, 2015].\\nfor each task gives still further improvements\\n(CNN-non-static).\\nThe differences in some of our results can\\nbe related to the different optimizer we used,\\nand the fact that we did not use early stopping.\\nWe do note that our results (at least on the\\nnon-static version) were achieved with much\\nless training than the original article11. We also\\nnote that on the TREC dataset we were able to\\nachieve a new state-of-art results, improving\\nthe current one ( 95%) by 3.6%. Both these\\nbene\ufb01ts can be related to the use of ADAM\\n[Kingma and Ba, 2014] optimizer.\\nOn table 3 we can see our results for datasets\\nthat were not used in the original article. We\\nalso compare them to other results where ap-\\nplicable.\\nOn the Opi and Irony dataset we note that\\nthe general line of improved results with pre-\\ntrained vectors is maintained. On the Opi\\ndataset we were also able to achieve a new\\nstate-of-art result. We were also able to achieve\\ncomparable results on the Irony dataset. No-\\ntice that the other reported result is AUC and\\nnot accuracy.\\nThe other two results are interesting. On the\\nTweet dataset we notice that random vectors\\nactually perform a lot better than pre-trained\\nstatic ones. The reason is that on this dataset,\\nalmost half of the vocabulary was not found in\\nthe Word2Vec embeddings. This makes sense,\\nas tweets usually contain a lot of marks (for\\nexample :-) ) and hashtags which would natu-\\nrally will not be available in embeddings that\\n11That, if we take the 25 epochs in the code we men-\\ntioned earlier as an indication to the nubmer of epochs\\ntraining used in the original article\\nwere trained on news. This makes the static\\nversion a bad choice as it keep the embeddings\\nrandom during training.\\nOn this dataset we also applied a simple\\nSVM classi\ufb01er on the TF-IDF features of each\\ntweet. This simple classi\ufb01er produced much\\nbetter results, as TF-IDF features are sensitive\\nto uniqe words in a tweet (like hashtags), that\\nusually indicates which is the author, thus mak-\\ning classi\ufb01cation easier.\\nOn the Poilte dataset we notice that results\\ndoes not matter on the choice of model. The\\nresults themselves are also not very good. This\\nresults needs further inspection but they might\\nsuggests that this model is not \ufb01tted for this\\ntask or that politeness is a complicated task for\\nautomatic classi\ufb01cation.\\nVIII. C onclusions andFuture\\nDirections\\nIn this work we reviewed word embeddings.\\nWe saw their origins, discussed the different\\nmethods for creating them, and saw some of\\ntheir interesting properties. We think that word\\nembeddings is a very exciting topic for both\\nresearch and applications, and expect that a\\nlot research is going to be carried for better\\nunderstanding of their properties and better\\ncreation methods.\\nIn this work we also compared Image fea-\\ntures and word embeddings and saw how they\\ncan be combined to build learning algorithms\\nthat can \ufb01nally gain a good understanding of\\npictures and scenes. This area is just in its be-\\nginning and we expect a lot of work to be car-\\nried towards creating a hybrid system which\\ngains understanding of both vision and lan-\\nguage, and that combines those understand-\\nings together to the bene\ufb01t of both \ufb01elds.\\nFinally, we saw that despite little tuning of\\nhyperparameters, a simple CNN with one layer\\nof convolution, trained on top of Word2Vec\\nembeddings, performs remarkably well on sen-\\ntence classi\ufb01cation tasks. These results add\\nto the well-established evidence that unsuper-\\nvised pre-training of word vectors is an impor-\\ntant ingredient in deep learning for NLP .\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nTo conclude this work we propose here two\\nlines for future work that we think might be\\ninteresting to check. First, in the spirit of\\n[Kotzias et al., 2014], we notice that in our net-\\nwork, the one before last layer is actually learn-\\ning sentence embeddings. It might be interest-\\ning to train the network on some classi\ufb01cation\\ntask with a relatively large dataset, and then\\nuse the learned sentence embeddings in the\\nsame fashion word embeddings are used in\\nour work. For example we can train the net-\\nwork on the MR task and then take the learned\\nsentence embeddings and use them as an em-\\nbedding input to some document classi\ufb01cation\\ntask. We can then check if this method achieves\\nimprovement over models that try to classify\\ndocuments using only pre-trained word em-\\nbeddings.\\nThe second line of research is in the spirit of\\n[Zeiler and Fergus, 2014]. ConvNets visualiza-\\ntion helped to gain a lot of insights about image\\nsturctre and how features in increasing level\\nof complexity are combined to create images.\\nIt might be interesting to apply those same\\nmethod of visualization to the \ufb01lters used in\\nour, or similar works and see if the ConvNet\\n\ufb01lters learn some interesting semantic proper-\\nties or compositions that can give insights on\\nthe structure of language and how computers\\n(or even humans) percept them.\\nReferences\\n[Arora et al., 2015] Arora, Sanjeev, Yuanzhi Li,\\nYingyu Liang, Tengyu Ma, and Andrej\\nRisteski. \"Rand-walk: A latent variable\\nmodel approach to word embeddings.\"\\narXiv preprint arXiv:1502.03520 (2015).\\n[Collobert and Weston, 2008] Collobert, Ro-\\nnan, and Jason Weston. \"A uni\ufb01ed ar-\\nchitecture for natural language process-\\ning: Deep neural networks with multitask\\nlearning.\" Proceedings of the 25th inter-\\nnational conference on Machine learning.\\nACM. (2008).\\n[Collobert et al., 2011] Collobert, Ronan, Jason\\nWeston, L\u00c3l\u2019on Bottou, Michael Karlen,\\nKoray Kavukcuoglu, and Pavel Kuksa.\\n\"Natural language processing (almost)\\nfrom scratch.\" Journal of Machine Learn-\\ning Research 12, no. Aug (2011): 2493-\\n2537.\\n[Dalal and Triggs, 2005] Dalal, Navneet, and\\nBill Triggs. \"Histograms of oriented gra-\\ndients for human detection.\" 2005 IEEE\\nComputer Society Conference on Com-\\nputer Vision and Pattern Recognition\\n(CVPR\u201905). Vol. 1. IEEE, 2005.\\n[Danescu-Niculescu-Mizil et al., 2013]\\nDanescu-Niculescu-Mizil, Cristian,\\nMoritz Sudhof, Dan Jurafsky, Jure\\nLeskovec, and Christopher Potts. \"A\\ncomputational approach to politeness\\nwith application to social factors.\" arXiv\\npreprint arXiv:1306.6078 (2013).\\n[Deerwester et al., 1990] Deerwester, Scott, Su-\\nsan T. Dumais, George W. Furnas, Thomas\\nK. Landauer, and Richard Harshman. \"In-\\ndexing by latent semantic analysis.\" Jour-\\nnal of the American society for informa-\\ntion science 41, no. 6 (1990): 391.\\n[Firth, 1957] Firth, J.R. (1957). \"A synopsis of\\nlinguistic theory 1930-1955\". Studies in\\nLinguistic Analysis (Oxford: Philological\\nSociety): 1-32. Reprinted in F.R. Palmer,\\ned. (1968). Selected Papers of J.R. Firth\\n1952-1959. London: Longman.\\n[Frome et al., 2013] Frome, Andrea, Greg S.\\nCorrado, Jon Shlens, Samy Bengio, Jeff\\nDean, and Tomas Mikolov. \"Devise: A\\ndeep visual-semantic embedding model.\"\\nIn Advances in neural information pro-\\ncessing systems, pp. 2121-2129. 2013.\\n[Ganesan et al., 2010] Ganesan, Kavita,\\nChengXiang Zhai, and Jiawei Han.\\n\"Opinosis: a graph-based approach to\\nabstractive summarization of highly\\nredundant opinions.\" Proceedings of\\nthe 23rd international conference on\\ncomputational linguistics. Association for\\nComputational Linguistics, 2010.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\n[Goldberg and Levy, 2014] Goldberg, Yoav,\\nand Omer Levy. \"word2vec Explained: de-\\nriving Mikolov et al.\u2019s negative-sampling\\nword-embedding method.\" arXiv preprint\\narXiv:1402.3722 (2014).\\n[Gutmann and Hyv\u00c3d\u2019rinen, 2012] Gutmann,\\nMichael U., and Aapo Hyv\u00c3d\u2019rinen.\\n\"Noise-contrastive estimation of un-\\nnormalized statistical models, with\\napplications to natural image statistics.\"\\nJournal of Machine Learning Research\\n13.Feb (2012): 307-361.\\n[Hill et al., 2016] Hill, Felix, Roi Reichart, and\\nAnna Korhonen. \"Simlex-999: Evaluating\\nsemantic models with (genuine) similar-\\nity estimation.\" Computational Linguistics\\n(2016).\\n[Hinton, 1986] Hinton, Geoffrey E. \"Dis-\\ntributed representations.\" Parallel Dis-\\ntributed Processing: Explorations in the\\nMicrostructure of Cognition (1986).\\n[Hinton et al., 2014] Srivastava, Nitish, Geof-\\nfrey E. Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov.\\n\"Dropout: a simple way to prevent neural\\nnetworks from over\ufb01tting.\" Journal of Ma-\\nchine Learning Research 15, no. 1 (2014):\\n1929-1958.\\n[Karpathy and Fei Fei, 2015] Karpathy, An-\\ndrej, and Li Fei-Fei. \"Deep visual-semantic\\nalignments for generating image de-\\nscriptions.\" Proceedings of the IEEE\\nConference on Computer Vision and\\nPattern Recognition. 2015.\\n[Kenneth et al., 1990] Church, Kenneth Ward,\\nand Patrick Hanks. \"Word association\\nnorms, mutual information, and lexi-\\ncography.\" Computational linguistics 16.1\\n(1990): 22-29.\\n[Kim ,2014] Kim, Yoon. \"Convolutional neu-\\nral networks for sentence classi\ufb01cation.\"\\narXiv preprint arXiv:1408.5882 (2014).\\n[Kingma and Ba, 2014] Kingma, Diederik,\\nand Jimmy Ba. \"Adam: A method for\\nstochastic optimization.\" arXiv preprint\\narXiv:1412.6980 (2014).\\n[Kotzias et al., 2014] Kotzias, Dimitrios, Misha\\nDenil, Phil Blunsom, and Nando de\\nFreitas. \"Deep multi-instance transfer\\nlearning.\" arXiv preprint arXiv:1411.3128\\n(2014).\\n[Krizhevsky et al., 2012] Krizhevsky, Alex,\\nIlya Sutskever, and Geoffrey E. Hinton.\\n\"Imagenet classi\ufb01cation with deep convo-\\nlutional neural networks.\" Advances in\\nneural information processing systems.\\n2012.\\n[Lev et al., 2015] Lev, Guy, Gil Sadeh, Ben-\\njamin Klein, and Lior Wolf. \"RNN\\nFisher Vectors for Action Recognition\\nand Image Annotation.\" arXiv preprint\\narXiv:1512.03958 (2015).\\n[Li and Roth, 2002] Li, Xin, and Dan Roth.\\n\"Learning question classi\ufb01ers.\" Proceed-\\nings of the 19th international conference\\non Computational linguistics-Volume 1.\\nAssociation for Computational Linguis-\\ntics, 2002.\\n[Lowe, 1999] Lowe, David G. \"Object recogni-\\ntion from local scale-invariant features.\"\\nComputer vision, 1999. The proceedings\\nof the seventh IEEE international confer-\\nence on. Vol. 2. Ieee, 1999.\\n[Maaten and Hinton, 2008] Maaten, Laurens\\nvan der, and Geoffrey Hinton. \"Visualiz-\\ning data using t-SNE.\" Journal of Machine\\nLearning Research 9.Nov (2008): 2579-\\n2605.\\n[Mikolov et. al., 2013] Mikolov, Tomas, Kai\\nChen, Greg Corrado, and Jeffrey Dean.\\n\"Ef\ufb01cient estimation of word represen-\\ntations in vector space.\" arXiv preprint\\narXiv:1301.3781 (2013).\\n[Mitchell et al., 2008] Mitchell, Tom M., Svet-\\nlana V . Shinkareva, Andrew Carlson, Kai-\\nMin Chang, Vicente L. Malave, Robert A.\\nMason, and Marcel Adam Just. \"Predict-\\ning human brain activity associated with\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nthe meanings of nouns.\" science 320, no.\\n5880 (2008): 1191-1195.\\n[Mitchell and Lapata, 2008] Mitchell, Jeff, and\\nMirella Lapata. \"Vector-based Models of\\nSemantic Composition.\" ACL. 2008.\\n[Osgood, 1964] Osgood, Charles E. \"Semantic\\ndifferential technique in the comparative\\nstudy of cultures.\" American Anthropolo-\\ngist 66.3 (1964): 171-200.\\n[Pang and Lee, 2004] Pang, B., Lee, L. A sen-\\ntimental education: Sentiment analysis\\nusing subjectivity summarization based\\non minimum cuts. In Proceedings of the\\n42nd annual meeting on Association for\\nComputational Linguistics (p. 271). As-\\nsociation for Computational Linguistics.\\n2004.\\n[Pang and Lee, 2005] Pang, Bo, and Lillian\\nLee. \"Seeing stars: Exploiting class re-\\nlationships for sentiment categorization\\nwith respect to rating scales.\" Proceedings\\nof the 43rd annual meeting on association\\nfor computational linguistics. Association\\nfor Computational Linguistics, 2005.\\n[Pennington et. al., 2014] Pennington, Jeffrey,\\nRichard Socher, and Christopher D. Man-\\nning. \"Glove: Global Vectors for Word\\nRepresentation.\" EMNLP . Vol. 14. (2014).\\n[Razavian et al., 2014] Sharif Razavian, Ali,\\nHossein Azizpour, Josephine Sullivan,\\nand Stefan Carlsson. \"CNN features off-\\nthe-shelf: an astounding baseline for\\nrecognition.\" In Proceedings of the IEEE\\nConference on Computer Vision and Pat-\\ntern Recognition Workshops, pp. 806-813.\\n2014.\\n[Reed et al., 2015] Reed, Scott E., Yi Zhang,\\nYuting Zhang, and Honglak Lee. \"Deep\\nvisual analogy-making.\" In Advances in\\nNeural Information Processing Systems,\\npp. 1252-1260. 2015.\\n[Rumelhart et al., 1988] Rumelhart, David E.,\\nGeoffrey E. Hinton, and Ronald J.\\nWilliams. \"Learning representations by\\nback-propagating errors.\" Cognitive mod-\\neling 5.3 (1988): 1.\\n[Russakovsky et al., 2015] Russakovsky, Olga,\\nJia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang\\net al. \"Imagenet large scale visual recog-\\nnition challenge.\" International Journal of\\nComputer Vision 115, no. 3 (2015): 211-\\n252.\\n[Salton et al., 1975] Salton, Gerard, Anita\\nWong, and Chung-Shu Yang. \"A vector\\nspace model for automatic indexing.\"\\nCommunications of the ACM 18.11 (1975):\\n613-620.\\n[Schwartz et al., 2015] Schwartz, Roy, Roi Re-\\nichart, and Ari Rappoport. \"Symmetric\\npattern based word embeddings for im-\\nproved word similarity prediction.\" Proc.\\nof CoNLL. 2015.\\n[Socher et al., 2013] Socher, Richard, Alex\\nPerelygin, Jean Y. Wu, Jason Chuang,\\nChristopher D. Manning, Andrew Y. Ng,\\nand Christopher Potts. \"Recursive deep\\nmodels for semantic compositionality over\\na sentiment treebank.\" In Proceedings of\\nthe conference on empirical methods in\\nnatural language processing (EMNLP),\\nvol. 1631, p. 1642. 2013.\\n[Socher et al., 2013] Socher, Richard, Milind\\nGanjoo, Christopher D. Manning, and\\nAndrew Ng. \"Zero-shot learning through\\ncross-modal transfer.\" In Advances in neu-\\nral information processing systems, pp.\\n935-943. 2013.\\n[Wallace et al., 2014] Byron C Wallace, Laura\\nKertz Do Kook Choe, and Eugene Char-\\nniak. Humans require context to infer\\nironic intent (so computers probably do,\\ntoo). In Proceedings of the Annual Meet-\\ning of the Association for Computational\\nLinguistics (ACL), 2014, pages 512-516.\\n[Weston, et al., 2010] Weston, Jason, Samy\\nBengio, and Nicolas Usunier. \"Large scale\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nimage annotation: learning to rank with\\njoint word-image embeddings.\" Machine\\nlearning 81.1 (2010): 21-35.\\n[Yangqing et al., 2014] Jia, Yangqing, Evan\\nShelhamer, Jeff Donahue, Sergey Karayev,\\nJonathan Long, Ross Girshick, Sergio\\nGuadarrama, and Trevor Darrell. \"Caffe:\\nConvolutional architecture for fast feature\\nembedding.\" In Proceedings of the 22nd\\nACM international conference on Multi-\\nmedia, pp. 675-678. ACM, 2014.\\n[Zhang and Wallace, 2015] Zhang, Ye, and By-\\nron Wallace. \"A Sensitivity Analysis of\\n(and Practitioners\u2019 Guide to) Convolu-\\ntional Neural Networks for Sentence Clas-\\nsi\ufb01cation.\"arXiv preprint arXiv:1510.03820\\n(2015).\\n[Zeiler, 2012] Zeiler, Matthew D. \"ADADELTA:\\nan adaptive learning rate method.\" arXiv\\npreprint arXiv:1212.5701 (2012).\\n[Zeiler and Fergus, 2014] Zeiler, Matthew D.,\\nand Rob Fergus. \"Visualizing and under-\\nstanding convolutional networks.\" Euro-\\npean Conference on Computer Vision.\\nSpringer International Publishing, 2014.\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser \u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n\u221adk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1\u221adk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221adk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = \u2211dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni \u2208Rdmodel\u00d7dk , WK\\ni \u2208Rdmodel\u00d7dk , WV\\ni \u2208Rdmodel\u00d7dv\\nand WO \u2208Rhdv\u00d7dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 \u00b7d) O(1) O(1)\\nRecurrent O(n\u00b7d2) O(n) O(n)\\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and \ufb01xed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to 10000 \u00b72\u03c0. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any \ufb01xed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi \u2208Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\u00b7n\u00b7d+ n\u00b7d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5= 10\u22129. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d\u22120.5\\nmodel \u00b7min(step_num\u22120.5,step_num\u00b7warmup_steps\u22121.5) (3)\\nThis corresponds to increasing the learning rate linearly for the \ufb01rst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 \u00b71020\\nGNMT + RL [31] 24.6 39.92 2.3 \u00b71019 1.4 \u00b71020\\nConvS2S [8] 25.16 40.46 9.6 \u00b71018 1.5 \u00b71020\\nMoE [26] 26.03 40.56 2.0 \u00b71019 1.2 \u00b71020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 \u00b71020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 \u00b71020 1.1 \u00b71021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 \u00b71019 1.2 \u00b71021\\nTransformer (base model) 27.3 38.1 3.3 \u00b7 1018\\nTransformer (big) 28.4 41.0 2.3 \u00b71019\\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The con\ufb01guration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty \u03b1= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of \ufb02oating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision \ufb02oating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop \u03f5ls\\ntrain PPL BLEU params\\nsteps (dev) (dev) \u00d7106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\u2013778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735\u20131780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\\nLearning Research, 15(1):1929\u20131958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract\u2014Due to object detection\u2019s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassi\ufb01ers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modi\ufb01cations and useful tricks\\nto improve detection performance further. As distinct speci\ufb01c\\ndetection tasks exhibit different characteristics, we also brie\ufb02y\\nsurvey several speci\ufb01c tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms\u2014deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classi\ufb01cation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these \ufb01elds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]\u2013[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it\u2019s dif\ufb01-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this \ufb01eld in recent years [15]\u2013[18].\\nThe problem de\ufb01nition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classi\ufb01ca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classi\ufb01cation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan \ufb01nd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a \ufb01xed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit\u2019s dif\ufb01cult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassi\ufb01cation. Besides, a classi\ufb01er is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classi\ufb01ers, the\\nDPM is a \ufb02exible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninef\ufb01cient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signi\ufb01cant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classi\ufb01cation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a \ufb01xed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]\u2013[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversi\ufb01ed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classi\ufb01cation and object detection, but\\npays little attention on detailing speci\ufb01c algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speci\ufb01c tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the over\ufb01tting of training, lack of large scale\\ntraining data, limited computation power and insigni\ufb01cance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n\u2022The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n\u2022Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n\u2022Signi\ufb01cant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\nover\ufb01tting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite ef\ufb01cient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton\u2019s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and \u2018dropout\u2019 regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose \u2018pixel\u2019 can be viewed as a speci\ufb01c feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive \ufb01eld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as \ufb01ltering and pooling. Filtering (convolution)\\noperation convolutes a \ufb01lter matrix (learned weights) with\\nthe values of a receptive \ufb01eld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain \ufb01nal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive \ufb01eld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be \ufb01ne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the \ufb01nal layer with different activation functions [6]\\nis added to get a speci\ufb01c conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassi\ufb01cation layer. The conv feature maps are produced by\\nconvoluting 3*3 \ufb01lter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n\u2022Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n\u2022 Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n\u2022 The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classi\ufb01cation and bounding box regression\\ninto a multi-task leaning manner).\\n\u2022 Bene\ufb01tting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research \ufb01elds, such as image super-resolution\\nreconstruction [54], [55], image classi\ufb01cation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]\u2013[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the con\ufb01dences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at \ufb01rst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classi\ufb01cation problem, adopting a uni\ufb01ed\\nframework to achieve \ufb01nal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modi\ufb01es R-\\nCNN with a SPP layer). The regression /classi\ufb01cation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario \ufb01rstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the con\ufb01dences of\\nunderlying object categories.\\n1) R-CNN: It is of signi\ufb01cance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the \ufb02owchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a \ufb01xed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the \ufb01nal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassi\ufb01cation and localization. With pre-trained category-\\nspeci\ufb01c linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classi\ufb01cation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012\u2014achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speci\ufb01c \ufb01ne-tuning, yields a signi\ufb01-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\n\u02dcrbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the \ufb01rst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima\u2019s \u201cneocognitron\u201d [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classi\ufb01es each\\nregion using class-speci\ufb01c linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classi\ufb01cation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun\u2019s CNN (e.g., max(x, 0) rectifying non-linearities and\\n\u201cdropout\u201d regularization).\\nThe signi\ufb01cance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classi\ufb01cation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classi\ufb01cation and object detection.\\nThis paper is the \ufb01rst to show that a CNN can lead to dra-\\n1\\nFig. 3. The \ufb02owchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classi\ufb01es each region with class-speci\ufb01c linear SVMs.\\nbounding box regression and \ufb01ltered with a greedy non-\\nmaximum suppression (NMS) to produce \ufb01nal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insuf\ufb01cient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN \ufb01rstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speci\ufb01c \ufb01ne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigni\ufb01cance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n\u2022Due to the existence of FC layers, the CNN requires a\\n\ufb01xed-size (e.g., 227\u00d7227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n\u2022Training of R-CNN is a multi-stage pipeline. At \ufb01rst,\\na convolutional network (ConvNet) on object proposals is\\n\ufb01ne-tuned. Then the softmax classi\ufb01er learned by \ufb01ne-\\ntuning is replaced by SVMs to \ufb01t in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n\u2022 Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n\u2022Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or re\ufb01ne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speci\ufb01c CNN classi\ufb01ers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classi\ufb01cation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net:FC layers must take a \ufb01xed-size input. That\u2019s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several \ufb01ner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]\u2020 56.07 74.41 \u00b11.0\\nLLC [18]\u2020 57.66 76.95 \u00b10.4\\nFK [19]\u2020 61.69 77.78 \u00b10.6\\nDeCAF [13] - 86.91 \u00b10.7\\nZeiler & Fergus [4] 75.90\u2021 86.5\u00b10.5\\nOquab et al. [34] 77.7 -\\nChat\ufb01eld et al. [6] 82.42 88.54\u00b10.3\\nours 82.44 93.42 \u00b10.5\\nTable 8: Classi\ufb01cation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). \u2020numbers reported\\nby [27]. \u2021our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brie\ufb02y review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN \ufb01rst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a \ufb01xed size (227 \u00d7227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classi\ufb01er is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a \ufb01xed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\nde\ufb01ne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n\u2026...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the \u201cfast\u201d mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 \u00d71, 2\u00d72, 3\u00d73, 6\u00d76, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 \u00d750) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classi\ufb01er for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classi\ufb01er\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns \u2208 S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically \ufb01nd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s \u2208 S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 \u00d7224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, \ufb01ne-tuning a network with log loss, training SVMs,\\nand \ufb01nally \ufb01tting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the \ufb01ne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (\ufb01xed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that \ufb01xes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it\u2019s comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network \ufb01rst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a \ufb01xed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that \ufb01nally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all \u201cbackground\u201d class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes re\ufb01ned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a \ufb01xed spatial extent of H \u00d7 W (e.g., 7 \u00d7 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\nde\ufb01ned by a four-tuple (r, c, h, w ) that speci\ufb01es its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a \ufb01xed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h \u00d7 w RoI win-\\ndow into an H \u00d7 W grid of sub-windows of approximate\\nsize h/H \u00d7 w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with \ufb01ve max pooling layers and between \ufb01ve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is con\ufb01gured by setting H and W to be\\ncompatible with the net\u2019s \ufb01rst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network\u2019s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classi\ufb01-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speci\ufb01c bounding-box regressors).\\nThird, the network is modi\ufb01ed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let\u2019s elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inef\ufb01cient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inef\ufb01ciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to \ufb01xed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the \ufb01nal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe \ufb01nal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 \u00d7(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection ef\ufb01ciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and ef\ufb01ciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network \ufb01ne-tuning, SVM training and bounding-\\nbox regressor \ufb01tting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the \ufb01ne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classi\ufb01cation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a \ufb01xed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\n\ufb01nally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one \u2018background\u2019\\nclass) and the other output layer encodes re\ufb01ned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is de\ufb01ned as below to jointly train\\nclassi\ufb01cation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +\u03bb[u\u22651]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =\u2212logpu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,\u00b7\u00b7\u00b7 ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is de\ufb01ned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u\u22651] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to \ufb01t\\nbounding-box regressors as below\\nLloc(tu,v) =\\n\u2211\\ni\u2208x,y,w,h\\nsmoothL1 (tu\\ni \u2212vi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|\u22120.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inef\ufb01cient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at \ufb01rst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nef\ufb01ciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving ef\ufb01ciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speci\ufb01c conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers\u2014a box-regression layer ( reg)\\nand a box-classi\ufb01cation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\n\ufb01eld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n\u00d7nconv layer followed by two sibling 1 \u00d71 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n\u00d7nconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW\u00d7H(typically \u223c2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) \u00d7800-dimensional output layer,\\nwhereas our method requires a (4+2)\u00d79-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of over\ufb01tting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these de\ufb01nitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is de\ufb01ned as:\\nL({pi},{ti}) = 1\\nNcls\\n\u2211\\ni\\nLcls (pi,p\u2217\\ni ) +\u03bb 1\\nNreg\\n\u2211\\ni\\np\u2217\\ni Lreg(ti,t\u2217\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K prede\ufb01ned anchor boxes are\\nconvoluted with each sliding window to produce \ufb01xed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn\u00d7n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classi\ufb01cation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n\u00d7n conv layer followed by two sibling 1 \u00d71 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n\u00d7n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n\u2211\\ni\\nLcls(pi,p\u2217\\ni) +\u03bb 1\\nNreg\\n\u2211\\ni\\np\u2217\\niLreg (ti,t\u2217\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p\u2217\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t\u2217\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg ), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassi\ufb01cation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speci\ufb01c spatial pooling layer.\\nRecent state-of-the-art image classi\ufb01cation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it\u2019s\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll\u00b4ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signi\ufb01cant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object\u2019s scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this \ufb01gure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classi\ufb01cation. In other words, shifting an object inside\\nan image should be indiscriminative in image classi\ufb01cation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a \ufb01xed grid of k\u00d7k \ufb01rstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classi\ufb01cation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at \ufb01rst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 \u00d71 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3\u00d73\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the \ufb01nal feature map is\\ngenerated. This process is iterated until the \ufb01nest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacri\ufb01cing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classi\ufb01cation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m\u00d7m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassi\ufb01cation and bounding box regression, an additional loss\\nfor segmentation mask branch is de\ufb01ned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classi\ufb01cation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classi\ufb01cation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a \ufb02exible and ef\ufb01cient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modi\ufb01cation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and \u2018stuff\u2019 (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classi\ufb01cation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive \ufb01elds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classi\ufb01ers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modi\ufb01cations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modi\ufb01ed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and ef\ufb01cient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classi\ufb01ers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classi\ufb01er\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signi\ufb01cant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classi\ufb01cation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\n\ufb01rstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the \u2018deep and thin\u2019 design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /Classi\ufb01cation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classi\ufb01cation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classi\ufb01cation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We \ufb01rstly reviews some pioneer CNN\\nmodels, and then focus on two signi\ufb01cant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classi\ufb01cation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has dif\ufb01culty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is ef\ufb01cient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uni\ufb01ed\\nloss was introduced to bias both localization and con\ufb01dences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the \ufb01nal layer.\\nYoo et al. adopted an iterative classi\ufb01cation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inef\ufb01cient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\n\ufb01nding a path from a \ufb01xed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a \ufb01xed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndif\ufb01culty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both con\ufb01dences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S\u00d7S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding con\ufb01dence scores. Formally, con\ufb01-\\ndence scores are de\ufb01ned as Pr(Object) \u2217IOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) \u22650) and\\nshows con\ufb01dences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speci\ufb01c con\ufb01dence scores for each box\\nare achieved by multiplying the individual box con\ufb01dence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) \u2217IOUtruth\\npred \u2217Pr(Classi|Object)\\n= Pr(Classi) \u2217IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speci\ufb01c objects in the\\nbox and the \ufb01tness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\n\u03bbcoord\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n[\\n(xi \u2212\u02c6xi)2 + (yi \u2212\u02c6yi)2]\\n+\u03bbcoord\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n[(\u221awi \u2212\\n\u221a\\n\u02c6wi)2 + (\\n\u221a\\nhi \u2212\\n\u221a\\n\u02c6hi\\n)2]\\n+\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n(\\nCi \u2212\u02c6Ci\\n)2\\n+\u03bbnoobj\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 noobj\\nij\\n(\\nCi \u2212\u02c6Ci\\n)2\\n+\\nS2\\n\u2211\\ni=0\\n1 obj\\ni\\n\u2211\\nc\u2208classes\\n(pi(c) \u2212\u02c6pi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents con\ufb01dence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classi\ufb01cation errors. Similarly,\\nwhen the predictor is \u2018responsible\u2019 for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 \u00d71 reduction layers followed by 3 \u00d73 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpli\ufb01ed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a dif\ufb01culty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\ncon\ufb01gurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speci\ufb01c feature map, instead of\\n\ufb01xed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated con\ufb01dences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and con\ufb01dence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale re\ufb01ned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signi\ufb01cantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300\u00d7300) runs\\nat 59 FPS, which is more accurate and ef\ufb01cient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated con\ufb01dences. Final detection results are obtained by conducting NMS on multi-scale re\ufb01ned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speci\ufb01c instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classi\ufb01cation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n\u2022If incorporated with a proper way, more powerful back-\\nbone CNN models can de\ufb01nitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n\u2022 With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n\u2022Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith \u201807\u2019 ,\u201807+12\u2019 and \u201807+12+coco\u2019).\\n\u2022Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\n\ufb01ed classi\ufb01cation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n\u2022As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and \ufb01ne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n\u2022By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n\u2022 Multi-scale training and test are bene\ufb01cial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n\u2022 Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/class\ufb01cation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/class\ufb01cation based approaches.\\n\u2022 Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n\u2022Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n\u2022The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classi\ufb01ers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classi\ufb01cation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classi\ufb01cation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object con\ufb01dence+background con\ufb01dence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object con\ufb01dence+background con\ufb01dence\\n* \u2018+\u2019 denotes that corresponding techniques are employed while \u2018-\u2019 denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* \u201807\u2019: VOC2007 trainval, \u201807+12\u2019: union of VOC2007 and VOC2012 trainval, \u201807+12+COCO\u2019: trained on COCO trainval35k at \ufb01rst and then \ufb01ne-tuned on 07+12. The S in ION \u201807+12+S\u2019 denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* \u201807++12\u2019: union of VOC2007 trainval and test and VOC2012 trainval. \u201807++12+COCO\u2019: trained on COCO trainval35k at \ufb01rst then \ufb01ne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for \u2018SS\u2019 which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n\u2022 By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uni\ufb01ed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It\u2019s also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: \u2018fast mode\u2019 Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n\u2022It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n\u2022It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n\u2022Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modi\ufb01ed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signi\ufb01cance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by \ufb01ne-tuning DNNs\u2019\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and re\ufb01ned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uni\ufb01ed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is bene\ufb01cial. To learn internal represen-\\ntations of saliency ef\ufb01ciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive \ufb01elds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signi\ufb01cance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye \ufb01xations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is ef\ufb01cient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is de\ufb01ned as below\\nF\u03b2 = (1 +\u03b22)Presion \u00d7Recall\\n\u03b22Presion + Recall (7)\\nwhere \u03b22 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH\u00d7W\\nH\u2211\\ni=1\\nW\u2211\\nj=1\\n\u23d0\u23d0\u23d0\u02c6S(i,j) = \u02c6Z(i,j)\\n\u23d0\u23d0\u23d0 (8)\\nwhere \u02c6Z and \u02c6S represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can \ufb01nd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insuf\ufb01cient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signi\ufb01cance for measuring local\\nconspicuity. Finally, it\u2019s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]\u2013[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural con\ufb01gurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classi\ufb01ers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nef\ufb01ciency. However, this detector may degrade signi\ufb01cantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]\u2013[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nef\ufb01cient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wF\u03b2 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wF\u03b2 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wF\u03b2 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wF\u03b2 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wF\u03b2 is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modi\ufb01cations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uni\ufb01ed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a \ufb01xed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating prede\ufb01ned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na con\ufb01guration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-\ufb01ne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and re\ufb01ne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to re\ufb01ne the positions of possible\\nfaces. Qin et al. proposed a uni\ufb01ed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can re\ufb02ect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the re\ufb02ection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signi\ufb01cant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is ef\ufb01cient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nef\ufb01ciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signi\ufb01cance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identi\ufb01cation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]\u2013[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin \u2018plain\u2019 features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different con\ufb01gurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modi\ufb01ed the down-\\nstream classi\ufb01er by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand \ufb01ne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassi\ufb01ers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of con\ufb01dences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 \ufb01ne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modi\ufb01cation and\\nsimpli\ufb01cation is of signi\ufb01cance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classi\ufb01ers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely \u2018Person (clear identi\ufb01cations)\u2019,\\n\u2018Person? (unclear identi\ufb01cations)\u2019 and \u2018People (large group of\\nindividuals)\u2019, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10\u22122 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The \ufb01rst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classi\ufb01ers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classi\ufb01ers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe \ufb01rst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n\u2022 Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speci\ufb01c\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n\u2022Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all bene\ufb01cial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n\u2022Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n\u2022Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classi\ufb01ers at latter stages can\\nhandle more dif\ufb01cult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are \ufb01xed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n\u2022 Unsupervised and weakly supervised learning. It\u2019s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand re\ufb01ne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n\u2022 Network optimization. Given speci\ufb01c applications and\\nplatforms, it is signi\ufb01cant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n\u20223D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n\u2022 Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical \ufb02ow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modi\ufb01cations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrie\ufb02y reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n\u201cObject detection with discriminatively trained part-based models,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, \u201cExample-based learning for view-based\\nhuman face detection,\u201dIEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39\u201351, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, \u201cPedestrian detection:\\nAn evaluation of the state of the art,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, \u201cDetection of spicules on mammogram\\nbased on skeleton analysis.\u201d IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235\u2013245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for\\nfast feature embedding,\u201d in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classi\ufb01cation\\nwith deep convolutional neural networks,\u201d in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, \u201cRealtime multi-person\\n2d pose estimation using part af\ufb01nity \ufb01elds,\u201d in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, \u201cA multi-scale cascade fully convolutional\\nnetwork face detector,\u201d in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, \u201cDeepdriving:\\nLearning affordance for direct perception in autonomous driving,\u201d in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object\\ndetection network for autonomous driving,\u201d in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, \u201cEmbedded streaming\\ndeep neural networks accelerator with applications,\u201d IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572\u20131583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, \u201cLow-complexity\\napproximate convolutional neural networks,\u201dIEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1\u201312, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n\u201cCost-sensitive learning of deep feature representations from imbal-\\nanced data.\u201d IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1\u201315, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, \u201cFeature extraction with deep\\nneural networks by a generalized discriminant analysis.\u201d IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596\u2013608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature\\nhierarchies for accurate object detection and semantic segmentation,\u201d\\nin CVPR, 2014.\\n[16] R. Girshick, \u201cFast r-cnn,\u201d in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look\\nonce: Uni\ufb01ed, real-time object detection,\u201d in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-\\ntime object detection with region proposal networks,\u201d in NIPS, 2015,\\npp. 91\u201399.\\n[19] D. G. Lowe, \u201cDistinctive image features from scale-invariant key-\\npoints,\u201d Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91\u2013110, 2004.\\n[20] N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human\\ndetection,\u201d in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, \u201cAn extended set of haar-like features for\\nrapid object detection,\u201d in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, \u201cSupport vector machine,\u201dMachine Learning,\\nvol. 20, no. 3, pp. 273\u2013297, 1995.\\n[23] Y . Freund and R. E. Schapire, \u201cA desicion-theoretic generalization of\\non-line learning and an application to boosting,\u201d J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663\u2013671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n\u201cObject detection with discriminatively trained part-based models,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627\u20131645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, \u201cThe pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),\u201d 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol.\\n521, no. 7553, pp. 436\u2013444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, \u201cPredicting eye \ufb01xations\\nusing convolutional neural networks,\u201d in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, \u201cLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,\u201d in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, \u201cFace detection with the faster r-cnn,\u201d\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, \u201cJoint cascade face\\ndetection and alignment,\u201d in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, \u201cSupervised transformer network\\nfor ef\ufb01cient face detection,\u201d in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, \u201cA real-time\\npedestrian detector using deep learning for human-aware navigation,\u201d\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, \u201cExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassi\ufb01ers,\u201d in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, \u201cA survey of deep learning methods and\\nsoftware tools for image classi\ufb01cation and object detection,\u201d Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, \u201cHow we know universals the perception\\nof auditory and visual forms,\u201dThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127\u2013147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \u201cLearning internal\\nrepresentation by back-propagation of errors,\u201d Nature, vol. 323, no.\\n323, pp. 533\u2013536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality\\nof data with neural networks,\u201d Sci., vol. 313, pp. 504\u2013507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., \u201cDeep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,\u201d IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82\u201397, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet:\\nA large-scale hierarchical image database,\u201d in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, \u201cBinary coding of speech spectrograms using a deep auto-\\nencoder,\u201d in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., \u201cPhone recognition with\\nthe mean-covariance restricted boltzmann machine,\u201d in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, \u201cImproving neural networks by preventing co-\\nadaptation of feature detectors,\u201d arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,\u201d in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n\u201cOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,\u201d arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, \u201cGoing deeper with\\nconvolutions,\u201d in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\\nfor large-scale image recognition,\u201d arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\\nrecognition,\u201d in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted\\nboltzmann machines,\u201d in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , \u201cWeakly supervised\\nobject recognition with convolutional neural networks,\u201d in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \u201cLearning and transferring\\nmid-level image representations using convolutional neural networks,\u201d\\nin CVPR, 2014.\\n[51] F. M. Wadley, \u201cProbit analysis: a statistical treatment of the sigmoid\\nresponse curve,\u201d Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549\u2013553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , \u201cLearning invariant\\nfeatures through topographic \ufb01lter maps,\u201d in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, \u201cLearning convolutional feature hierarchies for visual\\nrecognition,\u201d in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, \u201cDeconvolu-\\ntional networks,\u201d in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, \u201cLearning deconvolution network for\\nsemantic segmentation,\u201d in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, \u201cPlant leaf iden-\\nti\ufb01cation via a growing convolution neural network with progressive\\nsample learning,\u201d in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, \u201cNeural codes\\nfor image retrieval,\u201d in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n\u201cDeep learning for content-based image retrieval: A comprehensive\\nstudy,\u201d in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. Barof\ufb01o, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, \u201cDeep convolutional neural networks for pedestrian detec-\\ntion,\u201d Signal Process.: Image Commun. , vol. 47, pp. 482\u2013489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, \u201cSubcategory-aware\\nconvolutional neural networks for object proposals and detection,\u201d in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, \u201cPedestrian\\ndetection based on fast r-cnn and batch normalization,\u201d in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n\u201cMultimodal deep learning,\u201d in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, \u201cModeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classi\ufb01-\\ncation,\u201d in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid pooling in deep\\nconvolutional networks for visual recognition,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904\u20131916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., \u201cR-fcn: Object detection via region-based\\nfully convolutional networks,\u201d in NIPS, 2016, pp. 379\u2013387.\\n[66] T.-Y . Lin, P. Doll \u00b4ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, \u201cFeature pyramid networks for object detection,\u201d in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll \u00b4ar, and R. B. Girshick, \u201cMask r-cnn,\u201d in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, \u201cScalable object\\ndetection using deep neural networks,\u201d in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, \u201cAttentionnet:\\nAggregating weak directions for accurate object detection,\u201d in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, \u201cG-cnn: an iterative grid\\nbased object detector,\u201d in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, \u201cSsd: Single shot multibox detector,\u201d in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, \u201cYolo9000: better, faster, stronger,\u201d\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, \u201cDssd:\\nDeconvolutional single shot detector,\u201d arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, \u201cDsod:\\nLearning deeply supervised object detectors from scratch,\u201d in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, \u201cTransforming auto-\\nencoders,\u201d in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, \u201cLearning invariance\\nthrough imitation,\u201d in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, \u201cHistograms of sparse codes for object\\ndetection,\u201d in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n\u201cSelective search for object recognition,\u201d Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154\u2013171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, \u201cPedestrian\\ndetection with unsupervised multi-stage feature learning,\u201d in CVPR,\\n2013.\\n[80] P. Kr \u00a8ahenb\u00a8uhl and V . Koltun, \u201cGeodesic object proposals,\u201d in ECCV,\\n2014.\\n[81] P. Arbel \u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n\u201cMultiscale combinatorial grouping,\u201d in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll \u00b4ar, \u201cEdge boxes: Locating object proposals\\nfrom edges,\u201d in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, \u201cDeepbox: Learning objectness\\nwith convolutional networks,\u201d in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll \u00b4ar, \u201cLearning to\\nre\ufb01ne object segments,\u201d in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, \u201cImproving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,\u201d in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel \u00b4aez, and J. Malik, \u201cLearning rich features\\nfrom rgb-d images for object detection and segmentation,\u201d in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., \u201cDeepid-net: Deformable deep convolutional\\nneural networks for object detection,\u201d in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, \u201cR-cnn minus r,\u201d arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, \u201cBeyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,\u201d\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S \u00b4anchez, and T. Mensink, \u201cImproving the \ufb01sher kernel\\nfor large-scale image classi\ufb01cation,\u201d in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, \u201cRestructuring of deep neural network\\nacoustic models with singular value decomposition.\u201d in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\\nobject detection with region proposal networks,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137\u20131149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink-\\ning the inception architecture for computer vision,\u201d in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll \u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in\\ncontext,\u201d in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, \u201cInside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,\u201d in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, \u201cPixelwise instance segmentation with a\\ndynamically instantiated network,\u201d in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, \u201cInstance-aware semantic segmentation via\\nmulti-task network cascades,\u201d in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, \u201cFully convolutional instance-\\naware semantic segmentation,\u201d in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n\u201cSpatial transformer networks,\u201d in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, \u201cStuffnet: Using stuffto\\nimprove object detection,\u201d in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, \u201cHypernet: Towards accurate\\nregion proposal generation and joint object detection,\u201d in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, \u201cCurriculum learning\\nof multiple tasks,\u201d in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, \u201cRotating your\\nface using multi-task deep neural network,\u201d in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, \u201cMulti-stage object\\ndetection with group recursive learning,\u201d arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, \u201cA uni\ufb01ed multi-scale\\ndeep convolutional neural network for fast object detection,\u201d in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, \u201csegdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,\u201d in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, \u201cScene labeling\\nwith lstm recurrent neural networks,\u201d in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, \u201cLearning to detect and\\nlocalize many objects from few examples,\u201d arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, \u201cGated bi-\\ndirectional cnn for object detection,\u201d in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, \u201cObject detection via a multi-region and\\nsemantic segmentation-aware cnn model,\u201d in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, \u201cBidirectional recurrent neural net-\\nworks,\u201d IEEE Trans. Signal Process. , vol. 45, pp. 2673\u20132681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll \u00b4ar, \u201cA multipath network for object detection,\u201d\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, \u201cTraining region-based\\nobject detectors with online hard example mining,\u201d in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, \u201cObject detection\\nnetworks on convolutional feature maps,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476\u20131481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, \u201cFactors in \ufb01netuning\\ndeep model for object detection with long-tail distribution,\u201d in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, \u201cPvanet:\\nLightweight deep neural networks for real-time object detection,\u201d\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, \u201cUnderstanding and\\nimproving convolutional neural networks via concatenated recti\ufb01ed\\nlinear units,\u201d in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, \u201cDeep neural networks for object\\ndetection,\u201d in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll \u00b4ar, \u201cLearning to segment object\\ncandidates,\u201d in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, \u201cScalable,\\nhigh-quality object detection,\u201d arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n\u201cThe pascal visual object classes challenge 2012 (voc2012) results\\n(2012),\u201d in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolu-\\ntional networks,\u201d in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll \u00b4ar, Z. Tu, and K. He, \u201cAggregated residual\\ntransformations for deep neural networks,\u201d in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n\u201cDeformable convolutional networks,\u201d arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, \u201cAutocollage,\u201dACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847\u2013852, 2006.\\n[126] C. Jung and C. Kim, \u201cA uni\ufb01ed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,\u201d IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272\u20131283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, \u201cReal-time salient object\\ndetection with a minimum spanning tree,\u201d in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, \u201cTop-down visual saliency via joint crf and\\ndictionary learning,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576\u2013588, 2017.\\n[129] P. L. Rosin, \u201cA simple method for detecting salient regions,\u201d Pattern\\nRecognition, vol. 42, no. 11, pp. 2363\u20132371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n\u201cLearning to detect a salient object,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353\u2013367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks\\nfor semantic segmentation,\u201d in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, \u201cDiscriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989\u20131005, 2009.\\n[133] S. Xie and Z. Tu, \u201cHolistically-nested edge detection,\u201d in ICCV, 2015.\\n[134] M. K \u00a8ummerer, L. Theis, and M. Bethge, \u201cDeep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,\u201d\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, \u201cSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,\u201d\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, \u201cDeep networks for saliency\\ndetection via local estimation and global search,\u201d in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, \u201cWeakly supervised top-down\\nsalient object detection,\u201d arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, \u201cSaliency detection by\\nmulti-context deep learning,\u201d in CVPR, 2015.\\n[139] C \u00b8 . Bak, A. Erdem, and E. Erdem, \u201cTwo-stream convolutional networks\\nfor dynamic saliency prediction,\u201d arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, \u201cSupercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,\u201d\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330\u2013344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, \u201cDeepsaliency: Multi-task deep neural network model for\\nsalient object detection,\u201d IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919\u20133930, 2016.\\n[142] Y . Tang and X. Wu, \u201cSaliency detection via combining region-level\\nand pixel-level predictions with cnns,\u201d in ECCV, 2016.\\n[143] G. Li and Y . Yu, \u201cDeep contrast learning for salient object detection,\u201d\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, \u201cEdge preserving and\\nmulti-scale contextual neural network for salient object detection,\u201d\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, \u201cA deep multi-level\\nnetwork for saliency prediction,\u201d in ICPR, 2016.\\n[146] G. Li and Y . Yu, \u201cVisual saliency detection based on multiscale deep\\ncnn features,\u201d IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012\u2013\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O\u2019Connor,\\n\u201cShallow and deep convolutional networks for saliency prediction,\u201d in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, \u201cRecurrent attentional networks for\\nsaliency detection,\u201d in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, \u201cDeeply-supervised recurrent convolutional\\nneural network for saliency detection,\u201d in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, \u201cContextual\\nhypergraph modeling for salient object detection,\u201d in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, \u201cGlobal\\ncontrast based salient region detection,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569\u2013582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, \u201cSalient object\\ndetection: A discriminative regional feature integration approach,\u201d in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, \u201cDeep saliency with encoded low level\\ndistance map and high level features,\u201d in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n\u201cNon-local deep features for salient object detection,\u201d in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n\u201cDeeply supervised salient object detection with short connections,\u201d\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, \u201cHierarchical saliency detection,\u201d in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, \u201cThe secrets of\\nsalient object segmentation,\u201d in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, \u201cDesign and perceptual validation of\\nperformance measures for salient object segmentation,\u201d in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, \u201cSalient object detection:\\nA benchmark,\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706\u2013\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, \u201cGraphical representation for\\nheterogeneous face recognition,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301\u2013312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, \u201cFace recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,\u201d in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, \u201cFace sketchcphoto synthesis\\nand retrieval using sparse representation,\u201d IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213\u20131226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, \u201cA comprehensive survey\\nto face hallucination,\u201d Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9\u201330, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, \u201cMultiple\\nrepresentations-based face sketch-photo synthesis.\u201dIEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201\u20132215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, \u201cAutomatic facial\\nexpression recognition system using deep network-based data fusion,\u201d\\nIEEE Trans. Cybern. , vol. 48, pp. 103\u2013114, 2018.\\n[166] P. Viola and M. Jones, \u201cRobust real-time face detection,\u201d Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137\u2013154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, \u201cUnitbox: An advanced\\nobject detection network,\u201d in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, \u201cMulti-view face detection\\nusing deep convolutional neural networks,\u201d in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, \u201cFrom facial parts responses\\nto face detection: A deep learning approach,\u201d in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, \u201cFace detection through\\nscale-friendly deep convolutional networks,\u201d in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, \u201cScale-aware face\\ndetection,\u201d in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, \u201cFace r-cnn,\u201d arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, \u201cFace detection using deep learning: An\\nimproved faster rcnn approach,\u201d arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, \u201cDensebox: Unifying landmark\\nlocalization with end to end object detection,\u201d arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, \u201cface detection with end-to-end\\nintegration of a convnet and a 3d model,\u201d in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, \u201cJoint face detection and\\nalignment using multitask cascaded convolutional networks,\u201d IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499\u20131503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, \u201cCompact convolutional neural\\nnetwork cascadefor face detection,\u201d in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, \u201cJoint training of cascaded cnn for\\nface detection,\u201d in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, \u201cFddb: A benchmark for face detection\\nin unconstrained settings,\u201d Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, \u201cA convolutional neural\\nnetwork cascade for face detection,\u201d in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cAggregate channel features for\\nmulti-view face detection,\u201d in IJCB, 2014.\\n[182] N. Marku \u02c7s, M. Frljak, I. S. Pand \u02c7zi\u00b4c, J. Ahlberg, and R. Forchheimer,\\n\u201cObject detection with pixel intensity comparisons organized in deci-\\nsion trees,\u201d arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, \u201cFace\\ndetection without bells and whistles,\u201d in ECCV, 2014.\\n[184] J. Li and Y . Zhang, \u201cLearning surf cascade for fast and accurate object\\ndetection,\u201d in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, \u201cA fast and accurate unconstrained\\nface detector,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211\u2013223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cConvolutional channel features,\u201d\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, \u201cHyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,\u201d arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, \u201cFinding tiny faces,\u201d in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, \u201cMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,\u201d IEEE Trans.\\nImage Process., vol. 27, pp. 1361\u20131375, 2018.\\n[190] D. Gavrila and S. Munder, \u201cMulti-cue pedestrian detection and tracking\\nfrom a moving vehicle,\u201d Int. J. of Comput. Vision , vol. 73, pp. 41\u201359,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, \u201cJointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidenti\ufb01cation,\u201d in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, \u201cStepwise metric promotion for unsuper-\\nvised video person re-identi\ufb01cation,\u201d in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, \u201cCooperative robots to observe\\nmoving targets: Review,\u201d IEEE Trans. Cybern. , vol. 48, pp. 187\u2013198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\\nThe kitti dataset,\u201d Int. J. of Robotics Res. , vol. 32, pp. 1231\u20131237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, \u201cLearning complexity-aware\\ncascades for deep pedestrian detection,\u201d in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\\nfor pedestrian detection,\u201d in CVPR, 2015.\\n[197] P. Doll \u00b4ar, R. Appel, S. Belongie, and P. Perona, \u201cFast feature pyramids\\nfor object detection,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532\u20131545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, \u201cFiltered channel features for\\npedestrian detection,\u201d in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, \u201cPedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243\u20131257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, \u201cDiscriminatively trained\\nand-or graph models for object shape detection,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959\u2013972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, \u201cHandling\\nocclusions with franken-classi\ufb01ers,\u201d in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, \u201cDetection and tracking of\\noccluded people,\u201d Int. J. of Comput. Vision, vol. 110, pp. 58\u201369, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, \u201cIs faster r-cnn doing well for\\npedestrian detection?\u201d in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\\nfor pedestrian detection,\u201d in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, \u201cMultispectral deep\\nneural networks for pedestrian detection,\u201d arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cPedestrian detection aided by\\ndeep learning semantic tasks,\u201d in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, \u201cFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,\u201d in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, \u201cPushing\\nthe limits of deep cnns for pedestrian detection,\u201d IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom \u00b4e, L. Bondi, L. Barof\ufb01o, S. Tubaro, E. Plebani, and D. Pau,\\n\u201cReduced memory region based deep convolutional neural network\\ndetection,\u201d in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, \u201cTaking a deeper\\nlook at pedestrians,\u201d in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, \u201cScale-aware fast\\nr-cnn for pedestrian detection,\u201d arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, \u201cVisual-textual\\njoint relevance learning for tag-based social image search,\u201dIEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363\u2013376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, \u201cRon: Reverse\\nconnection with objectness prior networks for object detection,\u201d in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, \u201cGenerative adversarial nets,\u201d\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, \u201cObject\\ndetection meets knowledge graphs,\u201d in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, \u201cSaliency-based sequential\\nimage attention with multiset prediction,\u201d in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, \u201cLearning detection with diverse\\nproposals,\u201d in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, \u201cEnd-to-end\\nmemory networks,\u201d in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, \u201cReal time image saliency for black box\\nclassi\ufb01ers,\u201d in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cCraft objects from images,\u201d in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, \u201cUnsupervised learning\\nfrom video to detect foreground objects in single images,\u201d in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, \u201cWeakly supervised object\\nlocalization with latent category learning,\u201d in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n\u201cTraining object class detectors with click supervision,\u201d inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, \u201cSpeed/accuracy\\ntrade-offs for modern convolutional object detectors,\u201d in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, \u201cMimicking very ef\ufb01cient network for object\\ndetection,\u201d in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a\\nneural network,\u201d Comput. Sci., vol. 14, no. 7, pp. 38\u201339, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, \u201cFitnets: Hints for thin deep nets,\u201d Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, \u201c3d object proposals for accurate object class detection,\u201d\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,\u201d in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n\u201cObject detection in videos with tubelet proposal networks,\u201d in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor\u2019s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Arti\ufb01cial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### split texts and get into chuncks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, overlap_size=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = overlap_size,\n",
    "        length_function = len,\n",
    "        separators= [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(all_pdf_documents)\n",
    "    print(f\"Split {len(all_pdf_documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    ## Example of chunck\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunck: \")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7db291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 61 documents into 401 chunks\n",
      "\n",
      "Example chunck: \n",
      "Content: XGBoost: A Scalable Tree Boosting System\n",
      "Tianqi Chen\n",
      "University of Washington\n",
      "tqchen@cs.washington.edu\n",
      "Carlos Guestrin\n",
      "University of Washington\n",
      "guestrin@cs.washington.edu\n",
      "ABSTRACT\n",
      "Tree boosting is a h...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5cedbc",
   "metadata": {},
   "source": [
    "### Embedding and vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cf45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e3eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "            Initialize the embedding manager with a pre-trained model\n",
    "            Model Used: Huggingface model name for sentence transformer\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\" Load the sentence transformer model \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model: {self.model_name}...\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully whose embedding dimension is {self.model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name} : {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embedding(self, texts:List[str]) -> np.ndarray:\n",
    "        \"\"\" Generate embeddings for a list of texts using the sentence transformer model \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Please call _load_model() first.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Embeddings generated successfully with shape {embeddings.shape}\")\n",
    "\n",
    "        return embeddings            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abe0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully whose embedding dimension is 384\n"
     ]
    }
   ],
   "source": [
    "### initialise Embedding\n",
    "\n",
    "embedding_manager = EmbeddingManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5639f",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cde1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\" Manages document embeddings in vector database\"\"\"\n",
    "    def __init__(self, collection_name:str = \"pdf-documents\", persistent_directory = \"../data/vector_store\"):\n",
    "        \"\"\" Initialize the vector store with a collection name and persistent directory \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\" Initialize the chroma client \"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persistent_directory)\n",
    "            self.collection = self.client.get_or_create_collection(name = self.collection_name,\n",
    "                metadata = {\"Description\": \"This is a collection of documents for RAG\",\n",
    "                \"hnsw:space\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "            print(f\"Vector store initialized successfully with collection {self.collection_name}\")\n",
    "            print(f\"count of existing document in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        \"\"\" Add documents to the vector store \n",
    "        Arguments: \n",
    "            documents: List of langchain documents\n",
    "            embeddings: numpy array of their embeddings\n",
    "        \"\"\"\n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"Number of documents should match with number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        ## Prepare data to ingest in chroma db\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embedding_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            ## Generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            ## Prepare metadata\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['contect_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            ## Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            ## Embedding\n",
    "\n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "        ## Add to collection\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids = ids,\n",
    "                documents = documents_text,\n",
    "                metadatas = metadatas,\n",
    "                embeddings = embedding_list\n",
    "            )\n",
    "            print(f\"{len(documents)} documents added successfully to the vector store\")\n",
    "            print(f\"total documents in the collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to the vector store: {e}\")\n",
    "            raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53c2b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized successfully with collection pdf-documents\n",
      "count of existing document in collection: 0\n"
     ]
    }
   ],
   "source": [
    "vectorstore = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53031f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1377e4d70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c17ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='XGBoost: A Scalable Tree Boosting System\\nTianqi Chen\\nUniversity of Washington\\ntqchen@cs.washington.edu\\nCarlos Guestrin\\nUniversity of Washington\\nguestrin@cs.washington.edu\\nABSTRACT\\nTree boosting is a highly e\ufb00ective and widely used machine\\nlearning method. In this paper, we describe a scalable end-\\nto-end tree boosting system called XGBoost, which is used\\nwidely by data scientists to achieve state-of-the-art results\\non many machine learning challenges. We propose a novel\\nsparsity-aware algorithm for sparse data and weighted quan-\\ntile sketch for approximate tree learning. More importantly,\\nwe provide insights on cache access patterns, data compres-\\nsion and sharding to build a scalable tree boosting system.\\nBy combining these insights, XGBoost scales beyond billions\\nof examples using far fewer resources than existing systems.\\nKeywords\\nLarge-scale Machine Learning\\n1. INTRODUCTION\\nMachine learning and data-driven approaches are becom-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='of examples using far fewer resources than existing systems.\\nKeywords\\nLarge-scale Machine Learning\\n1. INTRODUCTION\\nMachine learning and data-driven approaches are becom-\\ning very important in many areas. Smart spam classi\ufb01ers\\nprotect our email by learning from massive amounts of spam\\ndata and user feedback; advertising systems learn to match\\nthe right ads with the right context; fraud detection systems\\nprotect banks from malicious attackers; anomaly event de-\\ntection systems help experimental physicists to \ufb01nd events\\nthat lead to new physics. There are two important factors\\nthat drive these successful applications: usage of e\ufb00ective\\n(statistical) models that capture the complex data depen-\\ndencies and scalable learning systems that learn the model\\nof interest from large datasets.\\nAmong the machine learning methods used in practice,\\ngradient tree boosting [10] 1 is one technique that shines\\nin many applications. Tree boosting has been shown to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='of interest from large datasets.\\nAmong the machine learning methods used in practice,\\ngradient tree boosting [10] 1 is one technique that shines\\nin many applications. Tree boosting has been shown to\\ngive state-of-the-art results on many standard classi\ufb01cation\\nbenchmarks [16]. LambdaMART [5], a variant of tree boost-\\ning for ranking, achieves state-of-the-art result for ranking\\n1Gradient tree boosting is also known as gradient boosting\\nmachine (GBM) or gradient boosted regression tree (GBRT)\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\\non the \ufb01rst page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD \u201916, August 13-17, 2016, San Francisco, CA, USA\\nc\u20dd2016 Copyright held by the owner/author(s).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='For all other uses, contact the owner/author(s).\\nKDD \u201916, August 13-17, 2016, San Francisco, CA, USA\\nc\u20dd2016 Copyright held by the owner/author(s).\\nACM ISBN .\\nDOI:\\nproblems. Besides being used as a stand-alone predictor, it\\nis also incorporated into real-world production pipelines for\\nad click through rate prediction [15]. Finally, it is the de-\\nfacto choice of ensemble method and is used in challenges\\nsuch as the Net\ufb02ix prize [3].\\nIn this paper, we describe XGBoost, a scalable machine\\nlearning system for tree boosting. The system is available as\\nan open source package2. The impact of the system has been\\nwidely recognized in a number of machine learning and data\\nmining challenges. Take the challenges hosted by the ma-\\nchine learning competition site Kaggle for example. Among\\nthe 29 challenge winning solutions 3 published at Kaggle\u2019s\\nblog during 2015, 17 solutions used XGBoost. Among these\\nsolutions, eight solely used XGBoost to train the model,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='the 29 challenge winning solutions 3 published at Kaggle\u2019s\\nblog during 2015, 17 solutions used XGBoost. Among these\\nsolutions, eight solely used XGBoost to train the model,\\nwhile most others combined XGBoost with neural nets in en-\\nsembles. For comparison, the second most popular method,\\ndeep neural nets, was used in 11 solutions. The success\\nof the system was also witnessed in KDDCup 2015, where\\nXGBoost was used by every winning team in the top-10.\\nMoreover, the winning teams reported that ensemble meth-\\nods outperform a well-con\ufb01gured XGBoost by only a small\\namount [1].\\nThese results demonstrate that our system gives state-of-\\nthe-art results on a wide range of problems. Examples of\\nthe problems in these winning solutions include: store sales\\nprediction; high energy physics event classi\ufb01cation; web text\\nclassi\ufb01cation; customer behavior prediction; motion detec-\\ntion; ad click through rate prediction; malware classi\ufb01cation;\\nproduct categorization; hazard risk prediction; massive on-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='classi\ufb01cation; customer behavior prediction; motion detec-\\ntion; ad click through rate prediction; malware classi\ufb01cation;\\nproduct categorization; hazard risk prediction; massive on-\\nline course dropout rate prediction. While domain depen-\\ndent data analysis and feature engineering play an important\\nrole in these solutions, the fact that XGBoost is the consen-\\nsus choice of learner shows the impact and importance of\\nour system and tree boosting.\\nThe most important factor behind the success of XGBoost\\nis its scalability in all scenarios. The system runs more than\\nten times faster than existing popular solutions on a single\\nmachine and scales to billions of examples in distributed or\\nmemory-limited settings. The scalability of XGBoost is due\\nto several important systems and algorithmic optimizations.\\nThese innovations include: a novel tree learning algorithm\\nis for handling sparse data; a theoretically justi\ufb01ed weighted\\nquantile sketch procedure enables handling instance weights'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='These innovations include: a novel tree learning algorithm\\nis for handling sparse data; a theoretically justi\ufb01ed weighted\\nquantile sketch procedure enables handling instance weights\\nin approximate tree learning. Parallel and distributed com-\\nputing makes learning faster which enables quicker model ex-\\nploration. More importantly, XGBoost exploits out-of-core\\n2https://github.com/dmlc/xgboost\\n3Solutions come from of top-3 teams of each competitions.\\narXiv:1603.02754v3  [cs.LG]  10 Jun 2016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='computation and enables data scientists to process hundred\\nmillions of examples on a desktop. Finally, it is even more\\nexciting to combine these techniques to make an end-to-end\\nsystem that scales to even larger data with the least amount\\nof cluster resources. The major contributions of this paper\\nis listed as follows:\\n\u2022We design and build a highly scalable end-to-end tree\\nboosting system.\\n\u2022We propose a theoretically justi\ufb01ed weighted quantile\\nsketch for e\ufb03cient proposal calculation.\\n\u2022We introduce a novel sparsity-aware algorithm for par-\\nallel tree learning.\\n\u2022We propose an e\ufb00ective cache-aware block structure\\nfor out-of-core tree learning.\\nWhile there are some existing works on parallel tree boost-\\ning [22, 23, 19], the directions such as out-of-core compu-\\ntation, cache-aware and sparsity-aware learning have not\\nbeen explored. More importantly, an end-to-end system\\nthat combines all of these aspects gives a novel solution for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='tation, cache-aware and sparsity-aware learning have not\\nbeen explored. More importantly, an end-to-end system\\nthat combines all of these aspects gives a novel solution for\\nreal-world use-cases. This enables data scientists as well as\\nresearchers to build powerful variants of tree boosting al-\\ngorithms [7, 8]. Besides these major contributions, we also\\nmake additional improvements in proposing a regularized\\nlearning objective, which we will include for completeness.\\nThe remainder of the paper is organized as follows. We\\nwill \ufb01rst review tree boosting and introduce a regularized\\nobjective in Sec. 2. We then describe the split \ufb01nding meth-\\nods in Sec. 3 as well as the system design in Sec. 4, including\\nexperimental results when relevant to provide quantitative\\nsupport for each optimization we describe. Related work\\nis discussed in Sec. 5. Detailed end-to-end evaluations are\\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7.\\n2. TREE BOOSTING IN A NUTSHELL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='is discussed in Sec. 5. Detailed end-to-end evaluations are\\nincluded in Sec. 6. Finally we conclude the paper in Sec. 7.\\n2. TREE BOOSTING IN A NUTSHELL\\nWe review gradient tree boosting algorithms in this sec-\\ntion. The derivation follows from the same idea in existing\\nliteratures in gradient boosting. Specicially the second order\\nmethod is originated from Friedman et al. [12]. We make mi-\\nnor improvements in the reguralized objective, which were\\nfound helpful in practice.\\n2.1 Regularized Learning Objective\\nFor a given data set with n examples and m features\\nD= {(xi,yi)}(|D|= n,xi \u2208Rm,yi \u2208R), a tree ensem-\\nble model (shown in Fig. 1) uses K additive functions to\\npredict the output.\\n\u02c6yi = \u03c6(xi) =\\nK\u2211\\nk=1\\nfk(xi), fk \u2208F, (1)\\nwhere F = {f(x) = wq(x)}(q : Rm \u2192T,w \u2208RT) is the\\nspace of regression trees (also known as CART). Here q rep-\\nresents the structure of each tree that maps an example to\\nthe corresponding leaf index. T is the number of leaves in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='space of regression trees (also known as CART). Here q rep-\\nresents the structure of each tree that maps an example to\\nthe corresponding leaf index. T is the number of leaves in the\\ntree. Each fk corresponds to an independent tree structure\\nq and leaf weights w. Unlike decision trees, each regression\\ntree contains a continuous score on each of the leaf, we use\\nwi to represent score on i-th leaf. For a given example, we\\nwill use the decision rules in the trees (given byq) to classify\\nFigure 1: Tree Ensemble Model. The \ufb01nal predic-\\ntion for a given example is the sum of predictions\\nfrom each tree.\\nit into the leaves and calculate the \ufb01nal prediction by sum-\\nming up the score in the corresponding leaves (given by w).\\nTo learn the set of functions used in the model, we minimize\\nthe following regularized objective.\\nL(\u03c6) =\\n\u2211\\ni\\nl(\u02c6yi,yi) +\\n\u2211\\nk\\n\u2126(fk)\\nwhere \u2126(f) = \u03b3T + 1\\n2\u03bb\u2225w\u22252\\n(2)\\nHere l is a di\ufb00erentiable convex loss function that measures'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='the following regularized objective.\\nL(\u03c6) =\\n\u2211\\ni\\nl(\u02c6yi,yi) +\\n\u2211\\nk\\n\u2126(fk)\\nwhere \u2126(f) = \u03b3T + 1\\n2\u03bb\u2225w\u22252\\n(2)\\nHere l is a di\ufb00erentiable convex loss function that measures\\nthe di\ufb00erence between the prediction \u02c6yi and the target yi.\\nThe second term \u2126 penalizes the complexity of the model\\n(i.e., the regression tree functions). The additional regular-\\nization term helps to smooth the \ufb01nal learnt weights to avoid\\nover-\ufb01tting. Intuitively, the regularized objective will tend\\nto select a model employing simple and predictive functions.\\nA similar regularization technique has been used in Regu-\\nlarized greedy forest (RGF) [25] model. Our objective and\\nthe corresponding learning algorithm is simpler than RGF\\nand easier to parallelize. When the regularization parame-\\nter is set to zero, the objective falls back to the traditional\\ngradient tree boosting.\\n2.2 Gradient Tree Boosting\\nThe tree ensemble model in Eq. (2) includes functions as\\nparameters and cannot be optimized using traditional opti-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='gradient tree boosting.\\n2.2 Gradient Tree Boosting\\nThe tree ensemble model in Eq. (2) includes functions as\\nparameters and cannot be optimized using traditional opti-\\nmization methods in Euclidean space. Instead, the model\\nis trained in an additive manner. Formally, let \u02c6 y(t)\\ni be the\\nprediction of the i-th instance at the t-th iteration, we will\\nneed to add ft to minimize the following objective.\\nL(t) =\\nn\u2211\\ni=1\\nl(yi, \u02c6yi\\n(t\u22121) + ft(xi)) + \u2126(ft)\\nThis means we greedily add the ft that most improves our\\nmodel according to Eq. (2). Second-order approximation\\ncan be used to quickly optimize the objective in the general\\nsetting [12].\\nL(t) \u2243\\nn\u2211\\ni=1\\n[l(yi,\u02c6y(t\u22121)) + gift(xi) + 1\\n2hif2\\nt(xi)] + \u2126(ft)\\nwhere gi = \u2202\u02c6y(t\u22121) l(yi,\u02c6y(t\u22121)) and hi = \u22022\\n\u02c6y(t\u22121) l(yi,\u02c6y(t\u22121))\\nare \ufb01rst and second order gradient statistics on the loss func-\\ntion. We can remove the constant terms to obtain the fol-\\nlowing simpli\ufb01ed objective at step t.\\n\u02dcL(t) =\\nn\u2211\\ni=1\\n[gift(xi) + 1\\n2hif2\\nt(xi)] + \u2126(ft) (3)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Structure Score Calculation. We only\\nneed to sum up the gradient and second order gra-\\ndient statistics on each leaf, then apply the scoring\\nformula to get the quality score.\\nDe\ufb01ne Ij = {i|q(xi) = j}as the instance set of leaf j. We\\ncan rewrite Eq (3) by expanding \u2126 as follows\\n\u02dcL(t) =\\nn\u2211\\ni=1\\n[gift(xi) + 1\\n2hif2\\nt(xi)] + \u03b3T + 1\\n2\u03bb\\nT\u2211\\nj=1\\nw2\\nj\\n=\\nT\u2211\\nj=1\\n[(\\n\u2211\\ni\u2208Ij\\ngi)wj + 1\\n2(\\n\u2211\\ni\u2208Ij\\nhi + \u03bb)w2\\nj] + \u03b3T\\n(4)\\nFor a \ufb01xed structure q(x), we can compute the optimal\\nweight w\u2217\\nj of leaf j by\\nw\u2217\\nj = \u2212\\n\u2211\\ni\u2208Ij\\ngi\\n\u2211\\ni\u2208Ij\\nhi + \u03bb, (5)\\nand calculate the corresponding optimal value by\\n\u02dcL(t)(q) = \u22121\\n2\\nT\u2211\\nj=1\\n(\u2211\\ni\u2208Ij\\ngi)2\\n\u2211\\ni\u2208Ij\\nhi + \u03bb + \u03b3T. (6)\\nEq (6) can be used as a scoring function to measure the\\nquality of a tree structure q. This score is like the impurity\\nscore for evaluating decision trees, except that it is derived\\nfor a wider range of objective functions. Fig. 2 illustrates\\nhow this score can be calculated.\\nNormally it is impossible to enumerate all the possible'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='for a wider range of objective functions. Fig. 2 illustrates\\nhow this score can be calculated.\\nNormally it is impossible to enumerate all the possible\\ntree structures q. A greedy algorithm that starts from a\\nsingle leaf and iteratively adds branches to the tree is used\\ninstead. Assume that IL and IR are the instance sets of left\\nand right nodes after the split. Lettting I = IL \u222aIR, then\\nthe loss reduction after the split is given by\\nLsplit = 1\\n2\\n[\\n(\u2211\\ni\u2208IL\\ngi)2\\n\u2211\\ni\u2208IL\\nhi + \u03bb +\\n(\u2211\\ni\u2208IR\\ngi)2\\n\u2211\\ni\u2208IR\\nhi + \u03bb \u2212 (\u2211\\ni\u2208I gi)2\\n\u2211\\ni\u2208I hi + \u03bb\\n]\\n\u2212\u03b3\\n(7)\\nThis formula is usually used in practice for evaluating the\\nsplit candidates.\\n2.3 Shrinkage and Column Subsampling\\nBesides the regularized objective mentioned in Sec. 2.1,\\ntwo additional techniques are used to further prevent over-\\n\ufb01tting. The \ufb01rst technique is shrinkage introduced by Fried-\\nman [11]. Shrinkage scales newly added weights by a factor\\n\u03b7 after each step of tree boosting. Similar to a learning rate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='\ufb01tting. The \ufb01rst technique is shrinkage introduced by Fried-\\nman [11]. Shrinkage scales newly added weights by a factor\\n\u03b7 after each step of tree boosting. Similar to a learning rate\\nin tochastic optimization, shrinkage reduces the in\ufb02uence of\\neach individual tree and leaves space for future trees to im-\\nprove the model. The second technique is column (feature)\\nsubsampling. This technique is used in RandomForest [4,\\nAlgorithm 1: Exact Greedy Algorithm for Split Finding\\nInput: I, instance set of current node\\nInput: d, feature dimension\\ngain\u21900\\nG\u2190\u2211\\ni\u2208I gi, H \u2190\u2211\\ni\u2208I hi\\nfor k= 1 to m do\\nGL \u21900, HL \u21900\\nfor j in sorted(I, by xjk) do\\nGL \u2190GL + gj, HL \u2190HL + hj\\nGR \u2190G\u2212GL, HR \u2190H\u2212HL\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\nend\\nOutput: Split with max score\\nAlgorithm 2: Approximate Algorithm for Split Finding\\nfor k= 1 to m do\\nPropose Sk = {sk1,sk2,\u00b7\u00b7\u00b7skl}by percentiles on feature k.\\nProposal can be done per tree (global), or per split(local).\\nend\\nfor k= 1 to m do\\nGkv \u2190= \u2211'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='for k= 1 to m do\\nPropose Sk = {sk1,sk2,\u00b7\u00b7\u00b7skl}by percentiles on feature k.\\nProposal can be done per tree (global), or per split(local).\\nend\\nfor k= 1 to m do\\nGkv \u2190= \u2211\\nj\u2208{j|sk,v\u2265xjk>sk,v\u22121}gj\\nHkv \u2190= \u2211\\nj\u2208{j|sk,v\u2265xjk>sk,v\u22121}hj\\nend\\nFollow same step as in previous section to \ufb01nd max\\nscore only among proposed splits.\\n13], It is implemented in a commercial software TreeNet 4\\nfor gradient boosting, but is not implemented in existing\\nopensource packages. According to user feedback, using col-\\numn sub-sampling prevents over-\ufb01tting even more so than\\nthe traditional row sub-sampling (which is also supported).\\nThe usage of column sub-samples also speeds up computa-\\ntions of the parallel algorithm described later.\\n3. SPLIT FINDING ALGORITHMS\\n3.1 Basic Exact Greedy Algorithm\\nOne of the key problems in tree learning is to \ufb01nd the\\nbest split as indicated by Eq (7). In order to do so, a split\\n\ufb01nding algorithm enumerates over all the possible splits on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='One of the key problems in tree learning is to \ufb01nd the\\nbest split as indicated by Eq (7). In order to do so, a split\\n\ufb01nding algorithm enumerates over all the possible splits on\\nall the features. We call this the exact greedy algorithm.\\nMost existing single machine tree boosting implementations,\\nsuch as scikit-learn [20], R\u2019s gbm [21] as well as the single\\nmachine version of XGBoost support the exact greedy algo-\\nrithm. The exact greedy algorithm is shown in Alg. 1. It\\nis computationally demanding to enumerate all the possible\\nsplits for continuous features. In order to do so e\ufb03ciently,\\nthe algorithm must \ufb01rst sort the data according to feature\\nvalues and visit the data in sorted order to accumulate the\\ngradient statistics for the structure score in Eq (7).\\n3.2 Approximate Algorithm\\nThe exact greedy algorithm is very powerful since it enu-\\nmerates over all possible splitting points greedily. However,\\nit is impossible to e\ufb03ciently do so when the data does not \ufb01t'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='The exact greedy algorithm is very powerful since it enu-\\nmerates over all possible splitting points greedily. However,\\nit is impossible to e\ufb03ciently do so when the data does not \ufb01t\\nentirely into memory. Same problem also arises in the dis-\\n4https://www.salford-systems.com/products/treenet'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='0 10 20 30 40 50 60 70 80 90\\nNumber of Iterations\\n0.75\\n0.76\\n0.77\\n0.78\\n0.79\\n0.80\\n0.81\\n0.82\\n0.83\\nTest AUC\\nexact greedy\\nglobal eps=0.3\\nlocal eps=0.3\\nglobal eps=0.05\\nFigure 3: Comparison of test AUC convergence on\\nHiggs 10M dataset. The eps parameter corresponds\\nto the accuracy of the approximate sketch. This\\nroughly translates to 1 / eps buckets in the proposal.\\nWe \ufb01nd that local proposals require fewer buckets,\\nbecause it re\ufb01ne split candidates.\\ntributed setting. To support e\ufb00ective gradient tree boosting\\nin these two settings, an approximate algorithm is needed.\\nWe summarize an approximate framework, which resem-\\nbles the ideas proposed in past literatures [17, 2, 22], in\\nAlg. 2. To summarize, the algorithm \ufb01rst proposes can-\\ndidate splitting points according to percentiles of feature\\ndistribution (a speci\ufb01c criteria will be given in Sec. 3.3).\\nThe algorithm then maps the continuous features into buck-\\nets split by these candidate points, aggregates the statistics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='distribution (a speci\ufb01c criteria will be given in Sec. 3.3).\\nThe algorithm then maps the continuous features into buck-\\nets split by these candidate points, aggregates the statistics\\nand \ufb01nds the best solution among proposals based on the\\naggregated statistics.\\nThere are two variants of the algorithm, depending on\\nwhen the proposal is given. The global variant proposes all\\nthe candidate splits during the initial phase of tree construc-\\ntion, and uses the same proposals for split \ufb01nding at all lev-\\nels. The local variant re-proposes after each split. The global\\nmethod requires less proposal steps than the local method.\\nHowever, usually more candidate points are needed for the\\nglobal proposal because candidates are not re\ufb01ned after each\\nsplit. The local proposal re\ufb01nes the candidates after splits,\\nand can potentially be more appropriate for deeper trees. A\\ncomparison of di\ufb00erent algorithms on a Higgs boson dataset\\nis given by Fig. 3. We \ufb01nd that the local proposal indeed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='and can potentially be more appropriate for deeper trees. A\\ncomparison of di\ufb00erent algorithms on a Higgs boson dataset\\nis given by Fig. 3. We \ufb01nd that the local proposal indeed\\nrequires fewer candidates. The global proposal can be as\\naccurate as the local one given enough candidates.\\nMost existing approximate algorithms for distributed tree\\nlearning also follow this framework. Notably, it is also possi-\\nble to directly construct approximate histograms of gradient\\nstatistics [22]. It is also possible to use other variants of bin-\\nning strategies instead of quantile [17]. Quantile strategy\\nbene\ufb01t from being distributable and recomputable, which\\nwe will detail in next subsection. From Fig. 3, we also \ufb01nd\\nthat the quantile strategy can get the same accuracy as exact\\ngreedy given reasonable approximation level.\\nOur system e\ufb03ciently supports exact greedy for the single\\nmachine setting, as well as approximate algorithm with both\\nlocal and global proposal methods for all settings. Users can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Our system e\ufb03ciently supports exact greedy for the single\\nmachine setting, as well as approximate algorithm with both\\nlocal and global proposal methods for all settings. Users can\\nfreely choose between the methods according to their needs.\\n3.3 Weighted Quantile Sketch\\nOne important step in the approximate algorithm is to\\npropose candidate split points. Usually percentiles of a fea-\\nFigure 4: Tree structure with default directions. An\\nexample will be classi\ufb01ed into the default direction\\nwhen the feature needed for the split is missing.\\nture are used to make candidates distribute evenly on the\\ndata. Formally, let multi-setDk = {(x1k,h1),(x2k,h2) \u00b7\u00b7\u00b7(xnk,hn)}\\nrepresent the k-th feature values and second order gradient\\nstatistics of each training instances. We can de\ufb01ne a rank\\nfunctions rk : R \u2192[0,+\u221e) as\\nrk(z) = 1\u2211\\n(x,h)\u2208Dk\\nh\\n\u2211\\n(x,h)\u2208Dk,x<z\\nh, (8)\\nwhich represents the proportion of instances whose feature\\nvalue k is smaller than z. The goal is to \ufb01nd candidate split'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='functions rk : R \u2192[0,+\u221e) as\\nrk(z) = 1\u2211\\n(x,h)\u2208Dk\\nh\\n\u2211\\n(x,h)\u2208Dk,x<z\\nh, (8)\\nwhich represents the proportion of instances whose feature\\nvalue k is smaller than z. The goal is to \ufb01nd candidate split\\npoints {sk1,sk2,\u00b7\u00b7\u00b7skl}, such that\\n|rk(sk,j) \u2212rk(sk,j+1)|<\u03f5, s k1 = min\\ni\\nxik,skl = max\\ni\\nxik.\\n(9)\\nHere \u03f5 is an approximation factor. Intuitively, this means\\nthat there is roughly 1 /\u03f5 candidate points. Here each data\\npoint is weighted byhi. To see why hi represents the weight,\\nwe can rewrite Eq (3) as\\nn\u2211\\ni=1\\n1\\n2hi(ft(xi) \u2212gi/hi)2 + \u2126(ft) + constant,\\nwhich is exactly weighted squared loss with labels gi/hi\\nand weights hi. For large datasets, it is non-trivial to \ufb01nd\\ncandidate splits that satisfy the criteria. When every in-\\nstance has equal weights, an existing algorithm called quan-\\ntile sketch [14, 24] solves the problem. However, there is no\\nexisting quantile sketch for the weighted datasets. There-\\nfore, most existing approximate algorithms either resorted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='tile sketch [14, 24] solves the problem. However, there is no\\nexisting quantile sketch for the weighted datasets. There-\\nfore, most existing approximate algorithms either resorted\\nto sorting on a random subset of data which have a chance of\\nfailure or heuristics that do not have theoretical guarantee.\\nTo solve this problem, we introduced a novel distributed\\nweighted quantile sketch algorithm that can handle weighted\\ndata with a provable theoretical guarantee. The general idea\\nis to propose a data structure that supportsmerge and prune\\noperations, with each operation proven to maintain a certain\\naccuracy level. A detailed description of the algorithm as\\nwell as proofs are given in the appendix.\\n3.4 Sparsity-aware Split Finding\\nIn many real-world problems, it is quite common for the\\ninput x to be sparse. There are multiple possible causes\\nfor sparsity: 1) presence of missing values in the data; 2)\\nfrequent zero entries in the statistics; and, 3) artifacts of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='input x to be sparse. There are multiple possible causes\\nfor sparsity: 1) presence of missing values in the data; 2)\\nfrequent zero entries in the statistics; and, 3) artifacts of\\nfeature engineering such as one-hot encoding. It is impor-\\ntant to make the algorithm aware of the sparsity pattern in\\nthe data. In order to do so, we propose to add a default\\ndirection in each tree node, which is shown in Fig. 4. When\\na value is missing in the sparse matrix x, the instance is\\nclassi\ufb01ed into the default direction. There are two choices'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Figure 6: Block structure for parallel learning. Each column in a block is sorted by the corresponding feature\\nvalue. A linear scan over one column in the block is su\ufb03cient to enumerate all the split points.\\nAlgorithm 3: Sparsity-aware Split Finding\\nInput: I, instance set of current node\\nInput: Ik = {i\u2208I|xik \u0338= missing}\\nInput: d, feature dimension\\nAlso applies to the approximate setting, only collect\\nstatistics of non-missing entries into buckets\\ngain\u21900\\nG\u2190\u2211\\ni\u2208I,gi,H \u2190\u2211\\ni\u2208I hi\\nfor k= 1 to m do\\n// enumerate missing value goto right\\nGL \u21900, HL \u21900\\nfor j in sorted(Ik, ascent order byxjk) do\\nGL \u2190GL + gj, HL \u2190HL + hj\\nGR \u2190G\u2212GL, HR \u2190H\u2212HL\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\n// enumerate missing value goto left\\nGR \u21900, HR \u21900\\nfor j in sorted(Ik, descent order byxjk) do\\nGR \u2190GR + gj, HR \u2190HR + hj\\nGL \u2190G\u2212GR, HL \u2190H\u2212HR\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\nend\\nOutput: Split and default directions with max gain\\nof default direction in each branch. The optimal default di-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='GL \u2190G\u2212GR, HL \u2190H\u2212HR\\nscore\u2190max(score,\\nG2\\nL\\nHL+\u03bb +\\nG2\\nR\\nHR+\u03bb \u2212 G2\\nH+\u03bb)\\nend\\nend\\nOutput: Split and default directions with max gain\\nof default direction in each branch. The optimal default di-\\nrections are learnt from the data. The algorithm is shown in\\nAlg. 3. The key improvement is to only visit the non-missing\\nentries Ik. The presented algorithm treats the non-presence\\nas a missing value and learns the best direction to handle\\nmissing values. The same algorithm can also be applied\\nwhen the non-presence corresponds to a user speci\ufb01ed value\\nby limiting the enumeration only to consistent solutions.\\nTo the best of our knowledge, most existing tree learn-\\ning algorithms are either only optimized for dense data, or\\nneed speci\ufb01c procedures to handle limited cases such as cat-\\negorical encoding. XGBoost handles all sparsity patterns in\\na uni\ufb01ed way. More importantly, our method exploits the\\nsparsity to make computation complexity linear to number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='egorical encoding. XGBoost handles all sparsity patterns in\\na uni\ufb01ed way. More importantly, our method exploits the\\nsparsity to make computation complexity linear to number\\nof non-missing entries in the input. Fig. 5 shows the com-\\nparison of sparsity aware and a naive implementation on an\\nAllstate-10K dataset (description of dataset given in Sec. 6).\\nWe \ufb01nd that the sparsity aware algorithm runs 50 times\\nfaster than the naive version. This con\ufb01rms the importance\\nof the sparsity aware algorithm.\\n1 2 4 8 16\\nNumber of Threads\\n0.03125\\n0.0625\\n0.125\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\n16\\n32\\nTime per Tree(sec) Sparsity aware algorithm\\nBasic algorithm\\nFigure 5: Impact of the sparsity aware algorithm\\non Allstate-10K. The dataset is sparse mainly due\\nto one-hot encoding. The sparsity aware algorithm\\nis more than 50 times faster than the naive version\\nthat does not take sparsity into consideration.\\n4. SYSTEM DESIGN\\n4.1 Column Block for Parallel Learning\\nThe most time consuming part of tree learning is to get'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='that does not take sparsity into consideration.\\n4. SYSTEM DESIGN\\n4.1 Column Block for Parallel Learning\\nThe most time consuming part of tree learning is to get\\nthe data into sorted order. In order to reduce the cost of\\nsorting, we propose to store the data in in-memory units,\\nwhich we called block. Data in each block is stored in the\\ncompressed column (CSC) format, with each column sorted\\nby the corresponding feature value. This input data layout\\nonly needs to be computed once before training, and can be\\nreused in later iterations.\\nIn the exact greedy algorithm, we store the entire dataset\\nin a single block and run the split search algorithm by lin-\\nearly scanning over the pre-sorted entries. We do the split\\n\ufb01nding of all leaves collectively, so one scan over the block\\nwill collect the statistics of the split candidates in all leaf\\nbranches. Fig. 6 shows how we transform a dataset into the\\nformat and \ufb01nd the optimal split using the block structure.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='will collect the statistics of the split candidates in all leaf\\nbranches. Fig. 6 shows how we transform a dataset into the\\nformat and \ufb01nd the optimal split using the block structure.\\nThe block structure also helps when using the approxi-\\nmate algorithms. Multiple blocks can be used in this case,\\nwith each block corresponding to subset of rows in the dataset.\\nDi\ufb00erent blocks can be distributed across machines, or stored\\non disk in the out-of-core setting. Using the sorted struc-\\nture, the quantile \ufb01nding step becomes a linear scan over\\nthe sorted columns. This is especially valuable for local pro-\\nposal algorithms, where candidates are generated frequently\\nat each branch. The binary search in histogram aggregation\\nalso becomes a linear time merge style algorithm.\\nCollecting statistics for each column can be parallelized,\\ngiving us a parallel algorithm for split \ufb01nding. Importantly,\\nthe column block structure also supports column subsam-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Collecting statistics for each column can be parallelized,\\ngiving us a parallel algorithm for split \ufb01nding. Importantly,\\nthe column block structure also supports column subsam-\\npling, as it is easy to select a subset of columns in a block.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='1 2 4 8 16\\nNumber of Threads\\n8\\n16\\n32\\n64\\n128Time per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm\\n(a) Allstate 10M\\n1 2 4 8 16\\nNumber of Threads\\n8\\n16\\n32\\n64\\n128\\n256Time per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (b) Higgs 10M\\n1 2 4 8 16\\nNumber of Threads\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\nTime per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (c) Allstate 1M\\n1 2 4 8 16\\nNumber of Threads\\n0.25\\n0.5\\n1\\n2\\n4\\n8\\nTime per Tree(sec)\\nBasic algorithm\\nCache-aware algorithm (d) Higgs 1M\\nFigure 7: Impact of cache-aware prefetching in exact greedy algorithm. We \ufb01nd that the cache-miss e\ufb00ect\\nimpacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves\\nthe performance by factor of two when the dataset is large.\\nFigure 8: Short range data dependency pattern\\nthat can cause stall due to cache miss.\\nTime Complexity AnalysisLet dbe the maximum depth\\nof the tree and K be total number of trees. For the ex-\\nact greedy algorithm, the time complexity of original spase'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Time Complexity AnalysisLet dbe the maximum depth\\nof the tree and K be total number of trees. For the ex-\\nact greedy algorithm, the time complexity of original spase\\naware algorithm is O(Kd\u2225x\u22250 log n). Here we use \u2225x\u22250 to\\ndenote number of non-missing entries in the training data.\\nOn the other hand, tree boosting on the block structure only\\ncost O(Kd\u2225x\u22250 + \u2225x\u22250 log n). Here O(\u2225x\u22250 log n) is the one\\ntime preprocessing cost that can be amortized. This analysis\\nshows that the block structure helps to save an additional\\nlog n factor, which is signi\ufb01cant when n is large. For the\\napproximate algorithm, the time complexity of original al-\\ngorithm with binary search is O(Kd\u2225x\u22250 log q). Here q is\\nthe number of proposal candidates in the dataset. While q\\nis usually between 32 and 100, the log factor still introduces\\noverhead. Using the block structure, we can reduce the time\\nto O(Kd\u2225x\u22250 + \u2225x\u22250 log B), where B is the maximum num-\\nber of rows in each block. Again we can save the additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='overhead. Using the block structure, we can reduce the time\\nto O(Kd\u2225x\u22250 + \u2225x\u22250 log B), where B is the maximum num-\\nber of rows in each block. Again we can save the additional\\nlog q factor in computation.\\n4.2 Cache-aware Access\\nWhile the proposed block structure helps optimize the\\ncomputation complexity of split \ufb01nding, the new algorithm\\nrequires indirect fetches of gradient statistics by row index,\\nsince these values are accessed in order of feature. This is\\na non-continuous memory access. A naive implementation\\nof split enumeration introduces immediate read/write de-\\npendency between the accumulation and the non-continuous\\nmemory fetch operation (see Fig. 8). This slows down split\\n\ufb01nding when the gradient statistics do not \ufb01t into CPU cache\\nand cache miss occur.\\nFor the exact greedy algorithm, we can alleviate the prob-\\nlem by a cache-aware prefetching algorithm. Speci\ufb01cally,\\nwe allocate an internal bu\ufb00er in each thread, fetch the gra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='and cache miss occur.\\nFor the exact greedy algorithm, we can alleviate the prob-\\nlem by a cache-aware prefetching algorithm. Speci\ufb01cally,\\nwe allocate an internal bu\ufb00er in each thread, fetch the gra-\\ndient statistics into it, and then perform accumulation in\\na mini-batch manner. This prefetching changes the direct\\nread/write dependency to a longer dependency and helps to\\nreduce the runtime overhead when number of rows in the\\nis large. Figure 7 gives the comparison of cache-aware vs.\\n1 2 4 8 16\\nNumber of Threads\\n4\\n8\\n16\\n32\\n64\\n128Time per Tree(sec)\\nblock size=2^12\\nblock size=2^16\\nblock size=2^20\\nblock size=2^24\\n(a) Allstate 10M\\n1 2 4 8 16\\nNumber of Threads\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512Time per Tree(sec)\\nblock size=2^12\\nblock size=2^16\\nblock size=2^20\\nblock size=2^24\\n(b) Higgs 10M\\nFigure 9: The impact of block size in the approxi-\\nmate algorithm. We \ufb01nd that overly small blocks re-\\nsults in ine\ufb03cient parallelization, while overly large\\nblocks also slows down training due to cache misses.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='mate algorithm. We \ufb01nd that overly small blocks re-\\nsults in ine\ufb03cient parallelization, while overly large\\nblocks also slows down training due to cache misses.\\nnon cache-aware algorithm on the the Higgs and the All-\\nstate dataset. We \ufb01nd that cache-aware implementation of\\nthe exact greedy algorithm runs twice as fast as the naive\\nversion when the dataset is large.\\nFor approximate algorithms, we solve the problem by choos-\\ning a correct block size. We de\ufb01ne the block size to be max-\\nimum number of examples in contained in a block, as this\\nre\ufb02ects the cache storage cost of gradient statistics. Choos-\\ning an overly small block size results in small workload for\\neach thread and leads to ine\ufb03cient parallelization. On the\\nother hand, overly large blocks result in cache misses, as the\\ngradient statistics do not \ufb01t into the CPU cache. A good\\nchoice of block size balances these two factors. We compared\\nvarious choices of block size on two data sets. The results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='gradient statistics do not \ufb01t into the CPU cache. A good\\nchoice of block size balances these two factors. We compared\\nvarious choices of block size on two data sets. The results\\nare given in Fig. 9. This result validates our discussion and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Table 1: Comparison of major tree boosting systems.\\nSystem exact\\ngreedy\\napproximate\\nglobal\\napproximate\\nlocal out-of-core sparsity\\naware parallel\\nXGBoost yes yes yes yes yes yes\\npGBRT no no yes no no yes\\nSpark MLLib no yes no no partially yes\\nH2O no yes no no partially yes\\nscikit-learn yes no no no no no\\nR GBM yes no no no partially no\\nshows that choosing 2 16 examples per block balances the\\ncache property and parallelization.\\n4.3 Blocks for Out-of-core Computation\\nOne goal of our system is to fully utilize a machine\u2019s re-\\nsources to achieve scalable learning. Besides processors and\\nmemory, it is important to utilize disk space to handle data\\nthat does not \ufb01t into main memory. To enable out-of-core\\ncomputation, we divide the data into multiple blocks and\\nstore each block on disk. During computation, it is impor-\\ntant to use an independent thread to pre-fetch the block into\\na main memory bu\ufb00er, so computation can happen in con-\\ncurrence with disk reading. However, this does not entirely'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='tant to use an independent thread to pre-fetch the block into\\na main memory bu\ufb00er, so computation can happen in con-\\ncurrence with disk reading. However, this does not entirely\\nsolve the problem since the disk reading takes most of the\\ncomputation time. It is important to reduce the overhead\\nand increase the throughput of disk IO. We mainly use two\\ntechniques to improve the out-of-core computation.\\nBlock Compression The \ufb01rst technique we use is block\\ncompression. The block is compressed by columns, and de-\\ncompressed on the \ufb02y by an independent thread when load-\\ning into main memory. This helps to trade some of the\\ncomputation in decompression with the disk reading cost.\\nWe use a general purpose compression algorithm for com-\\npressing the features values. For the row index, we substract\\nthe row index by the begining index of the block and use a\\n16bit integer to store each o\ufb00set. This requires 216 examples\\nper block, which is con\ufb01rmed to be a good setting. In most'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='the row index by the begining index of the block and use a\\n16bit integer to store each o\ufb00set. This requires 216 examples\\nper block, which is con\ufb01rmed to be a good setting. In most\\nof the dataset we tested, we achieve roughly a 26% to 29%\\ncompression ratio.\\nBlock Sharding The second technique is to shard the data\\nonto multiple disks in an alternative manner. A pre-fetcher\\nthread is assigned to each disk and fetches the data into an\\nin-memory bu\ufb00er. The training thread then alternatively\\nreads the data from each bu\ufb00er. This helps to increase the\\nthroughput of disk reading when multiple disks are available.\\n5. RELATED WORKS\\nOur system implements gradient boosting [10], which per-\\nforms additive optimization in functional space. Gradient\\ntree boosting has been successfully used in classi\ufb01cation [12],\\nlearning to rank [5], structured prediction [8] as well as other\\n\ufb01elds. XGBoost incorporates a regularized model to prevent\\nover\ufb01tting. This this resembles previous work on regularized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='learning to rank [5], structured prediction [8] as well as other\\n\ufb01elds. XGBoost incorporates a regularized model to prevent\\nover\ufb01tting. This this resembles previous work on regularized\\ngreedy forest [25], but simpli\ufb01es the objective and algorithm\\nfor parallelization. Column sampling is a simple but e\ufb00ective\\ntechnique borrowed from RandomForest [4]. While sparsity-\\naware learning is essential in other types of models such as\\nlinear models [9], few works on tree learning have considered\\nthis topic in a principled way. The algorithm proposed in\\nthis paper is the \ufb01rst uni\ufb01ed approach to handle all kinds of\\nsparsity patterns.\\nThere are several existing works on parallelizing tree learn-\\ning [22, 19]. Most of these algorithms fall into the ap-\\nproximate framework described in this paper. Notably, it\\nis also possible to partition data by columns [23] and ap-\\nply the exact greedy algorithm. This is also supported in\\nour framework, and the techniques such as cache-aware pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='is also possible to partition data by columns [23] and ap-\\nply the exact greedy algorithm. This is also supported in\\nour framework, and the techniques such as cache-aware pre-\\nfecthing can be used to bene\ufb01t this type of algorithm. While\\nmost existing works focus on the algorithmic aspect of par-\\nallelization, our work improves in two unexplored system di-\\nrections: out-of-core computation and cache-aware learning.\\nThis gives us insights on how the system and the algorithm\\ncan be jointly optimized and provides an end-to-end system\\nthat can handle large scale problems with very limited com-\\nputing resources. We also summarize the comparison be-\\ntween our system and existing opensource implementations\\nin Table 1.\\nQuantile summary (without weights) is a classical prob-\\nlem in the database community [14, 24]. However, the ap-\\nproximate tree boosting algorithm reveals a more general\\nproblem \u2013 \ufb01nding quantiles on weighted data. To the best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='lem in the database community [14, 24]. However, the ap-\\nproximate tree boosting algorithm reveals a more general\\nproblem \u2013 \ufb01nding quantiles on weighted data. To the best\\nof our knowledge, the weighted quantile sketch proposed in\\nthis paper is the \ufb01rst method to solve this problem. The\\nweighted quantile summary is also not speci\ufb01c to the tree\\nlearning and can bene\ufb01t other applications in data science\\nand machine learning in the future.\\n6. END TO END EV ALUATIONS\\n6.1 System Implementation\\nWe implemented XGBoost as an open source package 5.\\nThe package is portable and reusable. It supports various\\nweighted classi\ufb01cation and rank objective functions, as well\\nas user de\ufb01ned objective function. It is available in popular\\nlanguages such as python, R, Julia and integrates naturally\\nwith language native data science pipelines such as scikit-\\nlearn. The distributed version is built on top of the rabit\\nlibrary6 for allreduce. The portability of XGBoost makes it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='with language native data science pipelines such as scikit-\\nlearn. The distributed version is built on top of the rabit\\nlibrary6 for allreduce. The portability of XGBoost makes it\\navailable in many ecosystems, instead of only being tied to\\na speci\ufb01c platform. The distributed XGBoost runs natively\\non Hadoop, MPI Sun Grid engine. Recently, we also enable\\ndistributed XGBoost on jvm bigdata stacks such as Flink\\nand Spark. The distributed version has also been integrated\\ninto cloud platform Tianchi 7 of Alibaba. We believe that\\nthere will be more integrations in the future.\\n6.2 Dataset and Setup\\n5https://github.com/dmlc/xgboost\\n6https://github.com/dmlc/rabit\\n7https://tianchi.aliyun.com'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Table 2: Dataset used in the Experiments.\\nDataset n m Task\\nAllstate 10 M 4227 Insurance claim classi\ufb01cation\\nHiggs Boson 10 M 28 Event classi\ufb01cation\\nYahoo LTRC 473K 700 Learning to Rank\\nCriteo 1.7 B 67 Click through rate prediction\\nWe used four datasets in our experiments. A summary of\\nthese datasets is given in Table 2. In some of the experi-\\nments, we use a randomly selected subset of the data either\\ndue to slow baselines or to demonstrate the performance of\\nthe algorithm with varying dataset size. We use a su\ufb03x to\\ndenote the size in these cases. For example Allstate-10K\\nmeans a subset of the Allstate dataset with 10K instances.\\nThe \ufb01rst dataset we use is the Allstate insurance claim\\ndataset8. The task is to predict the likelihood and cost of\\nan insurance claim given di\ufb00erent risk factors. In the exper-\\niment, we simpli\ufb01ed the task to only predict the likelihood\\nof an insurance claim. This dataset is used to evaluate the\\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='iment, we simpli\ufb01ed the task to only predict the likelihood\\nof an insurance claim. This dataset is used to evaluate the\\nimpact of sparsity-aware algorithm in Sec. 3.4. Most of the\\nsparse features in this data come from one-hot encoding. We\\nrandomly select 10M instances as training set and use the\\nrest as evaluation set.\\nThe second dataset is the Higgs boson dataset9 from high\\nenergy physics. The data was produced using Monte Carlo\\nsimulations of physics events. It contains 21 kinematic prop-\\nerties measured by the particle detectors in the accelerator.\\nIt also contains seven additional derived physics quantities\\nof the particles. The task is to classify whether an event\\ncorresponds to the Higgs boson. We randomly select 10M\\ninstances as training set and use the rest as evaluation set.\\nThe third dataset is the Yahoo! learning to rank challenge\\ndataset [6], which is one of the most commonly used bench-\\nmarks in learning to rank algorithms. The dataset contains'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='The third dataset is the Yahoo! learning to rank challenge\\ndataset [6], which is one of the most commonly used bench-\\nmarks in learning to rank algorithms. The dataset contains\\n20K web search queries, with each query corresponding to a\\nlist of around 22 documents. The task is to rank the docu-\\nments according to relevance of the query. We use the o\ufb03cial\\ntrain test split in our experiment.\\nThe last dataset is the criteo terabyte click log dataset 10.\\nWe use this dataset to evaluate the scaling property of the\\nsystem in the out-of-core and the distributed settings. The\\ndata contains 13 integer features and 26 ID features of user,\\nitem and advertiser information. Since a tree based model\\nis better at handling continuous features, we preprocess the\\ndata by calculating the statistics of average CTR and count\\nof ID features on the \ufb01rst ten days, replacing the ID fea-\\ntures by the corresponding count statistics during the next\\nten days for training. The training set after preprocessing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='of ID features on the \ufb01rst ten days, replacing the ID fea-\\ntures by the corresponding count statistics during the next\\nten days for training. The training set after preprocessing\\ncontains 1.7 billion instances with 67 features (13 integer, 26\\naverage CTR statistics and 26 counts). The entire dataset\\nis more than one terabyte in LibSVM format.\\nWe use the \ufb01rst three datasets for the single machine par-\\nallel setting, and the last dataset for the distributed and\\nout-of-core settings. All the single machine experiments are\\nconducted on a Dell PowerEdge R420 with two eight-core\\nIntel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If\\nnot speci\ufb01ed, all the experiments are run using all the avail-\\n8https://www.kaggle.com/c/ClaimPredictionChallenge\\n9https://archive.ics.uci.edu/ml/datasets/HIGGS\\n10http://labs.criteo.com/downloads/download-terabyte-\\nclick-logs/\\nTable 3: Comparison of Exact Greedy Methods with\\n500 trees on Higgs-1M data.\\nMethod Time per Tree (sec) Test AUC\\nXGBoost 0.6841 0.8304'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='click-logs/\\nTable 3: Comparison of Exact Greedy Methods with\\n500 trees on Higgs-1M data.\\nMethod Time per Tree (sec) Test AUC\\nXGBoost 0.6841 0.8304\\nXGBoost (colsample=0.5) 0.6401 0.8245\\nscikit-learn 28.51 0.8302\\nR.gbm 1.032 0.6224\\n1 2 4 8 16\\nNumber of Threads\\n0.5\\n1\\n2\\n4\\n8\\n16\\n32Time per Tree(sec)\\nXGBoost\\npGBRT\\nFigure 10: Comparison between XGBoost and pG-\\nBRT on Yahoo LTRC dataset.\\nTable 4: Comparison of Learning to Rank with 500\\ntrees on Yahoo! LTRC Dataset\\nMethod Time per Tree (sec) NDCG@10\\nXGBoost 0.826 0.7892\\nXGBoost (colsample=0.5) 0.506 0.7913\\npGBRT [22] 2.576 0.7915\\nable cores in the machine. The machine settings of the dis-\\ntributed and the out-of-core experiments will be described in\\nthe corresponding section. In all the experiments, we boost\\ntrees with a common setting of maximum depth equals 8,\\nshrinkage equals 0.1 and no column subsampling unless ex-\\nplicitly speci\ufb01ed. We can \ufb01nd similar results when we use\\nother settings of maximum depth.\\n6.3 Classi\ufb01cation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='shrinkage equals 0.1 and no column subsampling unless ex-\\nplicitly speci\ufb01ed. We can \ufb01nd similar results when we use\\nother settings of maximum depth.\\n6.3 Classi\ufb01cation\\nIn this section, we evaluate the performance of XGBoost\\non a single machine using the exact greedy algorithm on\\nHiggs-1M data, by comparing it against two other commonly\\nused exact greedy tree boosting implementations. Since\\nscikit-learn only handles non-sparse input, we choose the\\ndense Higgs dataset for a fair comparison. We use the 1M\\nsubset to make scikit-learn \ufb01nish running in reasonable time.\\nAmong the methods in comparison, R\u2019s GBM uses a greedy\\napproach that only expands one branch of a tree, which\\nmakes it faster but can result in lower accuracy, while both\\nscikit-learn and XGBoost learn a full tree. The results are\\nshown in Table 3. Both XGBoost and scikit-learn give better\\nperformance than R\u2019s GBM, while XGBoost runs more than\\n10x faster than scikit-learn. In this experiment, we also \ufb01nd'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='shown in Table 3. Both XGBoost and scikit-learn give better\\nperformance than R\u2019s GBM, while XGBoost runs more than\\n10x faster than scikit-learn. In this experiment, we also \ufb01nd\\ncolumn subsamples gives slightly worse performance than\\nusing all the features. This could due to the fact that there\\nare few important features in this dataset and we can bene\ufb01t\\nfrom greedily select from all the features.\\n6.4 Learning to Rank\\nWe next evaluate the performance of XGBoost on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='128 256 512 1024 2048\\nNumber of Training Examples (million)\\n128\\n256\\n512\\n1024\\n2048\\n4096Time per Tree(sec)\\nBasic algorithm\\nBlock compression\\nCompression+shard\\nOut of system file cache\\nstart from this point\\nFigure 11: Comparison of out-of-core methods on\\ndi\ufb00erent subsets of criteo data. The missing data\\npoints are due to out of disk space. We can \ufb01nd\\nthat basic algorithm can only handle 200M exam-\\nples. Adding compression gives 3x speedup, and\\nsharding into two disks gives another 2x speedup.\\nThe system runs out of \ufb01le cache start from 400M\\nexamples. The algorithm really has to rely on disk\\nafter this point. The compression+shard method\\nhas a less dramatic slowdown when running out of\\n\ufb01le cache, and exhibits a linear trend afterwards.\\nlearning to rank problem. We compare against pGBRT [22],\\nthe best previously pubished system on this task. XGBoost\\nruns exact greedy algorithm, while pGBRT only support an\\napproximate algorithm. The results are shown in Table 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='the best previously pubished system on this task. XGBoost\\nruns exact greedy algorithm, while pGBRT only support an\\napproximate algorithm. The results are shown in Table 4\\nand Fig. 10. We \ufb01nd that XGBoost runs faster. Interest-\\ningly, subsampling columns not only reduces running time,\\nand but also gives a bit higher performance for this prob-\\nlem. This could due to the fact that the subsampling helps\\nprevent over\ufb01tting, which is observed by many of the users.\\n6.5 Out-of-core Experiment\\nWe also evaluate our system in the out-of-core setting on\\nthe criteo data. We conducted the experiment on one AWS\\nc3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB\\nRAM). The results are shown in Figure 11. We can \ufb01nd\\nthat compression helps to speed up computation by factor of\\nthree, and sharding into two disks further gives 2x speedup.\\nFor this type of experiment, it is important to use a very\\nlarge dataset to drain the system \ufb01le cache for a real out-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='three, and sharding into two disks further gives 2x speedup.\\nFor this type of experiment, it is important to use a very\\nlarge dataset to drain the system \ufb01le cache for a real out-\\nof-core setting. This is indeed our setup. We can observe a\\ntransition point when the system runs out of \ufb01le cache. Note\\nthat the transition in the \ufb01nal method is less dramatic. This\\nis due to larger disk throughput and better utilization of\\ncomputation resources. Our \ufb01nal method is able to process\\n1.7 billion examples on a single machine.\\n6.6 Distributed Experiment\\nFinally, we evaluate the system in the distributed setting.\\nWe set up a YARN cluster on EC2 with m3.2xlarge ma-\\nchines, which is a very common choice for clusters. Each\\nmachine contains 8 virtual cores, 30GB of RAM and two\\n80GB SSD local disks. The dataset is stored on AWS S3\\ninstead of HDFS to avoid purchasing persistent storage.\\nWe \ufb01rst compare our system against two production-level\\ndistributed systems: Spark MLLib [18] and H2O 11. We use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='instead of HDFS to avoid purchasing persistent storage.\\nWe \ufb01rst compare our system against two production-level\\ndistributed systems: Spark MLLib [18] and H2O 11. We use\\n11www.h2o.ai\\n128 256 512 1024 2048\\nNumber of Training Examples (million)\\n128\\n256\\n512\\n1024\\n2048\\n4096\\n8192\\n16384\\n32768Total Running Time (sec)\\nSpark MLLib\\nH2O\\nXGBoost\\n(a) End-to-end time cost include data loading\\n128 256 512 1024 2048\\nNumber of Training Examples (million)\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096Time per Iteration (sec)\\nSpark MLLib\\nH2O\\nXGBoost\\n(b) Per iteration cost exclude data loading\\nFigure 12: Comparison of di\ufb00erent distributed sys-\\ntems on 32 EC2 nodes for 10 iterations on di\ufb00erent\\nsubset of criteo data. XGBoost runs more 10x than\\nspark per iteration and 2.2x as H2O\u2019s optimized ver-\\nsion (However, H2O is slow in loading the data, get-\\nting worse end-to-end time). Note that spark su\ufb00ers\\nfrom drastic slow down when running out of mem-\\nory. XGBoost runs faster and scales smoothly to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='ting worse end-to-end time). Note that spark su\ufb00ers\\nfrom drastic slow down when running out of mem-\\nory. XGBoost runs faster and scales smoothly to\\nthe full 1.7 billion examples with given resources by\\nutilizing out-of-core computation.\\n32 m3.2xlarge machines and test the performance of the sys-\\ntems with various input size. Both of the baseline systems\\nare in-memory analytics frameworks that need to store the\\ndata in RAM, while XGBoost can switch to out-of-core set-\\nting when it runs out of memory. The results are shown\\nin Fig. 12. We can \ufb01nd that XGBoost runs faster than the\\nbaseline systems. More importantly, it is able to take ad-\\nvantage of out-of-core computing and smoothly scale to all\\n1.7 billion examples with the given limited computing re-\\nsources. The baseline systems are only able to handle sub-\\nset of the data with the given resources. This experiment\\nshows the advantage to bring all the system improvement\\ntogether and solve a real-world scale problem. We also eval-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='set of the data with the given resources. This experiment\\nshows the advantage to bring all the system improvement\\ntogether and solve a real-world scale problem. We also eval-\\nuate the scaling property of XGBoost by varying the number\\nof machines. The results are shown in Fig. 13. We can \ufb01nd\\nXGBoost\u2019s performance scales linearly as we add more ma-\\nchines. Importantly, XGBoost is able to handle the entire\\n1.7 billion data with only four machines. This shows the\\nsystem\u2019s potential to handle even larger data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='4 8 16 32\\nNumber of Machines\\n128\\n256\\n512\\n1024\\n2048Time per Iteration (sec)\\nFigure 13: Scaling of XGBoost with di\ufb00erent num-\\nber of machines on criteo full 1.7 billion dataset.\\nUsing more machines results in more \ufb01le cache and\\nmakes the system run faster, causing the trend\\nto be slightly super linear. XGBoost can process\\nthe entire dataset using as little as four machines,\\nand scales smoothly by utilizing more available re-\\nsources.\\n7. CONCLUSION\\nIn this paper, we described the lessons we learnt when\\nbuilding XGBoost, a scalable tree boosting system that is\\nwidely used by data scientists and provides state-of-the-art\\nresults on many problems. We proposed a novel sparsity\\naware algorithm for handling sparse data and a theoretically\\njusti\ufb01ed weighted quantile sketch for approximate learning.\\nOur experience shows that cache access patterns, data com-\\npression and sharding are essential elements for building a\\nscalable end-to-end system for tree boosting. These lessons'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Our experience shows that cache access patterns, data com-\\npression and sharding are essential elements for building a\\nscalable end-to-end system for tree boosting. These lessons\\ncan be applied to other machine learning systems as well.\\nBy combining these insights, XGBoost is able to solve real-\\nworld scale problems using a minimal amount of resources.\\nAcknowledgments\\nWe would like to thank Tyler B. Johnson, Marco Tulio Ribeiro,\\nSameer Singh, Arvind Krishnamurthy for their valuable feedback.\\nWe also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan\\nTang, Hongliang Liu, Qiang Kou, Nan Zhu and all other con-\\ntributors in the XGBoost community. This work was supported\\nin part by ONR (PECASE) N000141010672, NSF IIS 1258741\\nand the TerraSwarm Research Center sponsored by MARCO and\\nDARPA.\\n8. REFERENCES\\n[1] R. Bekkerman. The present and the future of the kdd cup\\ncompetition: an outsider\u2019s perspective.\\n[2] R. Bekkerman, M. Bilenko, and J. Langford. Scaling Up'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='DARPA.\\n8. REFERENCES\\n[1] R. Bekkerman. The present and the future of the kdd cup\\ncompetition: an outsider\u2019s perspective.\\n[2] R. Bekkerman, M. Bilenko, and J. Langford. Scaling Up\\nMachine Learning: Parallel and Distributed Approaches .\\nCambridge University Press, New York, NY, USA, 2011.\\n[3] J. Bennett and S. Lanning. The net\ufb02ix prize. In\\nProceedings of the KDD Cup Workshop 2007 , pages 3\u20136,\\nNew York, Aug. 2007.\\n[4] L. Breiman. Random forests. Maching Learning,\\n45(1):5\u201332, Oct. 2001.\\n[5] C. Burges. From ranknet to lambdarank to lambdamart:\\nAn overview. Learning, 11:23\u2013581, 2010.\\n[6] O. Chapelle and Y. Chang. Yahoo! Learning to Rank\\nChallenge Overview. Journal of Machine Learning\\nResearch - W & CP , 14:1\u201324, 2011.\\n[7] T. Chen, H. Li, Q. Yang, and Y. Yu. General functional\\nmatrix factorization using gradient boosting. In Proceeding\\nof 30th International Conference on Machine Learning\\n(ICML\u201913), volume 1, pages 436\u2013444, 2013.\\n[8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. E\ufb03cient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='of 30th International Conference on Machine Learning\\n(ICML\u201913), volume 1, pages 436\u2013444, 2013.\\n[8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. E\ufb03cient\\nsecond-order gradient boosting for conditional random\\n\ufb01elds. In Proceeding of 18th Arti\ufb01cial Intelligence and\\nStatistics Conference (AISTATS\u201915), volume 1, 2015.\\n[9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and\\nC.-J. Lin. LIBLINEAR: A library for large linear\\nclassi\ufb01cation. Journal of Machine Learning Research ,\\n9:1871\u20131874, 2008.\\n[10] J. Friedman. Greedy function approximation: a gradient\\nboosting machine. Annals of Statistics , 29(5):1189\u20131232,\\n2001.\\n[11] J. Friedman. Stochastic gradient boosting. Computational\\nStatistics & Data Analysis , 38(4):367\u2013378, 2002.\\n[12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic\\nregression: a statistical view of boosting. Annals of\\nStatistics, 28(2):337\u2013407, 2000.\\n[13] J. H. Friedman and B. E. Popescu. Importance sampled\\nlearning ensembles, 2003.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='regression: a statistical view of boosting. Annals of\\nStatistics, 28(2):337\u2013407, 2000.\\n[13] J. H. Friedman and B. E. Popescu. Importance sampled\\nlearning ensembles, 2003.\\n[14] M. Greenwald and S. Khanna. Space-e\ufb03cient online\\ncomputation of quantile summaries. In Proceedings of the\\n2001 ACM SIGMOD International Conference on\\nManagement of Data , pages 58\u201366, 2001.\\n[15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi,\\nA. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela.\\nPractical lessons from predicting clicks on ads at facebook.\\nIn Proceedings of the Eighth International Workshop on\\nData Mining for Online Advertising , ADKDD\u201914, 2014.\\n[16] P. Li. Robust Logitboost and adaptive base class (ABC)\\nLogitboost. In Proceedings of the Twenty-Sixth Conference\\nAnnual Conference on Uncertainty in Arti\ufb01cial Intelligence\\n(UAI\u201910), pages 302\u2013311, 2010.\\n[17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank\\nusing multiple classi\ufb01cation and gradient boosting. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='(UAI\u201910), pages 302\u2013311, 2010.\\n[17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank\\nusing multiple classi\ufb01cation and gradient boosting. In\\nAdvances in Neural Information Processing Systems 20 ,\\npages 897\u2013904. 2008.\\n[18] X. Meng, J. Bradley, B. Yavuz, E. Sparks,\\nS. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde,\\nS. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,\\nM. Zaharia, and A. Talwalkar. MLlib: Machine learning in\\napache spark. Journal of Machine Learning Research ,\\n17(34):1\u20137, 2016.\\n[19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo.\\nPlanet: Massively parallel learning of tree ensembles with\\nmapreduce. Proceeding of VLDB Endowment,\\n2(2):1426\u20131437, Aug. 2009.\\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.\\nScikit-learn: Machine learning in Python. Journal of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.\\nScikit-learn: Machine learning in Python. Journal of\\nMachine Learning Research, 12:2825\u20132830, 2011.\\n[21] G. Ridgeway. Generalized Boosted Models: A guide to the\\ngbm package.\\n[22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin.\\nParallel boosted regression trees for web search ranking. In\\nProceedings of the 20th international conference on World\\nwide web, pages 387\u2013396. ACM, 2011.\\n[23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic\\ngradient boosted distributed decision trees. In Proceedings\\nof the 18th ACM Conference on Information and\\nKnowledge Management, CIKM \u201909.\\n[24] Q. Zhang and W. Wang. A fast algorithm for approximate\\nquantiles in high speed data streams. In Proceedings of the\\n19th International Conference on Scienti\ufb01c and Statistical\\nDatabase Management, 2007.\\n[25] T. Zhang and R. Johnson. Learning nonlinear functions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='19th International Conference on Scienti\ufb01c and Statistical\\nDatabase Management, 2007.\\n[25] T. Zhang and R. Johnson. Learning nonlinear functions\\nusing regularized greedy forest. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 36(5), 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='APPENDIX\\nA. WEIGHTED QUANTILE SKETCH\\nIn this section, we introduce the weighted quantile sketch algo-\\nrithm. Approximate answer of quantile queries is for many real-\\nworld applications. One classical approach to this problem is GK\\nalgorithm [14] and extensions based on the GK framework [24].\\nThe main component of these algorithms is a data structure called\\nquantile summary, that is able to answer quantile queries with\\nrelative accuracy of \u03f5. Two operations are de\ufb01ned for a quantile\\nsummary:\\n\u2022 A merge operation that combines two summaries with ap-\\nproximation error \u03f51 and \u03f52 together and create a merged\\nsummary with approximation error max( \u03f51,\u03f52).\\n\u2022 A prune operation that reduces the number of elements in\\nthe summary to b+1 and changes approximation error from\\n\u03f5 to \u03f5+ 1\\nb.\\nA quantile summary with merge and prune operations forms basic\\nbuilding blocks of the distributed and streaming quantile comput-\\ning algorithms [24].\\nIn order to use quantile computation for approximate tree boost-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='building blocks of the distributed and streaming quantile comput-\\ning algorithms [24].\\nIn order to use quantile computation for approximate tree boost-\\ning, we need to \ufb01nd quantiles on weighted data. This more gen-\\neral problem is not supported by any of the existing algorithm. In\\nthis section, we describe a non-trivial weighted quantile summary\\nstructure to solve this problem. Importantly, the new algorithm\\ncontains merge and prune operations with the same guarantee as\\nGK summary. This allows our summary to be plugged into all\\nthe frameworks used GK summary as building block and answer\\nquantile queries over weighted data e\ufb03ciently.\\nA.1 Formalization and De\ufb01nitions\\nGiven an input multi-set D= {(x1,w1),(x2,w2) \u00b7\u00b7\u00b7 (xn,wn)}\\nsuch that wi \u2208[0,+\u221e),xi \u2208X . Each xi corresponds to a po-\\nsition of the point and wi is the weight of the point. Assume\\nwe have a total order < de\ufb01ned on X. Let us de\ufb01ne two rank\\nfunctions r\u2212\\nD,r+\\nD: X\u2192 [0,+\u221e)\\nr\u2212\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x<y\\nw (10)\\nr+\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x\u2264y'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='we have a total order < de\ufb01ned on X. Let us de\ufb01ne two rank\\nfunctions r\u2212\\nD,r+\\nD: X\u2192 [0,+\u221e)\\nr\u2212\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x<y\\nw (10)\\nr+\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x\u2264y\\nw (11)\\nWe should note that since Dis de\ufb01ned to be a multiset of the\\npoints. It can contain multiple record with exactly same position\\nx and weight w. We also de\ufb01ne another weight function \u03c9D :\\nX\u2192 [0,+\u221e) as\\n\u03c9D(y) = r+\\nD(y) \u2212r\u2212\\nD(y) =\\n\u2211\\n(x,w)\u2208D,x=y\\nw. (12)\\nFinally, we also de\ufb01ne the weight of multi-set Dto be the sum of\\nweights of all the points in the set\\n\u03c9(D) =\\n\u2211\\n(x,w)\u2208D\\nw (13)\\nOur task is given a series of input D, to estimate r+(y) and r\u2212(y)\\nfor y\u2208X as well as \ufb01nding points with speci\ufb01c rank. Given these\\nnotations, we de\ufb01ne quantile summary of weighted examples as\\nfollows:\\nDefinition A.1. Quantile Summary of Weighted Data\\nA quantile summary for Dis de\ufb01ned to be tuple Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D),\\nwhere S = {x1,x2,\u00b7\u00b7\u00b7 ,xk}is selected from the points in D(i.e.\\nxi \u2208{x|(x,w) \u2208D}) with the following properties:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='A quantile summary for Dis de\ufb01ned to be tuple Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D),\\nwhere S = {x1,x2,\u00b7\u00b7\u00b7 ,xk}is selected from the points in D(i.e.\\nxi \u2208{x|(x,w) \u2208D}) with the following properties:\\n1) xi <xi+1 for all i, and x1 and xk are minimum and max-\\nimum point in D:\\nx1 = min\\n(x,w)\u2208D\\nx, xk = max\\n(x,w)\u2208D\\nx\\n2) \u02dcr+\\nD, \u02dcr\u2212\\nD and \u02dc\u03c9D are functions in S \u2192[0,+\u221e), that satis\ufb01es\\n\u02dcr\u2212\\nD(xi) \u2264r\u2212\\nD(xi), \u02dcr+\\nD(xi) \u2265r+\\nD(xi), \u02dc\u03c9D(xi) \u2264\u03c9D(xi), (14)\\nthe equality sign holds for maximum and minimum point ( \u02dcr\u2212\\nD(xi) =\\nr\u2212\\nD(xi), \u02dcr+\\nD(xi) = r+\\nD(xi) and \u02dc\u03c9D(xi) = \u03c9D(xi) for i\u2208{1,k}).\\nFinally, the function value must also satisfy the following con-\\nstraints\\n\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) \u2264\u02dcr\u2212\\nD(xi+1), \u02dcr+\\nD(xi) \u2264\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n(15)\\nSince these functions are only de\ufb01ned onS, it is su\ufb03ce to use 4k\\nrecord to store the summary. Speci\ufb01cally, we need to remember\\neach xi and the corresponding function values of each xi.\\nDefinition A.2. Extension of Function Domains\\nGiven a quantile summary Q(D) = ( S,\u02dcr+\\nD,\u02dcr\u2212'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='each xi and the corresponding function values of each xi.\\nDefinition A.2. Extension of Function Domains\\nGiven a quantile summary Q(D) = ( S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) de\ufb01ned in\\nDe\ufb01nition A.1, the domain of \u02dcr+\\nD, \u02dcr\u2212\\nD and \u02dc\u03c9D were de\ufb01ned only\\nin S. We extend the de\ufb01nition of these functions to X\u2192 [0,+\u221e)\\nas follows\\nWhen y <x1:\\n\u02dcr\u2212\\nD(y) = 0, \u02dcr+\\nD(y) = 0, \u02dc\u03c9D(y) = 0 (16)\\nWhen y >xk:\\n\u02dcr\u2212\\nD(y) = \u02dcr+\\nD(xk), \u02dcr+\\nD(y) = \u02dcr+\\nD(xk), \u02dc\u03c9D(y) = 0 (17)\\nWhen y\u2208(xi,xi+1) for some i:\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi),\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1),\\n\u02dc\u03c9D(y) = 0\\n(18)\\nLemma A.1. Extended Constraint\\nThe extended de\ufb01nition of \u02dcr\u2212\\nD, \u02dcr+\\nD, \u02dc\u03c9D satis\ufb01es the following\\nconstraints\\n\u02dcr\u2212\\nD(y) \u2264r\u2212\\nD(y), \u02dcr+\\nD(y) \u2265r+\\nD(y), \u02dc\u03c9D(y) \u2264\u03c9D(y) (19)\\n\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y) \u2264\u02dcr\u2212\\nD(x), \u02dcr+\\nD(y) \u2264\u02dcr+\\nD(x) \u2212\u02dc\u03c9D(x), for all y <x\\n(20)\\nProof. The only non-trivial part is to prove the case when\\ny\u2208(xi,xi+1):\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) \u2264r\u2212\\nD(xi) + \u03c9D(xi) \u2264r\u2212\\nD(y)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2265r+\\nD(xi+1) \u2212\u03c9D(xi+1) \u2265r+\\nD(y)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Proof. The only non-trivial part is to prove the case when\\ny\u2208(xi,xi+1):\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) \u2264r\u2212\\nD(xi) + \u03c9D(xi) \u2264r\u2212\\nD(y)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2265r+\\nD(xi+1) \u2212\u03c9D(xi+1) \u2265r+\\nD(y)\\nThis proves Eq. (19). Furthermore, we can verify that\\n\u02dcr+\\nD(xi) \u2264\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) = \u02dcr+\\nD(y) \u2212\u02dc\u03c9D(y)\\n\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y) = \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + 0 \u2264\u02dcr\u2212\\nD(xi+1)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\nUsing these facts and transitivity of < relation, we can prove\\nEq. (20)\\nWe should note that the extension is based on the ground case\\nde\ufb01ned in S, and we do not require extra space to store the sum-\\nmary in order to use the extended de\ufb01nition. We are now ready\\nto introduce the de\ufb01nition of \u03f5-approximate quantile summary.\\nDefinition A.3. \u03f5-Approximate Quantile Summary\\nGiven a quantile summary Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D), we call it is\\n\u03f5-approximate summary if for any y\u2208X\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y) \u2264\u03f5\u03c9(D) (21)\\nWe use this de\ufb01nition since we know that r\u2212(y) \u2208[\u02dcr\u2212\\nD(y),\u02dcr+\\nD(y)\u2212\\n\u02dc\u03c9D(y)] and r+(y) \u2208[\u02dcr\u2212'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='D,\u02dcr\u2212\\nD,\u02dc\u03c9D), we call it is\\n\u03f5-approximate summary if for any y\u2208X\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y) \u2264\u03f5\u03c9(D) (21)\\nWe use this de\ufb01nition since we know that r\u2212(y) \u2208[\u02dcr\u2212\\nD(y),\u02dcr+\\nD(y)\u2212\\n\u02dc\u03c9D(y)] and r+(y) \u2208[\u02dcr\u2212\\nD(y) + \u02dc\u03c9D(y),\u02dcr+\\nD(y)]. Eq. (21) means the\\nwe can get estimation of r+(y) and r\u2212(y) by error of at most\\n\u03f5\u03c9(D).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Lemma A.2. Quantile summary Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) is an\\n\u03f5-approximate summary if and only if the following two condition\\nholds\\n\u02dcr+\\nD(xi) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi) \u2264\u03f5\u03c9(D) (22)\\n\u02dcr+\\nD(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dc\u03c9D(xi) \u2264\u03f5\u03c9(D) (23)\\nProof. The key is again consider y\u2208(xi,xi+1)\\n\u02dcr+\\nD(y)\u2212\u02dcr\u2212\\nD(y)\u2212\u02dc\u03c9D(y) = [\u02dcr+\\nD(xi+1)\u2212\u02dc\u03c9D(xi+1)]\u2212[\u02dcr+\\nD(xi)+\u02dc\u03c9D(xi)]\u22120\\nThis means the condition in Eq. (23) plus Eq. (22) can give us\\nEq. (21)\\nProperty of Extended FunctionIn this section, we have in-\\ntroduced the extension of function \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D to X \u2192[0,+\u221e).\\nThe key theme discussed in this section is the relation of con-\\nstraints on the original function and constraints on the extended\\nfunction. Lemma A.1 and A.2 show that the constraints on the\\noriginal function can lead to in more general constraints on the\\nextended function. This is a very useful property which will be\\nused in the proofs in later sections.\\nA.2 Construction of Initial Summary\\nGiven a small multi-set D= {(x1,w1),(x2,w2),\u00b7\u00b7\u00b7 ,(xn,wn)},'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='extended function. This is a very useful property which will be\\nused in the proofs in later sections.\\nA.2 Construction of Initial Summary\\nGiven a small multi-set D= {(x1,w1),(x2,w2),\u00b7\u00b7\u00b7 ,(xn,wn)},\\nwe can construct initial summaryQ(D) = {S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D}, with S\\nto the set of all values in D(S = {x|(x,w) \u2208D}), and \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D\\nde\ufb01ned to be\\n\u02dcr+\\nD(x) = r+\\nD(x), \u02dcr\u2212\\nD(x) = r\u2212\\nD(x), \u02dc\u03c9D(x) = \u03c9D(x) for x\u2208S\\n(24)\\nThe constructed summary is 0-approximate summary, since it can\\nanswer all the queries accurately. The constructed summary can\\nbe feed into future operations described in the latter sections.\\nA.3 Merge Operation\\nIn this section, we de\ufb01ne how we can merge the two summaries\\ntogether. Assume we have Q(D1) = ( S1,\u02dcr+\\nD1 ,\u02dcr\u2212\\nD1 ,\u02dc\u03c9D1 ) and\\nQ(D2) = ( S2,\u02dcr+\\nD1 ,\u02dcr\u2212\\nD2 ,\u02dc\u03c9D2 ) quantile summary of two dataset\\nD1 and D2. Let D= D1 \u222aD2, and de\ufb01ne the merged summary\\nQ(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) as follows.\\nS = {x1,x2 \u00b7\u00b7\u00b7 ,xk},xi \u2208S1 or xi \u2208S2 (25)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='D1 ,\u02dcr\u2212\\nD2 ,\u02dc\u03c9D2 ) quantile summary of two dataset\\nD1 and D2. Let D= D1 \u222aD2, and de\ufb01ne the merged summary\\nQ(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) as follows.\\nS = {x1,x2 \u00b7\u00b7\u00b7 ,xk},xi \u2208S1 or xi \u2208S2 (25)\\nThe points in S are combination of points in S1 and S2. And the\\nfunction \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9Dare de\ufb01ned to be\\n\u02dcr\u2212\\nD(xi) = \u02dcr\u2212\\nD1 (xi) + \u02dcr\u2212\\nD2 (xi) (26)\\n\u02dcr+\\nD(xi) = \u02dcr+\\nD1 (xi) + \u02dcr+\\nD2 (xi) (27)\\n\u02dc\u03c9D(xi) = \u02dc\u03c9D1 (xi) + \u02dc\u03c9D2 (xi) (28)\\nHere we use functions de\ufb01ned on S \u2192[0,+\u221e) on the left sides of\\nequalities and use the extended function de\ufb01nitions on the right\\nsides.\\nDue to additive nature of r+, r\u2212and \u03c9, which can be formally\\nwritten as\\nr\u2212\\nD(y) =r\u2212\\nD1 (y) + r\u2212\\nD2 (y),\\nr+\\nD(y) =r+\\nD1 (y) + r+\\nD2 (y),\\n\u03c9D(y) =\u03c9D1 (y) + \u03c9D2 (y),\\n(29)\\nand the extended constraint property in Lemma A.1, we can verify\\nthat Q(D) satis\ufb01es all the constraints in De\ufb01nition A.1. Therefore\\nit is a valid quantile summary.\\nLemma A.3. The combined quantile summary satis\ufb01es\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y) (30)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD1 (y) + \u02dcr+'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='it is a valid quantile summary.\\nLemma A.3. The combined quantile summary satis\ufb01es\\n\u02dcr\u2212\\nD(y) = \u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y) (30)\\n\u02dcr+\\nD(y) = \u02dcr+\\nD1 (y) + \u02dcr+\\nD2 (y) (31)\\n\u02dc\u03c9D(y) = \u02dc\u03c9D1 (y) + \u02dc\u03c9D2 (y) (32)\\nfor all y\u2208X\\nAlgorithm 4: Query Function g(Q,d)\\nInput: d: 0 \u2264d\u2264\u03c9(D)\\nInput: Q(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) where\\nS = x1,x2,\u00b7\u00b7\u00b7 ,xk\\nif d< 1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)] then return x1 ;\\nif d\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)] then return xk ;\\nFind i such that\\n1\\n2 [\u02dcr\u2212\\nD(xi) + \u02dcr+\\nD(xi)] \u2264d< 1\\n2 [\u02dcr\u2212\\nD(xi+1) + \u02dcr+\\nD(xi+1)]\\nif 2d< \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) then\\nreturn xi\\nelse\\nreturn xi+1\\nend\\nThis can be obtained by straight-forward application of De\ufb01ni-\\ntion A.2.\\nTheorem A.1. If Q(D1) is \u03f51-approximate summary, and Q(D2)\\nis \u03f52-approximate summary. Then the merged summary Q(D) is\\nmax(\u03f51,\u03f52)-approximate summary.\\nProof. For any y\u2208X, we have\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y)\\n=[\u02dcr+\\nD1 (y) + \u02dcr+\\nD2 (y)] \u2212[\u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y)] \u2212[\u02dc\u03c9D1 (y) + \u02dc\u03c9D2 (y)]\\n\u2264\u03f51\u03c9(D1) + \u03f52\u03c9(D2) \u2264max(\u03f51,\u03f52)\u03c9(D1 \u222aD2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='Proof. For any y\u2208X, we have\\n\u02dcr+\\nD(y) \u2212\u02dcr\u2212\\nD(y) \u2212\u02dc\u03c9D(y)\\n=[\u02dcr+\\nD1 (y) + \u02dcr+\\nD2 (y)] \u2212[\u02dcr\u2212\\nD1 (y) + \u02dcr\u2212\\nD2 (y)] \u2212[\u02dc\u03c9D1 (y) + \u02dc\u03c9D2 (y)]\\n\u2264\u03f51\u03c9(D1) + \u03f52\u03c9(D2) \u2264max(\u03f51,\u03f52)\u03c9(D1 \u222aD2)\\nHere the \ufb01rst inequality is due to Lemma A.3.\\nA.4 Prune Operation\\nBefore we start discussing the prune operation, we \ufb01rst in-\\ntroduce a query function g(Q,d). The de\ufb01nition of function is\\nshown in Algorithm 4. For a given rank d, the function returns\\na x whose rank is close to d. This property is formally described\\nin the following Lemma.\\nLemma A.4. For a given \u03f5-approximate summary Q(D) =\\n(S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D), x\u2217= g(Q,d) satis\ufb01es the following property\\nd\u2265\u02dcr+\\nD(x\u2217) \u2212\u02dc\u03c9D(x\u2217) \u2212\u03f5\\n2 \u03c9(D)\\nd\u2264\u02dcr\u2212\\nD(x\u2217) + \u02dc\u03c9D(x\u2217) + \u03f5\\n2 \u03c9(D)\\n(33)\\nProof. We need to discuss four possible cases\\n\u2022 d <1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)] and x\u2217= x1. Note that the rank\\ninformation for x1 is accurate (\u02dc\u03c9D(x1) = \u02dcr+\\nD(x1) = \u03c9(x1),\\n\u02dcr\u2212\\nD(x1) = 0), we have\\nd\u22650 \u2212\u03f5\\n2 \u03c9(D) = \u02dcr+\\nD(x1) \u2212\u02dc\u03c9D(x1) \u2212\u03f5\\n2 \u03c9(D)\\nd< 1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)]\\n\u2264\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)\\n= \u02dcr\u2212'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='information for x1 is accurate (\u02dc\u03c9D(x1) = \u02dcr+\\nD(x1) = \u03c9(x1),\\n\u02dcr\u2212\\nD(x1) = 0), we have\\nd\u22650 \u2212\u03f5\\n2 \u03c9(D) = \u02dcr+\\nD(x1) \u2212\u02dc\u03c9D(x1) \u2212\u03f5\\n2 \u03c9(D)\\nd< 1\\n2 [\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)]\\n\u2264\u02dcr\u2212\\nD(x1) + \u02dcr+\\nD(x1)\\n= \u02dcr\u2212\\nD(x1) + \u02dc\u03c9+\\nD(x1)\\n\u2022 d\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)] and x\u2217= xk, then\\nd\u22651\\n2 [\u02dcr\u2212\\nD(xk) + \u02dcr+\\nD(xk)]\\n= \u02dcr+\\nD(xk) \u22121\\n2 [\u02dcr+\\nD(xk) \u2212\u02dcr\u2212\\nD(xk)]\\n= \u02dcr+\\nD(xk) \u22121\\n2 \u02dc\u03c9D(xk)\\nd<\u03c9 (D) + \u03f5\\n2 \u03c9(D) = \u02dcr\u2212\\nD(xk) + \u02dc\u03c9D(xk) + \u03f5\\n2 \u03c9(D)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='\u2022 x\u2217= xi in the general case, then\\n2d< \u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n= 2[\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi)] + [\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi)]\\n\u22642[\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi)] + \u03f5\u03c9(D)\\n2d\u2265\u02dcr\u2212\\nD(xi) + \u02dcr+\\nD(xi)\\n= 2[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi)] \u2212[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi) \u2212\u02dcr\u2212\\nD(xi)] + \u02dc\u03c9D(xi)\\n\u22652[\u02dcr+\\nD(xi) \u2212\u02dc\u03c9D(xi)] \u2212\u03f5\u03c9(D) + 0\\n\u2022 x\u2217= xi+1 in the general case\\n2d\u2265\u02dcr\u2212\\nD(xi) + \u02dc\u03c9D(xi) + \u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)\\n= 2[\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1)]\\n\u2212[\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi) \u2212\u02dc\u03c9D(xi)]\\n\u22652[\u02dcr+\\nD(xi+1) + \u02dc\u03c9D(xi+1)] \u2212\u03f5\u03c9(D)\\n2d\u2264\u02dcr\u2212\\nD(xi+1) + \u02dcr+\\nD(xi+1)\\n= 2[\u02dcr\u2212\\nD(xi+1) + \u02dc\u03c9D(xi+1)]\\n+ [\u02dcr+\\nD(xi+1) \u2212\u02dc\u03c9D(xi+1) \u2212\u02dcr\u2212\\nD(xi+1)] \u2212\u02dc\u03c9D(xi+1)\\n\u22642[\u02dcr\u2212\\nD(xi+1) + \u02dc\u03c9D(xi+1)] + \u03f5\u03c9(D) \u22120\\nNow we are ready to introduce the prune operation. Given a\\nquantile summaryQ(D) = (S,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) with S = {x1,x2,\u00b7\u00b7\u00b7 ,xk}\\nelements, and a memory budget b. The prune operation creates\\nanother summary Q\u2032(D) = (S\u2032,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) with S\u2032= {x\u2032\\n1,x\u2032\\n2,\u00b7\u00b7\u00b7 ,x\u2032\\nb+1},\\nwhere x\u2032\\ni are selected by query the original summary such that\\nx\u2032\\ni = g\\n(\\nQ,i\u22121'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='another summary Q\u2032(D) = (S\u2032,\u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D) with S\u2032= {x\u2032\\n1,x\u2032\\n2,\u00b7\u00b7\u00b7 ,x\u2032\\nb+1},\\nwhere x\u2032\\ni are selected by query the original summary such that\\nx\u2032\\ni = g\\n(\\nQ,i\u22121\\nb \u03c9(D)\\n)\\n.\\nThe de\ufb01nition of \u02dcr+\\nD,\u02dcr\u2212\\nD,\u02dc\u03c9D in Q\u2032 is copied from original sum-\\nmary Q, by restricting input domain from S to S\u2032. There could\\nbe duplicated entries in the S\u2032. These duplicated entries can be\\nsafely removed to further reduce the memory cost. Since all the\\nelements in Q\u2032comes from Q, we can verify that Q\u2032satis\ufb01es all\\nthe constraints in De\ufb01nition A.1 and is a valid quantile summary.\\nTheorem A.2. Let Q\u2032(D) be the summary pruned from an\\n\u03f5-approximate quantile summary Q(D) with b memory budget.\\nThen Q\u2032(D) is a (\u03f5+ 1\\nb)-approximate summary.\\nProof. We only need to prove the property in Eq. (23) forQ\u2032.\\nUsing Lemma A.4, we have\\ni\u22121\\nb \u03c9(D) + \u03f5\\n2 \u03c9(D) \u2265\u02dcr+\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\ni\u22121\\nb \u03c9(D) \u2212\u03f5\\n2 \u03c9(D) \u2264\u02dcr\u2212\\nD(x\u2032\\ni) + \u02dc\u03c9D(x\u2032\\ni)\\nCombining these inequalities gives\\n\u02dcr+\\nD(x\u2032\\ni+1) \u2212\u02dc\u03c9D(x\u2032\\ni+1) \u2212\u02dcr\u2212\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\n\u2264[ i\\nb\u03c9(D) + \u03f5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/xgboost.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': 'xgboost.pdf', 'file_type': 'pdf'}, page_content='i\u22121\\nb \u03c9(D) + \u03f5\\n2 \u03c9(D) \u2265\u02dcr+\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\ni\u22121\\nb \u03c9(D) \u2212\u03f5\\n2 \u03c9(D) \u2264\u02dcr\u2212\\nD(x\u2032\\ni) + \u02dc\u03c9D(x\u2032\\ni)\\nCombining these inequalities gives\\n\u02dcr+\\nD(x\u2032\\ni+1) \u2212\u02dc\u03c9D(x\u2032\\ni+1) \u2212\u02dcr\u2212\\nD(x\u2032\\ni) \u2212\u02dc\u03c9D(x\u2032\\ni)\\n\u2264[ i\\nb\u03c9(D) + \u03f5\\n2 \u03c9(D)] \u2212[ i\u22121\\nb \u03c9(D) \u2212\u03f5\\n2 \u03c9(D)] = ( 1\\nb + \u03f5)\u03c9(D)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings and Their Use In\\nSentence Classi\ufb01cation Tasks\\nAmit Mandelbaum Adi Shalev\\nHebrew University of Jerusalm\\namit.mandelbaum@mail.huji.ac.il bitan.adi@gmail.com\\nOctober 27, 2016\\nAbstract\\nThis paper has two parts. In the \ufb01rst part we discuss word embeddings. We discuss the need for them, some\\nof the methods to create them, and some of their interesting properties. We also compare them to image\\nembeddings and see how word embedding and image embedding can be combined to perform different tasks.\\nIn the second part we implement a convolutional neural network trained on top of pre-trained word vectors.\\nThe network is used for several sentence-level classi\ufb01cation tasks, and achieves state-of-art (or comparable)\\nresults, demonstrating the great power of pre-trainted word embeddings over random ones.\\nI. I ntroduction\\nT\\nhere are some de\ufb01nitions for what Word\\nEmbeddings are, but in the most general\\nnotion, word embeddings are the numer-\\nical representation of words, usually in a shape'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='I. I ntroduction\\nT\\nhere are some de\ufb01nitions for what Word\\nEmbeddings are, but in the most general\\nnotion, word embeddings are the numer-\\nical representation of words, usually in a shape\\nof a vector in \u211cd. Being more speci\ufb01c, word\\nembeddings are unsupervisedly learned word\\nrepresentation vectors whose relative similari-\\nties correlate with semantic similarity. In com-\\nputational linguistics they are often referred as\\ndistributional semantic model or distributed repre-\\nsentations.\\nThe theoretical foundations of word embed-\\ndings can be traced back to the early 1950\u2019s\\nand in particular in the works of Zellig Harris,\\nJohn Firth, and Ludwig Wittgenstein. The ear-\\nliest attempts at using feature representations\\nto quantify (semantic) similarity used hand-\\ncrafted features. A good example is the work\\non semantic differentials [Osgood, 1964]. The\\nearly 1990\u2019s saw the rise of automatically gen-\\nerated contextual features, and the rise of Deep\\nLearning methods for Natural Language Pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='on semantic differentials [Osgood, 1964]. The\\nearly 1990\u2019s saw the rise of automatically gen-\\nerated contextual features, and the rise of Deep\\nLearning methods for Natural Language Pro-\\ncessing (NLP) in the early 2010\u2019s helped to in-\\ncrease their popularity, to the point that, these\\ndays, word embeddings are the most popular\\nresearch area in NLP 1.\\nThis work will be divided into two parts.\\nIn the \ufb01rst part we will discuss the need for\\nword embeddings, some of the methods to\\ncreate them, and some interesting features of\\nthose embeddings. We also compare them to\\nimage embeddings (usually referred as image\\nfeatures) and see how word embedding and\\nimage embedding can be combined to perform\\ndifferent tasks.\\nIn the second part of this paper we will\\npresent our implementation of Convolutional\\nNeural Networks for Sentence Classi\ufb01cation\\n[Kim ,2014]. This work which became very\\npopular is a very good demonstration of the\\npower of pre-trained word embeddings. Us-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Neural Networks for Sentence Classi\ufb01cation\\n[Kim ,2014]. This work which became very\\npopular is a very good demonstration of the\\npower of pre-trained word embeddings. Us-\\ning a relatively simple model, the authors were\\nable to achieve state-of-art (or comparable) re-\\nsults, for several sentence-level classi\ufb01cation\\ntasks. In this part we will present the model,\\ndiscuss the results and compare them to those\\nof the original article. We will also extend and\\ntest the model on some datasets that were not\\nused in the original article. Finally, we will\\n1In 2015 the dominating subject at EMNLP (\"Empiri-\\ncal Methods in NLP\") conference was word embeddings,\\nsource: http://sebastianruder.com/word-embeddings-1/\\n1\\narXiv:1610.08229v1  [cs.LG]  26 Oct 2016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\npropose some extensions for the model which\\nmight be a good proposition for future work.\\nII. W ord Embeddings\\ni. Motivation\\nIt is obvious that every mathematical system\\nor algorithm needs some sort of numeric input\\nto work with. However, while images and\\naudio naturally come in the form of rich, high-\\ndimensional vectors (i.e. pixel intensity for\\nimages and power spectral density coef\ufb01cients\\nfor audio data), words are treated as discrete\\natomic symbols.\\nThe naive way of converting words to vectors\\nmight assign each word a one-hot vector in\\n\u211c|V| where |V| being vocabulary size. This\\nvector will be all zeros except one unique index\\nfor each word. Representing words in this way\\nleads to substantial data sparsity and usually\\nmeans that we may need more data in order to\\nsuccessfully train statistical models.\\nFigure 1: Density of different data sources.\\nWhat mentioned above raise the need for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='means that we may need more data in order to\\nsuccessfully train statistical models.\\nFigure 1: Density of different data sources.\\nWhat mentioned above raise the need for\\ncontinuous, vector space representations of\\nwords that contain data that can be leveraged\\nby models. To be more speci\ufb01c we want seman-\\ntically similar words to be mapped to nearby\\npoints, thus making the representation carry\\nuseful information about the word actual mean-\\ning.\\nii. Word Embeddings Methods\\nWord embeddings models can be divided into\\nmain categories:\\n\u2022Count-based methods\\n\u2022Predictive methods\\nModels in both categories share, in at least\\nsome way, the assumption that words that ap-\\npear in the same contexts share semantic mean-\\ning.\\nOne of the most in\ufb02uential early works\\nin count-based methods is the LSI/LSA\\n(Latent Semantic Indexing/Analysis)\\n[Deerwester et al., 1990] method. This\\nmethod is based on the Firth\u2019s hypothesisfrom\\n1957 [Firth, 1957] that the meaning of a word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='(Latent Semantic Indexing/Analysis)\\n[Deerwester et al., 1990] method. This\\nmethod is based on the Firth\u2019s hypothesisfrom\\n1957 [Firth, 1957] that the meaning of a word\\nis de\ufb01ned \"by the company it keeps\". This\\nhypothesis leads to a very simple albeit a very\\nhigh-dimensional word embedding. Formally,\\neach word can be represented as a vector in \u211cN\\nwhere N is the unique number of words in a\\ngiven dictionary (in practice N=100,000). Then,\\nby taking a very large corpus (e.g. Wikipedia),\\nlet Count 5(w1, w2) be the number of times\\nw1 and w2 occur within a distance 5 of each\\nother in the corpus. Then the word embedding\\nfor a word w is a vector of dimension N,\\nwith one coordinate for each dictionary word.\\nThe coordinate corresponding to word w2 is\\nCount 5(w, w2).\\nThe problem with the resulting embedding\\nis that it uses extremely high-dimensional vec-\\ntors. In the LSA article, is was empirically\\ndiscovered that these embeddings can be re-\\nduced to vectors R300 by doing a rank-300 SVD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='is that it uses extremely high-dimensional vec-\\ntors. In the LSA article, is was empirically\\ndiscovered that these embeddings can be re-\\nduced to vectors R300 by doing a rank-300 SVD\\non the NxN original embeddings matrix.\\nThis method was later re\ufb01ned with reweight-\\ning heuristics, such as taking the loga-\\nrithm, or Pointwise Mutual Information (PMI)\\n[Kenneth et al., 1990] on the count, which is a\\nvery popular method.\\nThe second family of methods, sometimes\\nalso referred as neural probabilistic language mod-\\nels, had theoretical and some practical appear-\\nance as early as 1986 [ Hinton, 1986], but \ufb01rst\\nto show the utility of pre-trained word em-\\nbeddings were arguably Collobert and Weston\\nin 2008 [ Collobert and Weston, 2008]. Unlike\\ncount-based models, predictive models try to\\npredict a word from its neighbors in terms of\\nlearned small, dense embedding vectors.\\nTwo of the most popular methods which\\nappeared recently are the Glove (Global\\nVectors for Word Representation) method\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\n[Pennington et. al., 2014 ], which is an unsuper-\\nvised learning method, although not predictive\\nin the common sense, and Word2Vec, a family\\nof energy based predictive models, presented\\nby [Mikolov et. al., 2013 ]. As Word2Vec is the\\nembedding method used in our work it shall\\nbe brie\ufb02y discussed here.\\niii. Word2Vec\\nWord2vec is a particularly computationally-\\nef\ufb01cient predictive model for learning word\\nembeddings from raw text. It comes in two\\n\ufb02avors, the Continuous Bag-of-Words model\\n(CBOW) and the Skip-Gram model. Algo-\\nrithmically, these models are similar, except\\nthat CBOW predicts target words (e.g. \u2019mat\u2019)\\nfrom source context words (\u2019the cat sits on\\nthe\u2019), while the skip-gram does the inverse\\nand predicts source context-words from the\\ntarget words. In the skip-gram model (see \ufb01g-\\nFigure 2: The Skip-gram model architecture\\nure 2) a neural network is trained over a large\\ncorpus in where the training objective is to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='target words. In the skip-gram model (see \ufb01g-\\nFigure 2: The Skip-gram model architecture\\nure 2) a neural network is trained over a large\\ncorpus in where the training objective is to\\nlearn word vector representations that are good\\nat predicting the nearby words. The method\\nis also using a simpli\ufb01ed version of NCE\\n[Gutmann and Hyv\u00c3d\u2019rinen, 2012] called Neg-\\native sampling where the objective function is\\nde\ufb01ned as follows:\\nlog \u03c3(v\u2032T\\nwO vwI ) +\\nk\\n\u2211\\ni=1\\nEwi\u223cPn (w)[\u03c3(\u2212v\u2032T\\nwi vwI )]\\n(1)\\nwhere v\u2032\\nw and vw are the \"input\" and \"output\"\\nvector representations of w, \u03c3 is the sigmoid\\nfunction but can also be seen as the network\\nparameters function, and Pn is some noise prob-\\nability used to sample random words. In the\\narticle they recommend k to be between 5 to 20,\\nwhile the context of predicted words should be\\n5 or 10. This above objective is later put in the\\nSkip-Gram objective (equtaion 2) to produce\\noptimal word embeddings.\\n1\\nT\\nT\\n\u2211\\nt=1\\n\u2211\\n\u2212c\u2264j\u2264c,j\u0338=0\\nlogp (wt+j|wt) (2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='5 or 10. This above objective is later put in the\\nSkip-Gram objective (equtaion 2) to produce\\noptimal word embeddings.\\n1\\nT\\nT\\n\u2211\\nt=1\\n\u2211\\n\u2212c\u2264j\u2264c,j\u0338=0\\nlogp (wt+j|wt) (2)\\nThis objective enables the model to differentiate\\ndata from noise by means of logistic regression,\\nthus learning high-quality vector representa-\\ntions.\\nThe CBOW does exactly the same but the di-\\nrection is inverted. In other words the CBOW\\ntrains a binary logistic classi\ufb01er where, given a\\nwindow of context words, gives a higher proba-\\nbility to \"correct\" if the next word is correct and\\na higher probability to \"incorrect\" if the next\\nword is a random sampled one. Notice that\\nCBOW smoothes over a lot of the distributional\\ninformation (by treating an entire context as\\none observation). For the most part, this turns\\nout to be a useful thing for smaller datasets.\\nHowever, skip-gram treats each context-target\\npair as a new observation, and this tends to do\\nbetter when we have larger datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='out to be a useful thing for smaller datasets.\\nHowever, skip-gram treats each context-target\\npair as a new observation, and this tends to do\\nbetter when we have larger datasets.\\nFinally the vector we used in our work had\\na dimension of 300. The Network was trained\\non the Google News dataset which contains 30\\nbillion training words, with negative sampling\\nas mentioned above. These embeddings can be\\nfound online2.\\nA lot of follow-up work was done on the\\nWord2Vec method. One interesting work was\\n2code.google.com/p/word2vec\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 3: Left: Word2Vec t-SNE [Maaten and Hinton, 2008] visualization of our implementation, using text8 dataset\\nand a window size of 5. Only 400 words are visualized. Right: Zooming in of the rectangle in the left \ufb01gure.\\ndone by [ Goldberg and Levy, 2014] where ex-\\nperiments and theory were used to suggest that\\nthese newer methods are related to the older\\nPMI based models, but with new hyperparam-\\neters and/or term reweightings. In this project\\nappendix you can \ufb01nd a simpli\ufb01ed version\\nof Word2Vec we implemented in TensorFlow\\narchitecture using the text8 dataset 3 and the\\nSkip-Gram model. See \ufb01gure 3 for visualized\\nresults.\\niv. Word Embeddings Properties\\nSimilarity: The simplest property of embed-\\ndings obtained by all the methods described\\nabove is that similar words tend to have sim-\\nilar vectors. More formally, the similarity be-\\ntween two words (as rated by humans on a\\n[-1,1] scale) correlates with the cosine similar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='above is that similar words tend to have sim-\\nilar vectors. More formally, the similarity be-\\ntween two words (as rated by humans on a\\n[-1,1] scale) correlates with the cosine similar-\\nity between those words\u2019 vectors. The fact that\\nFigure 4: What words have embeddings closest to a\\ngiven word? From [Collobert et al., 2011]\\nwords embedding are related to their context-\\nwords stand behind the similarity property\\n3http://mattmahoney.net/dc/textdata\\nas naturally, similar words tend to appear in\\nsimilar context. This, however creates the\\nproblem that antonyms (e.g. cold and hot\\netc.) also appear with the same context while\\nthey are, by de\ufb01nition, have opposite mean-\\ning. In [ Mikolov et. al., 2013 ] the score of the\\n(accept,reject) pair is 0.73, and the score of\\n(long,short) is 0.71.\\nThe problem of antonyms was tackled di-\\nrectly by [ Schwartz et al., 2015]. In this arti-\\ncle, the authors introduce a symmetric pattern\\nbased approach to word representation which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='The problem of antonyms was tackled di-\\nrectly by [ Schwartz et al., 2015]. In this arti-\\ncle, the authors introduce a symmetric pattern\\nbased approach to word representation which\\nis particularly suitable for capturing word sim-\\nilarity. Symmetric patterns are a special type\\nof patterns that contain exactly two wildcards\\nand that tend to be instantiated by wildcard\\npairs such that each member of the pair can\\ntake the X or the Y position. For example, the\\nsymmetry of the pattern \"X or Y\" is exempli-\\n\ufb01ed by the semantically plausible expressions\\n\"cats or dogs\" and \"dogs or cats\". Speci\ufb01cally\\nit was found that two patterns are particularly\\nindicative of antonymy - \"from X to Y\" and\\n\"either X or Y\".\\nUsing their model the authors were able\\nto achieve a \u03c1 score of 0.56 on the simlex999\\ndataset [ Hill et al., 2016], improving state-of-\\nthe-art word2vec skip-gram model results by\\nas much as 5.5-16.7%. Furthermore, the au-\\nthors demonstrated the adaptability of their'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dataset [ Hill et al., 2016], improving state-of-\\nthe-art word2vec skip-gram model results by\\nas much as 5.5-16.7%. Furthermore, the au-\\nthors demonstrated the adaptability of their\\nmodel to antonym judgment speci\ufb01cations.\\nLinear analogy relationships: A more\\ninteresting property of recent embeddings\\n[Mikolov et. al., 2013 ] is that they can solve\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nanalogy relationships via linear algebra. This\\nis despite the fact that those embeddings are\\nbeing produced via nonlinear methods. For\\nexample, vqueen is the most similar answer to\\nthe vking \u2212vmen + vwomen equation. It turns out,\\nthough, that much more sophisticated relation-\\nships are also encoded in this way as we can\\nsee in \ufb01gure 5 below.\\nFigure 5: Relationship pairs in a word embedding. From\\n[Mikolov et. al., 2013]\\nAn interesting theoretical work on non-linear\\nembeddings (especially PMI) was done by\\n[Arora et al., 2015]. In their article they sug-\\ngest that the creation of a textual corpus is\\ndriven by the random walk of a discourse vec-\\ntor ct \u2208\u211cd, which is a unit vector whose direc-\\ntion in space represents what is being talked\\nabout. Each word has a (time-invariant) la-\\ntent vector vw \u2208\u211cd that captures its correla-\\ntions with the discourse vector. Using a word\\nproduction model they predict that words oc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='about. Each word has a (time-invariant) la-\\ntent vector vw \u2208\u211cd that captures its correla-\\ntions with the discourse vector. Using a word\\nproduction model they predict that words oc-\\ncurring at successive time steps will also tend\\nto have vectors that are close together, thus\\nexplaining why similar words have similar vec-\\ntors.\\nUsing the above model the authors introduce\\nthe \"RELATIONS = DIRECTIONS\" notion for\\nlinear analogies. The authors claim that for\\neach relation R, some direction \u00b5R can be found\\nwhich satis\ufb01es some equation. This leads to\\nthe \ufb01nding that given enough examples of a\\nrelationship R, it is possible to compute \u00b5R us-\\ning SVD and then given a pair of words with a\\nrealtion R and a word c, \ufb01nd the best analogy\\nwith word d by \ufb01nding the pair c and d such\\nthat vc \u2212vd has highest possible projection over\\n\u00b5R. In this way, thay also explain that low di-\\nmension of the vectors has a \"purifying\" effect\\nthat reduces the effect of the over\ufb01tting coming'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='that vc \u2212vd has highest possible projection over\\n\u00b5R. In this way, thay also explain that low di-\\nmension of the vectors has a \"purifying\" effect\\nthat reduces the effect of the over\ufb01tting coming\\nfrom the PMI approximation, thus achieving\\nmuch better results than higher dimensional\\nvectors.\\nv. Word Embeddings Extensions\\nIn this last subsection we will review two inter-\\nesting works that extend the word embedding\\nconcept to phrases and sentences using differ-\\nent approaches.\\nIn [ Mitchell and Lapata, 2008] the authors\\naddress the problem that vector-based mod-\\nels are typically directed at representing words\\nin isolation and methods for constructing rep-\\nresentations for phrases or sentences have re-\\nceived little attention in the literature. The\\nauthors suggests the use of two composition\\noperations, multiplication and addition (and\\ntheir combination). This way the authors are\\nable to combine word embeddings into phrase\\nor sentences embeddings while taking into ac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='operations, multiplication and addition (and\\ntheir combination). This way the authors are\\nable to combine word embeddings into phrase\\nor sentences embeddings while taking into ac-\\ncount important properties like word order\\nand semantic relationship between words (i.e.\\nsemantic composition types).\\nIn MIL (Multi Instance Transfer Learning)\\n[Kotzias et al., 2014] the authors propose a neu-\\nral network model which learns embedding\\nat increasing level of hierarchy, starting from\\nword embeddings, going to sentences and end-\\ning with entire document embeddings. The au-\\nthors then use transfer learning by pulling the\\nsentence or word embedding that were trained\\nas part of the document embeddings and use\\nthem for sentence or word review classi\ufb01cation\\nor similarity tasks (See \ufb01gure 6 below).\\nIII. W ord Embeddings vs. Image\\nEmbeddings\\ni. Image Embeddings\\nImage embeddings, or image features, were\\nwildly used for most image processing and\\nclassi\ufb01cation tasks until the early 2010\u2019s. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='III. W ord Embeddings vs. Image\\nEmbeddings\\ni. Image Embeddings\\nImage embeddings, or image features, were\\nwildly used for most image processing and\\nclassi\ufb01cation tasks until the early 2010\u2019s. The\\nfeatures ranged from simple histograms or\\nedge maps to the more sophisticated and\\nvery popular SIFT [ Lowe, 1999] and HOG\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 6: Deep multi-instance transfer learning\\napproach for review data, taken from\\n[Kotzias et al., 2014]\\n[Dalal and Triggs, 2005]. However, recent\\nyears have seen the rise of Deep Learning\\nfor image classi\ufb01cation, especially since 2012\\nwhen the AlexNet [Krizhevsky et al., 2012] ar-\\nticle was published. As those Convolutional\\nNeural Networks (CNN) operated directly on\\nthe images, it was suggested that these net-\\nworks learn the best image features for the spe-\\nci\ufb01c task that they are trained for, thus obviat-\\ning the need for speci\ufb01c hand-crafted features.\\nFigure 7: The CNN architecture of AlexNet\\nIn recent years though, an extensive research\\nwas done on the nature and usage of the ker-\\nnels and features learned by CNN\u2019s. Exten-\\nsive study of CNN feature layers was done\\nin [Zeiler and Fergus, 2014] where they empir-\\nically con\ufb01rmed that each convolutional layer\\nof the CNN learns a set of \ufb01lters. Their ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sive study of CNN feature layers was done\\nin [Zeiler and Fergus, 2014] where they empir-\\nically con\ufb01rmed that each convolutional layer\\nof the CNN learns a set of \ufb01lters. Their ex-\\nperiments also con\ufb01rm that \ufb01lters complexity\\nand expressive power is rising from layer to\\nlayer (i.e. as the network goes deeper) starting\\nfrom simple edge detectors to complex objects\\ndetectors like eyes, \ufb02owers, faces and more.\\nThe authors also suggest using the pre-trained\\none before last layer as a feature map, or Image\\nEmbeddings input for simpler SVM classi\ufb01ers.\\nAnother popular work was done a bit\\nearlier in [ Yangqing et al., 2014] where they\\nalso used a pre-trained CNN features as a\\nbase for visual recognition tasks. This work\\nwas followed by several works with one of\\nthem being considered the philosophical fa-\\nther of the algorithm we implement later. In\\n[Razavian et al., 2014] the authors used the\\none before last layer of a network similar to\\nAlexNet that was pre-trained on ImageNet'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ther of the algorithm we implement later. In\\n[Razavian et al., 2014] the authors used the\\none before last layer of a network similar to\\nAlexNet that was pre-trained on ImageNet\\n[Russakovsky et al., 2015] as image embed-\\ndings. The authors were able to acheive state-\\nof-art results on several recognition tasks, us-\\ning simple classi\ufb01ers like SVM. The result was\\nsurprising due to the fact that the CNN model\\nwas originally optimized for the task of object\\nclassi\ufb01cation in ILSVRC 2013 dataset. Never-\\ntheless, it showed itself to be a strong competi-\\ntor to the more sophisticated and highly tuned\\nstate-of-the-art methods.\\nThese works and others suggested that given\\na large enough database of images, a CNN can\\nlearn an image embedding which captures the\\n\"essence\" of the picture and can be used later\\nas an input to different tasks, similar to what\\nis done with word embeddings.\\nii. Similarities and Differences\\nAs we saw earlier Word embedding and Im-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='\"essence\" of the picture and can be used later\\nas an input to different tasks, similar to what\\nis done with word embeddings.\\nii. Similarities and Differences\\nAs we saw earlier Word embedding and Im-\\nage embeddings are similar in the sense that\\nwhile they are being learned as part of a spe-\\nci\ufb01c task, they can be successfully used later\\nfor a variety of other tasks. Also, in both cases,\\nsimilar images or words will usually have sim-\\nilar embeddings. However Word embeddings\\nand image embeddings differ in some aspects.\\nThe \ufb01rst difference is that while word embed-\\ndings depends mostly on the words surround-\\ning the given word, image embeddings usually\\nrely on the speci\ufb01c image itself. This might ex-\\nplain the fact that linear analogies does not ap-\\npear naturally in images. An interesting work\\nwas done in [Reed et al., 2015] where a neural\\nnetwork is trained to make visual analogies\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nand learn to make them based on appearance,\\nrotation, 3D pose, and various object attributes.\\nAnother difference is that while word em-\\nbeddings are usually low-ranked, image em-\\nbeddings might have same or even higher di-\\nmension then the original image. Those em-\\nbeddings are still useful as they contain a lot\\nof information that is extracted from the image\\nand can be used easily.\\nLastly, we notice that word embeddings are\\ntrained on a speci\ufb01c corpus where the \ufb01nal\\nembedding results come as the form of word-\\nvectors. This limits the embedding to be valid\\nonly for words that were found in the original\\ncorpus while other words will need to be ini-\\ntialized as random vectors (as also done in our\\nwork). In images on the other hand, the em-\\nbeddings come as a pre-trained model where\\nfeatures or embeddings can be pulled for any\\nsort of image by feeding the model with the\\nimage, making image embeddings models a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='beddings come as a pre-trained model where\\nfeatures or embeddings can be pulled for any\\nsort of image by feeding the model with the\\nimage, making image embeddings models a\\nbit more robust (although they might subjected\\nto other constraints like size and image type).\\niii. Joint Word-Image Embeddings\\nTo conclude this part we will review some\\nof the recent work done in the exciting area\\nof joint word-image embeddings. The \ufb01rst\\nimmediate usage of joint word-image embed-\\ndings is image annotations or image label-\\ning. An early notable work was done by\\n[Weston, et al., 2010] where a representation\\nof images and representation of annotations\\nwhere both mapped to a joint feature space by\\nlearning a mapping which optimizes top-of-\\nthe-list ranking measures for images and an-\\nnotations. This method, however, learns linear\\nmappings from image features to the embed-\\nding space, and the available labels were only\\nthose provided in the image training set. It'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='notations. This method, however, learns linear\\nmappings from image features to the embed-\\nding space, and the available labels were only\\nthose provided in the image training set. It\\ncould thus not generalize to new classes.\\nIn 2014 DeVise (A Deep Visual-Semantic\\nEmbedding Model) model was shown by\\n[Frome et al., 2013]. This work which con-\\ntinued earlier work [ Socher et al., 2013 ], com-\\nbined image embedding and word embedding\\ntrained separately into joint similarity metric\\n(see \ufb01gure 8). This enabled them to give perfor-\\nmance comparable to a state-of-the-art softmax\\nbased model on a \ufb02at object classi\ufb01cation met-\\nric, while simultaneously making more seman-\\ntically reasonable errors. Their model was also\\nable to make correct predictions across thou-\\nsands of previously unseen classes by lever-\\naging semantic knowledge elicited only from\\nun-annotated text.\\nAnother line of works which combines im-\\nage and words embeddings is the image cap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sands of previously unseen classes by lever-\\naging semantic knowledge elicited only from\\nun-annotated text.\\nAnother line of works which combines im-\\nage and words embeddings is the image cap-\\ntioning area. In this area the embeddings are\\nusually not combined into a joint space but\\nrather used together to create captions for im-\\nages. In [Karpathy and Fei Fei, 2015 ] an image\\nFigure 8: : (a) Left: a visual object categorization network with a softmax output layer; Right: a skip-gram language\\nmodel; Center: the joint model, which is initialized with parameters pre-trained at the lower layers of the\\nother two models. (b) t-SNE visualization [19] of a subset of the ILSVRC 2012 1K label embeddings learned\\nusing skip-gram. Taken from [Frome et al., 2013]\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 9: Image captiones generated with Deep Visual-Semantic model Taken from [Karpathy and Fei Fei, 2015]\\nfeatures pulled from a pre-trained CNN are\\nfed into a Recurrent Neural Network (RNN)\\nwhich uses word embeddings in order to gen-\\nerate a captioning for the image, based on the\\nimage features and previous words (see \ufb01g-\\nure 9). This sort of combination appears in\\nmost image captioning works or video action\\nrecognition tasks.\\nFinally, a slightly more sophisticated method\\ncombining RNN\u2019s and Fisher Vectors can be\\nfound in [ Lev et al., 2015] where the authors\\nwere able to achieve state-of-art results on both\\nimage captioning and video action recognition\\ntasks, using transfer learning on the embed-\\ndings learned for the image captioning tasks.\\nIV . CNN for Sentence\\nClassification Model\\nIn this section and the following we are going\\nto represent our implementation of The Convo-\\nlutional Neural Networks for Sentence Classi\ufb01-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='IV . CNN for Sentence\\nClassification Model\\nIn this section and the following we are going\\nto represent our implementation of The Convo-\\nlutional Neural Networks for Sentence Classi\ufb01-\\ncation model [Kim ,2014] and our results. This\\nmodel gained much popularity since it was\\n\ufb01rst introduced in late 2014, mainly because it\\nprovides a very strong demonstration for the\\npower of pre-trained word embeddings.\\nThe model and results were examined in de-\\ntail in [ Zhang and Wallace, 2015] where they\\ntest many types of con\ufb01gurations for the model,\\nincluding different sizes and number of \ufb01lters,\\ndifferent activation units and different word\\nembbeddings.\\nA partial implementation of the model was\\ndone in Theano framework by the authors 4\\nand another simpli\ufb01ed version of the model\\nwas done in TensorFlow 5. In our work we\\nused small parts of the mentioned codes, how-\\never most of the code had to be re-written and\\nexpanded in order to perform a true implmen-\\ntation of the article\u2019s model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='used small parts of the mentioned codes, how-\\never most of the code had to be re-written and\\nexpanded in order to perform a true implmen-\\ntation of the article\u2019s model.\\ni. Model details\\nThe model architecture, shown in \ufb01gure 10,\\nis a slight variant of the CNN architecture of\\n[Collobert et al., 2011]. Formally, let xi \u2208\u211ck\\nbe the k-dimensional word vector correspond-\\ning to the i-th word in the sentence. Let n be\\nthe length (in number of words) of the longest\\nsentence in the dataset, and let lh be the width\\nof the widest \ufb01lter in the network. Then, the\\ninput to the network is a k \u00d7(n + lh \u22121) ma-\\ntrix, which is a concatenation of the word em-\\nbeddings vectors of each sentences, padded\\nby lh \u22121 zero vectors in the beginning and\\nsome more zero vectors in the end so there are\\nn + lh \u22121 vectors eventually.\\nThe input of the network is convolved with\\n\ufb01lters of different widths (i.e. number of words\\nin the window) and different sizes (i.e. number\\nof features). For example, a feature ci is gen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='The input of the network is convolved with\\n\ufb01lters of different widths (i.e. number of words\\nin the window) and different sizes (i.e. number\\nof features). For example, a feature ci is gen-\\nerated from a window of words xi:i+h\u22121 by a\\n\ufb01lter with width h is:\\nci = f (wxi:i+h\u22121 + b) (3)\\nwhere w are the \ufb01lter weights, b is a bias term\\n4https://github.com/yoonkim/CNN_sentence\\n5https://github.com/dennybritz/cnn-text-\\nclassi\ufb01cation-tf\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nFigure 10: Model architecture with two channels for an example sentence. Taken from [Kim ,2014]\\nand f is a non-linear function like ReLU. This\\nprocess is done for all \ufb01lters and for all words\\nto create a number of feature maps for each\\n\ufb01lter. Next, those features maps are then max-\\npooled (so we can deal with different sentence\\nsizes) and \ufb01nally connected to a soft-max clas-\\nsi\ufb01cation layer.\\nFor regularization we employ dropout\\n[Hinton et al., 2014] on the penultimate layer.\\nThis entails randomly (under some probabil-\\nity) setting values in the weight vector to\\n0. In the original article they also employed\\nconstraint on l2 norms of this layer, however\\n[Zhang and Wallace, 2015] found that it had\\nnegligible contribution to results and therefore\\nwas not used here.\\nTraining of the network is done by minimiz-\\ning the corss-entropy loss between predicted\\nlabels (soft-max layer) and correct ones. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='was not used here.\\nTraining of the network is done by minimiz-\\ning the corss-entropy loss between predicted\\nlabels (soft-max layer) and correct ones. The\\nparameters to be estimated include the weight\\nvector(s), of the \ufb01lter(s), the bias term in the ac-\\ntivation function, the weight vector of the soft-\\nmax function and (optionally) the word embed-\\ndings. Optimization is performed using SGD\\n[Rumelhart et al., 1988] and back-propagation,\\nwith a small mini-batch size speci\ufb01ed later.\\nV . D atasets\\nWe test our model on various benchmarks.\\nSome of them used in the original article while\\nothers are extension we do to the original work.\\nYou can see the dataset statistics summary in\\ntable 1 below.\\n\u2022 MR: Movie reviews with one sentence\\nper review. Classi\ufb01cation involves\\ndetecting positive/negative reviews\\n[Pang and Lee, 2005]6\\n\u2022 SST-1: Stanford Sentiment Treebank-an\\nextension of MR but with train/dev/test\\nsplits provided and \ufb01ne-grained la-\\nbels (very positive, positive, neutral,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[Pang and Lee, 2005]6\\n\u2022 SST-1: Stanford Sentiment Treebank-an\\nextension of MR but with train/dev/test\\nsplits provided and \ufb01ne-grained la-\\nbels (very positive, positive, neutral,\\nnegative, very negative), re-labeled by\\n[Socher et al., 2013] 7\\n\u2022 SST-2: Same as SST-1 but with neutral\\nreviews removed and binary labels.\\n\u2022 Subj: Subjectivity dataset where the task\\nis to classify a sentence as being subjective\\nor objective [Pang and Lee, 2004].\\n\u2022 TREC: TREC question dataset-task\\ninvolves classifying a question into 6 ques-\\ntion types (whether the question is about\\nperson, location, numeric information,\\n6https://www.cs.cornell.edu/people/pabo/movie-\\nreview-data/\\n7http://nlp.stanford.edu/sentiment/ Data is actually\\nprovided at the phrase-level and hence we train the model\\non both phrases and sentences but only score on sentences\\nat test time, as in [ Socher et al., 2013 ] Thus the training set\\nis an order of magnitude larger than listed in table 1.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\netc.) [Li and Roth, 2002] 8.\\n\u2022Irony: [Wallace et al., 2014] This contains\\n16,006 sentences from reddit labeled as\\nironic (or not). The dataset is imbalanced\\n(relatively few sentences are ironic).\\nThus before training, we under-sampled\\nnegative instances to make classes sizes\\nequal. This dataset was not used in\\nthe original article but was tested in\\n[Zhang and Wallace, 2015].\\n\u2022Opi: Opinions dataset, which comprises\\nsentences extracted from user reviews\\non a given topic, e.g. \"sound quality of\\nipod nano\". There are 51 such topics\\nand each topic contains approximately\\n100 sentences. The test is to classify\\nwhich opinion belongs to which topic\\n[Ganesan et al., 2010]. This dataset was\\nnot used in the original article but was\\ntested in [Zhang and Wallace, 2015].\\n\u2022Tweet: Tweets from 10 different authors.\\nClassi\ufb01cation involves classifying which\\nTweet belongs to which author 9. This\\ndataset was not used in the original article.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='\u2022Tweet: Tweets from 10 different authors.\\nClassi\ufb01cation involves classifying which\\nTweet belongs to which author 9. This\\ndataset was not used in the original article.\\n\u2022Polite: Sentnces taken Wikipedia editors\u2019\\nlogs which have 25 ranges of politeness\\n[Danescu-Niculescu-Mizil et al., 2013].\\nWe narrowed it to 2 binary classes (po-\\nlite/inpolite). This dataset was not used\\nin the original article.\\nVI. E xperimental Setup\\ni. Hyperparameters and Training\\nIn our implementation of the model we experi-\\nmented with a lof of different con\ufb01gurations.\\nEventually, since results differences were mi-\\nnor, we decided to use the same architecture\\nand parameters mentioned in the original ar-\\nticle for all experiments, with some changes\\n8http://cogcomp.cs.illinois.edu/Data/QA/QC/\\n9http://www.cs.huji.ac.il/ nogazas/pages/projects.html,\\nThanks to Noga Zaslavsky\\nData c l N |V| |Vpre | Test\\nMR 2 20 10662 18765 16488 CV\\nSST-1 5 18 11855 17836 16262 2210\\nSST-2 2 19 9613 16185 14838 1821'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Thanks to Noga Zaslavsky\\nData c l N |V| |Vpre | Test\\nMR 2 20 10662 18765 16488 CV\\nSST-1 5 18 11855 17836 16262 2210\\nSST-2 2 19 9613 16185 14838 1821\\nSubj 2 23 10000 21323 17913 CV\\nTREC 6 10 5952 9592 9125 500\\nIrony 2 75 1074 6138 5718 CV\\nOpi 51 38 7086 7310 6538 CV\\nTweet 10 39 25552 33438 17023 5964\\nPolite 2 53 4353 10135 7951 CV\\nTable 1: Summary statistics for the datasets after tok-\\nenization. c: Number of target classes. l: Av-\\nerage sentence length. N: Dataset size. |V|:\\nVocabulary size. | Vpre|: Number of words\\npresent in the set of pre-trained word vectors.\\nTest: Test set size (CV means there was no stan-\\ndard train/test split and thus 10-fold CV was\\nused).\\nmentioned below. Below is a list of parame-\\nters and speci\ufb01cations that were used for all\\nexperiments:\\n\u2022 Word embeddings: We used the pre-\\ntrained Word2Vec [ Mikolov et. al., 2013 ]\\nmentioned earlier. Each word embedding\\nis in \u211c300. For words that are not found in\\nWord2Vec we randomly initialized them'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='\u2022 Word embeddings: We used the pre-\\ntrained Word2Vec [ Mikolov et. al., 2013 ]\\nmentioned earlier. Each word embedding\\nis in \u211c300. For words that are not found in\\nWord2Vec we randomly initialized them\\nwith a uniform distribution in the range\\nof [-0.5,0.5].\\n\u2022 Filters: We used \ufb01lters with window sizes\\nof [3,4,5] with 100 features each. For\\nactivation we used ReLU.\\n\u2022 Dropout rate: 0.5.\\n\u2022 Mini-Batch size: 50.\\n\u2022 Optimizer: While AdaDelte optimizer\\n[Zeiler, 2012] was used in the original\\narticle. We decided to use the more recent\\nADAM optimizer [Kingma and Ba, 2014],\\nas it seemed to converge much faster (i.e.\\nneeded less epochs on training) and in\\nsome cases improved the results.\\n\u2022 Learning rate: 0.001. We lower it to\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nMR SST-1 SST-2 Subj TREC\\nModel Orig Ours Orig Ours Orig Ours Orig Ours Orig Ours\\nCNN-rand 76.1 76.4 45.0 41 82.7 80.2 89.6 91.1 91.2 97.6\\nCNN-static 81.0 80.3 45.5 48.1 86.8 85.4 93.0 92.5 92.8 98.2\\nCNN-non-static 81.5 80.5 48.0 47.3 87.2 85.8 93.4 93 93.6 98.6\\nTable 2: Results on datasets that were tested in [Kim ,2014] (Orig above)\\n.\\n0.0005 after 8 epchs and to 0.00005 after\\n16 epochs. Notice that the original article\\ndidn\u2019t mention the learning rate.\\n\u2022Number of epochs: This was also not\\nmentioned in the original article but\\ncan be found in the authors code 10. We\\nused 25 epochs for the static version (see\\nModel Variations below). For non static\\nwe used either 4 (MR, SST1, SST2, Subj),\\n10 (Polite), 16 (Twitter, Opi), or 25 (TREC).\\nFor the random version we used 25 except\\nfor Tweet where we used 10, and MR and\\nSST-1 where we used 4.\\n\u2022l2-loss: We added l2-loss with \u03bb = 0.15\\non the weights and biases of the \ufb01nal layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='For the random version we used 25 except\\nfor Tweet where we used 10, and MR and\\nSST-1 where we used 4.\\n\u2022l2-loss: We added l2-loss with \u03bb = 0.15\\non the weights and biases of the \ufb01nal layer.\\nAltough this was not done in the original\\nartcle, we found it to slightly improve the\\nresults.\\nAs mentioned earlier, we decided not to use\\nl2 constrains on the norms due to their negligi-\\nble contribution.\\nii. Model Variations\\nWe experiment with several variants of the\\nmodel like in the original article.\\n\u2022CNN-rand: Our baseline model where all\\nwords are randomly initialized and then\\nmodi\ufb01ed during training.\\n10We note here that in the original article they used early\\nstopping with a dev set. However, the early stopping\\nparameters are not mentioned and experiments demanded\\na lot of coding which is behind the scope of this project.\\nWe do assume that the 25 number used in the code might\\nbe close enough to the actual number used in the article.\\n\u2022 CNN-static: model with pre-trained vec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='We do assume that the 25 number used in the code might\\nbe close enough to the actual number used in the article.\\n\u2022 CNN-static: model with pre-trained vec-\\ntors from word2vec. All words-including\\nthe unknown ones that are randomly\\ninitialized-are kept static and only the\\nother parameters of the model are learned.\\n\u2022 CNN-non-static: Same as above but the\\npretrained vectors are \ufb01ne-tuned for each\\ntask.\\nThe authors also used a multi-channel model\\nwhere one channel is static and the other is not.\\nHowever, experiments showed that on most\\ndatasets, this did not improve the results. As\\nimplementing this would have required a lot\\nmore coding, we decided to drop it.\\nVII. R esults and discussion\\nIn this section we will compare the results we\\ngot in our implementation to the ones achieved\\nin the original article. Full results can be found\\nin the original article, and we do note that\\nmost of them are state-of-art results, or com-\\nparable. For datasets that were not present in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='in the original article. Full results can be found\\nin the original article, and we do note that\\nmost of them are state-of-art results, or com-\\nparable. For datasets that were not present in\\nthe original article we shall compare with other\\nachieved results, whether ours or others\u2019.\\nIn table 2 above we can see a comparison\\nbetween our results and the ones in the origi-\\nnal article [Kim ,2014]. We can see that overall,\\nour results are comparable (and sometimes bet-\\nter) to the ones in the original article. We also\\nsee that like in the original article, our base-\\nline model with all randomly initialized words\\n(CNN-rand) does not perform well on its own\\n(on most cases). These results suggest that\\nthe pre-trained vectors are good, \u2019universal\u2019\\nfeature extractors and can be utilized across\\ndatasets. Finetuning the pre-trained vectors\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nModel Opi Irony Tweet Polite\\nRandom 64.8 60.2 89.1 66.2\\nStatic 65.3 60.5 83.8 66.2\\nNon-Static 66.4 62.1 89.2 65.7\\nConvSent 64.9 67.112 - -\\nSVM+TF-IDF - - 92.5 -\\nTable 3: Resutls for datasets that were not used\\nin the original article. Convesent is\\n[Zhang and Wallace, 2015].\\nfor each task gives still further improvements\\n(CNN-non-static).\\nThe differences in some of our results can\\nbe related to the different optimizer we used,\\nand the fact that we did not use early stopping.\\nWe do note that our results (at least on the\\nnon-static version) were achieved with much\\nless training than the original article11. We also\\nnote that on the TREC dataset we were able to\\nachieve a new state-of-art results, improving\\nthe current one ( 95%) by 3.6%. Both these\\nbene\ufb01ts can be related to the use of ADAM\\n[Kingma and Ba, 2014] optimizer.\\nOn table 3 we can see our results for datasets\\nthat were not used in the original article. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='bene\ufb01ts can be related to the use of ADAM\\n[Kingma and Ba, 2014] optimizer.\\nOn table 3 we can see our results for datasets\\nthat were not used in the original article. We\\nalso compare them to other results where ap-\\nplicable.\\nOn the Opi and Irony dataset we note that\\nthe general line of improved results with pre-\\ntrained vectors is maintained. On the Opi\\ndataset we were also able to achieve a new\\nstate-of-art result. We were also able to achieve\\ncomparable results on the Irony dataset. No-\\ntice that the other reported result is AUC and\\nnot accuracy.\\nThe other two results are interesting. On the\\nTweet dataset we notice that random vectors\\nactually perform a lot better than pre-trained\\nstatic ones. The reason is that on this dataset,\\nalmost half of the vocabulary was not found in\\nthe Word2Vec embeddings. This makes sense,\\nas tweets usually contain a lot of marks (for\\nexample :-) ) and hashtags which would natu-\\nrally will not be available in embeddings that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the Word2Vec embeddings. This makes sense,\\nas tweets usually contain a lot of marks (for\\nexample :-) ) and hashtags which would natu-\\nrally will not be available in embeddings that\\n11That, if we take the 25 epochs in the code we men-\\ntioned earlier as an indication to the nubmer of epochs\\ntraining used in the original article\\nwere trained on news. This makes the static\\nversion a bad choice as it keep the embeddings\\nrandom during training.\\nOn this dataset we also applied a simple\\nSVM classi\ufb01er on the TF-IDF features of each\\ntweet. This simple classi\ufb01er produced much\\nbetter results, as TF-IDF features are sensitive\\nto uniqe words in a tweet (like hashtags), that\\nusually indicates which is the author, thus mak-\\ning classi\ufb01cation easier.\\nOn the Poilte dataset we notice that results\\ndoes not matter on the choice of model. The\\nresults themselves are also not very good. This\\nresults needs further inspection but they might\\nsuggests that this model is not \ufb01tted for this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='does not matter on the choice of model. The\\nresults themselves are also not very good. This\\nresults needs further inspection but they might\\nsuggests that this model is not \ufb01tted for this\\ntask or that politeness is a complicated task for\\nautomatic classi\ufb01cation.\\nVIII. C onclusions andFuture\\nDirections\\nIn this work we reviewed word embeddings.\\nWe saw their origins, discussed the different\\nmethods for creating them, and saw some of\\ntheir interesting properties. We think that word\\nembeddings is a very exciting topic for both\\nresearch and applications, and expect that a\\nlot research is going to be carried for better\\nunderstanding of their properties and better\\ncreation methods.\\nIn this work we also compared Image fea-\\ntures and word embeddings and saw how they\\ncan be combined to build learning algorithms\\nthat can \ufb01nally gain a good understanding of\\npictures and scenes. This area is just in its be-\\nginning and we expect a lot of work to be car-\\nried towards creating a hybrid system which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='that can \ufb01nally gain a good understanding of\\npictures and scenes. This area is just in its be-\\nginning and we expect a lot of work to be car-\\nried towards creating a hybrid system which\\ngains understanding of both vision and lan-\\nguage, and that combines those understand-\\nings together to the bene\ufb01t of both \ufb01elds.\\nFinally, we saw that despite little tuning of\\nhyperparameters, a simple CNN with one layer\\nof convolution, trained on top of Word2Vec\\nembeddings, performs remarkably well on sen-\\ntence classi\ufb01cation tasks. These results add\\nto the well-established evidence that unsuper-\\nvised pre-training of word vectors is an impor-\\ntant ingredient in deep learning for NLP .\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nTo conclude this work we propose here two\\nlines for future work that we think might be\\ninteresting to check. First, in the spirit of\\n[Kotzias et al., 2014], we notice that in our net-\\nwork, the one before last layer is actually learn-\\ning sentence embeddings. It might be interest-\\ning to train the network on some classi\ufb01cation\\ntask with a relatively large dataset, and then\\nuse the learned sentence embeddings in the\\nsame fashion word embeddings are used in\\nour work. For example we can train the net-\\nwork on the MR task and then take the learned\\nsentence embeddings and use them as an em-\\nbedding input to some document classi\ufb01cation\\ntask. We can then check if this method achieves\\nimprovement over models that try to classify\\ndocuments using only pre-trained word em-\\nbeddings.\\nThe second line of research is in the spirit of\\n[Zeiler and Fergus, 2014]. ConvNets visualiza-\\ntion helped to gain a lot of insights about image'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='documents using only pre-trained word em-\\nbeddings.\\nThe second line of research is in the spirit of\\n[Zeiler and Fergus, 2014]. ConvNets visualiza-\\ntion helped to gain a lot of insights about image\\nsturctre and how features in increasing level\\nof complexity are combined to create images.\\nIt might be interesting to apply those same\\nmethod of visualization to the \ufb01lters used in\\nour, or similar works and see if the ConvNet\\n\ufb01lters learn some interesting semantic proper-\\nties or compositions that can give insights on\\nthe structure of language and how computers\\n(or even humans) percept them.\\nReferences\\n[Arora et al., 2015] Arora, Sanjeev, Yuanzhi Li,\\nYingyu Liang, Tengyu Ma, and Andrej\\nRisteski. \"Rand-walk: A latent variable\\nmodel approach to word embeddings.\"\\narXiv preprint arXiv:1502.03520 (2015).\\n[Collobert and Weston, 2008] Collobert, Ro-\\nnan, and Jason Weston. \"A uni\ufb01ed ar-\\nchitecture for natural language process-\\ning: Deep neural networks with multitask'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[Collobert and Weston, 2008] Collobert, Ro-\\nnan, and Jason Weston. \"A uni\ufb01ed ar-\\nchitecture for natural language process-\\ning: Deep neural networks with multitask\\nlearning.\" Proceedings of the 25th inter-\\nnational conference on Machine learning.\\nACM. (2008).\\n[Collobert et al., 2011] Collobert, Ronan, Jason\\nWeston, L\u00c3l\u2019on Bottou, Michael Karlen,\\nKoray Kavukcuoglu, and Pavel Kuksa.\\n\"Natural language processing (almost)\\nfrom scratch.\" Journal of Machine Learn-\\ning Research 12, no. Aug (2011): 2493-\\n2537.\\n[Dalal and Triggs, 2005] Dalal, Navneet, and\\nBill Triggs. \"Histograms of oriented gra-\\ndients for human detection.\" 2005 IEEE\\nComputer Society Conference on Com-\\nputer Vision and Pattern Recognition\\n(CVPR\u201905). Vol. 1. IEEE, 2005.\\n[Danescu-Niculescu-Mizil et al., 2013]\\nDanescu-Niculescu-Mizil, Cristian,\\nMoritz Sudhof, Dan Jurafsky, Jure\\nLeskovec, and Christopher Potts. \"A\\ncomputational approach to politeness\\nwith application to social factors.\" arXiv\\npreprint arXiv:1306.6078 (2013).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Moritz Sudhof, Dan Jurafsky, Jure\\nLeskovec, and Christopher Potts. \"A\\ncomputational approach to politeness\\nwith application to social factors.\" arXiv\\npreprint arXiv:1306.6078 (2013).\\n[Deerwester et al., 1990] Deerwester, Scott, Su-\\nsan T. Dumais, George W. Furnas, Thomas\\nK. Landauer, and Richard Harshman. \"In-\\ndexing by latent semantic analysis.\" Jour-\\nnal of the American society for informa-\\ntion science 41, no. 6 (1990): 391.\\n[Firth, 1957] Firth, J.R. (1957). \"A synopsis of\\nlinguistic theory 1930-1955\". Studies in\\nLinguistic Analysis (Oxford: Philological\\nSociety): 1-32. Reprinted in F.R. Palmer,\\ned. (1968). Selected Papers of J.R. Firth\\n1952-1959. London: Longman.\\n[Frome et al., 2013] Frome, Andrea, Greg S.\\nCorrado, Jon Shlens, Samy Bengio, Jeff\\nDean, and Tomas Mikolov. \"Devise: A\\ndeep visual-semantic embedding model.\"\\nIn Advances in neural information pro-\\ncessing systems, pp. 2121-2129. 2013.\\n[Ganesan et al., 2010] Ganesan, Kavita,\\nChengXiang Zhai, and Jiawei Han.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='deep visual-semantic embedding model.\"\\nIn Advances in neural information pro-\\ncessing systems, pp. 2121-2129. 2013.\\n[Ganesan et al., 2010] Ganesan, Kavita,\\nChengXiang Zhai, and Jiawei Han.\\n\"Opinosis: a graph-based approach to\\nabstractive summarization of highly\\nredundant opinions.\" Proceedings of\\nthe 23rd international conference on\\ncomputational linguistics. Association for\\nComputational Linguistics, 2010.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\n[Goldberg and Levy, 2014] Goldberg, Yoav,\\nand Omer Levy. \"word2vec Explained: de-\\nriving Mikolov et al.\u2019s negative-sampling\\nword-embedding method.\" arXiv preprint\\narXiv:1402.3722 (2014).\\n[Gutmann and Hyv\u00c3d\u2019rinen, 2012] Gutmann,\\nMichael U., and Aapo Hyv\u00c3d\u2019rinen.\\n\"Noise-contrastive estimation of un-\\nnormalized statistical models, with\\napplications to natural image statistics.\"\\nJournal of Machine Learning Research\\n13.Feb (2012): 307-361.\\n[Hill et al., 2016] Hill, Felix, Roi Reichart, and\\nAnna Korhonen. \"Simlex-999: Evaluating\\nsemantic models with (genuine) similar-\\nity estimation.\" Computational Linguistics\\n(2016).\\n[Hinton, 1986] Hinton, Geoffrey E. \"Dis-\\ntributed representations.\" Parallel Dis-\\ntributed Processing: Explorations in the\\nMicrostructure of Cognition (1986).\\n[Hinton et al., 2014] Srivastava, Nitish, Geof-\\nfrey E. Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tributed Processing: Explorations in the\\nMicrostructure of Cognition (1986).\\n[Hinton et al., 2014] Srivastava, Nitish, Geof-\\nfrey E. Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov.\\n\"Dropout: a simple way to prevent neural\\nnetworks from over\ufb01tting.\" Journal of Ma-\\nchine Learning Research 15, no. 1 (2014):\\n1929-1958.\\n[Karpathy and Fei Fei, 2015] Karpathy, An-\\ndrej, and Li Fei-Fei. \"Deep visual-semantic\\nalignments for generating image de-\\nscriptions.\" Proceedings of the IEEE\\nConference on Computer Vision and\\nPattern Recognition. 2015.\\n[Kenneth et al., 1990] Church, Kenneth Ward,\\nand Patrick Hanks. \"Word association\\nnorms, mutual information, and lexi-\\ncography.\" Computational linguistics 16.1\\n(1990): 22-29.\\n[Kim ,2014] Kim, Yoon. \"Convolutional neu-\\nral networks for sentence classi\ufb01cation.\"\\narXiv preprint arXiv:1408.5882 (2014).\\n[Kingma and Ba, 2014] Kingma, Diederik,\\nand Jimmy Ba. \"Adam: A method for\\nstochastic optimization.\" arXiv preprint\\narXiv:1412.6980 (2014).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:1408.5882 (2014).\\n[Kingma and Ba, 2014] Kingma, Diederik,\\nand Jimmy Ba. \"Adam: A method for\\nstochastic optimization.\" arXiv preprint\\narXiv:1412.6980 (2014).\\n[Kotzias et al., 2014] Kotzias, Dimitrios, Misha\\nDenil, Phil Blunsom, and Nando de\\nFreitas. \"Deep multi-instance transfer\\nlearning.\" arXiv preprint arXiv:1411.3128\\n(2014).\\n[Krizhevsky et al., 2012] Krizhevsky, Alex,\\nIlya Sutskever, and Geoffrey E. Hinton.\\n\"Imagenet classi\ufb01cation with deep convo-\\nlutional neural networks.\" Advances in\\nneural information processing systems.\\n2012.\\n[Lev et al., 2015] Lev, Guy, Gil Sadeh, Ben-\\njamin Klein, and Lior Wolf. \"RNN\\nFisher Vectors for Action Recognition\\nand Image Annotation.\" arXiv preprint\\narXiv:1512.03958 (2015).\\n[Li and Roth, 2002] Li, Xin, and Dan Roth.\\n\"Learning question classi\ufb01ers.\" Proceed-\\nings of the 19th international conference\\non Computational linguistics-Volume 1.\\nAssociation for Computational Linguis-\\ntics, 2002.\\n[Lowe, 1999] Lowe, David G. \"Object recogni-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ings of the 19th international conference\\non Computational linguistics-Volume 1.\\nAssociation for Computational Linguis-\\ntics, 2002.\\n[Lowe, 1999] Lowe, David G. \"Object recogni-\\ntion from local scale-invariant features.\"\\nComputer vision, 1999. The proceedings\\nof the seventh IEEE international confer-\\nence on. Vol. 2. Ieee, 1999.\\n[Maaten and Hinton, 2008] Maaten, Laurens\\nvan der, and Geoffrey Hinton. \"Visualiz-\\ning data using t-SNE.\" Journal of Machine\\nLearning Research 9.Nov (2008): 2579-\\n2605.\\n[Mikolov et. al., 2013] Mikolov, Tomas, Kai\\nChen, Greg Corrado, and Jeffrey Dean.\\n\"Ef\ufb01cient estimation of word represen-\\ntations in vector space.\" arXiv preprint\\narXiv:1301.3781 (2013).\\n[Mitchell et al., 2008] Mitchell, Tom M., Svet-\\nlana V . Shinkareva, Andrew Carlson, Kai-\\nMin Chang, Vicente L. Malave, Robert A.\\nMason, and Marcel Adam Just. \"Predict-\\ning human brain activity associated with\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nthe meanings of nouns.\" science 320, no.\\n5880 (2008): 1191-1195.\\n[Mitchell and Lapata, 2008] Mitchell, Jeff, and\\nMirella Lapata. \"Vector-based Models of\\nSemantic Composition.\" ACL. 2008.\\n[Osgood, 1964] Osgood, Charles E. \"Semantic\\ndifferential technique in the comparative\\nstudy of cultures.\" American Anthropolo-\\ngist 66.3 (1964): 171-200.\\n[Pang and Lee, 2004] Pang, B., Lee, L. A sen-\\ntimental education: Sentiment analysis\\nusing subjectivity summarization based\\non minimum cuts. In Proceedings of the\\n42nd annual meeting on Association for\\nComputational Linguistics (p. 271). As-\\nsociation for Computational Linguistics.\\n2004.\\n[Pang and Lee, 2005] Pang, Bo, and Lillian\\nLee. \"Seeing stars: Exploiting class re-\\nlationships for sentiment categorization\\nwith respect to rating scales.\" Proceedings\\nof the 43rd annual meeting on association\\nfor computational linguistics. Association\\nfor Computational Linguistics, 2005.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='with respect to rating scales.\" Proceedings\\nof the 43rd annual meeting on association\\nfor computational linguistics. Association\\nfor Computational Linguistics, 2005.\\n[Pennington et. al., 2014] Pennington, Jeffrey,\\nRichard Socher, and Christopher D. Man-\\nning. \"Glove: Global Vectors for Word\\nRepresentation.\" EMNLP . Vol. 14. (2014).\\n[Razavian et al., 2014] Sharif Razavian, Ali,\\nHossein Azizpour, Josephine Sullivan,\\nand Stefan Carlsson. \"CNN features off-\\nthe-shelf: an astounding baseline for\\nrecognition.\" In Proceedings of the IEEE\\nConference on Computer Vision and Pat-\\ntern Recognition Workshops, pp. 806-813.\\n2014.\\n[Reed et al., 2015] Reed, Scott E., Yi Zhang,\\nYuting Zhang, and Honglak Lee. \"Deep\\nvisual analogy-making.\" In Advances in\\nNeural Information Processing Systems,\\npp. 1252-1260. 2015.\\n[Rumelhart et al., 1988] Rumelhart, David E.,\\nGeoffrey E. Hinton, and Ronald J.\\nWilliams. \"Learning representations by\\nback-propagating errors.\" Cognitive mod-\\neling 5.3 (1988): 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[Rumelhart et al., 1988] Rumelhart, David E.,\\nGeoffrey E. Hinton, and Ronald J.\\nWilliams. \"Learning representations by\\nback-propagating errors.\" Cognitive mod-\\neling 5.3 (1988): 1.\\n[Russakovsky et al., 2015] Russakovsky, Olga,\\nJia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang\\net al. \"Imagenet large scale visual recog-\\nnition challenge.\" International Journal of\\nComputer Vision 115, no. 3 (2015): 211-\\n252.\\n[Salton et al., 1975] Salton, Gerard, Anita\\nWong, and Chung-Shu Yang. \"A vector\\nspace model for automatic indexing.\"\\nCommunications of the ACM 18.11 (1975):\\n613-620.\\n[Schwartz et al., 2015] Schwartz, Roy, Roi Re-\\nichart, and Ari Rappoport. \"Symmetric\\npattern based word embeddings for im-\\nproved word similarity prediction.\" Proc.\\nof CoNLL. 2015.\\n[Socher et al., 2013] Socher, Richard, Alex\\nPerelygin, Jean Y. Wu, Jason Chuang,\\nChristopher D. Manning, Andrew Y. Ng,\\nand Christopher Potts. \"Recursive deep\\nmodels for semantic compositionality over'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[Socher et al., 2013] Socher, Richard, Alex\\nPerelygin, Jean Y. Wu, Jason Chuang,\\nChristopher D. Manning, Andrew Y. Ng,\\nand Christopher Potts. \"Recursive deep\\nmodels for semantic compositionality over\\na sentiment treebank.\" In Proceedings of\\nthe conference on empirical methods in\\nnatural language processing (EMNLP),\\nvol. 1631, p. 1642. 2013.\\n[Socher et al., 2013] Socher, Richard, Milind\\nGanjoo, Christopher D. Manning, and\\nAndrew Ng. \"Zero-shot learning through\\ncross-modal transfer.\" In Advances in neu-\\nral information processing systems, pp.\\n935-943. 2013.\\n[Wallace et al., 2014] Byron C Wallace, Laura\\nKertz Do Kook Choe, and Eugene Char-\\nniak. Humans require context to infer\\nironic intent (so computers probably do,\\ntoo). In Proceedings of the Annual Meet-\\ning of the Association for Computational\\nLinguistics (ACL), 2014, pages 512-516.\\n[Weston, et al., 2010] Weston, Jason, Samy\\nBengio, and Nicolas Usunier. \"Large scale\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Word Embeddings for Sentence Classi\ufb01cation Tasks \u2022July 2016\\nimage annotation: learning to rank with\\njoint word-image embeddings.\" Machine\\nlearning 81.1 (2010): 21-35.\\n[Yangqing et al., 2014] Jia, Yangqing, Evan\\nShelhamer, Jeff Donahue, Sergey Karayev,\\nJonathan Long, Ross Girshick, Sergio\\nGuadarrama, and Trevor Darrell. \"Caffe:\\nConvolutional architecture for fast feature\\nembedding.\" In Proceedings of the 22nd\\nACM international conference on Multi-\\nmedia, pp. 675-678. ACM, 2014.\\n[Zhang and Wallace, 2015] Zhang, Ye, and By-\\nron Wallace. \"A Sensitivity Analysis of\\n(and Practitioners\u2019 Guide to) Convolu-\\ntional Neural Networks for Sentence Clas-\\nsi\ufb01cation.\"arXiv preprint arXiv:1510.03820\\n(2015).\\n[Zeiler, 2012] Zeiler, Matthew D. \"ADADELTA:\\nan adaptive learning rate method.\" arXiv\\npreprint arXiv:1212.5701 (2012).\\n[Zeiler and Fergus, 2014] Zeiler, Matthew D.,\\nand Rob Fergus. \"Visualizing and under-\\nstanding convolutional networks.\" Euro-\\npean Conference on Computer Vision.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-10-27T00:23:22+00:00', 'author': '', 'keywords': '', 'moddate': '2016-10-27T00:23:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/embeddings.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:1212.5701 (2012).\\n[Zeiler and Fergus, 2014] Zeiler, Matthew D.,\\nand Rob Fergus. \"Visualizing and under-\\nstanding convolutional networks.\" Euro-\\npean Conference on Computer Vision.\\nSpringer International Publishing, 2014.\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani\u2217\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\u2217\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\u2217\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\u2217\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\u2217\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n\u0141ukasz Kaiser \u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\u2217\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\u2020Work performed while at Google Brain.\\n\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more dif\ufb01cult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n\u221adk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1\u221adk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='of 1\u221adk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221adk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='we found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = \u2211dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni \u2208Rdmodel\u00d7dk , WK\\ni \u2208Rdmodel\u00d7dk , WV\\ni \u2208Rdmodel\u00d7dv\\nand WO \u2208Rhdv\u00d7dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by \u221admodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 \u00b7d) O(1) O(1)\\nRecurrent O(n\u00b7d2) O(n) O(n)\\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and \ufb01xed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to 10000 \u00b72\u03c0. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any \ufb01xed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi \u2208Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\u00b7n\u00b7d+ n\u00b7d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5= 10\u22129. We varied the learning\\nrate over the course of training, according to the formula:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5= 10\u22129. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d\u22120.5\\nmodel \u00b7min(step_num\u22120.5,step_num\u00b7warmup_steps\u22121.5) (3)\\nThis corresponds to increasing the learning rate linearly for the \ufb01rst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 \u00b71020\\nGNMT + RL [31] 24.6 39.92 2.3 \u00b71019 1.4 \u00b71020\\nConvS2S [8] 25.16 40.46 9.6 \u00b71018 1.5 \u00b71020\\nMoE [26] 26.03 40.56 2.0 \u00b71019 1.2 \u00b71020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 \u00b71020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 \u00b71020 1.1 \u00b71021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 \u00b71019 1.2 \u00b71021\\nTransformer (base model) 27.3 38.1 3.3 \u00b7 1018\\nTransformer (big) 28.4 41.0 2.3 \u00b71019\\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The con\ufb01guration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty \u03b1= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of \ufb02oating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision \ufb02oating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='single-precision \ufb02oating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop \u03f5ls\\ntrain PPL BLEU params\\nsteps (dev) (dev) \u00d7106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770\u2013778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recognition, pages 770\u2013778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735\u20131780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\\nLearning Research, 15(1):1929\u20131958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract\u2014Due to object detection\u2019s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassi\ufb01ers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modi\ufb01cations and useful tricks\\nto improve detection performance further. As distinct speci\ufb01c\\ndetection tasks exhibit different characteristics, we also brie\ufb02y\\nsurvey several speci\ufb01c tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms\u2014deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classi\ufb01cation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classi\ufb01cation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these \ufb01elds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]\u2013[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it\u2019s dif\ufb01-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this \ufb01eld in recent years [15]\u2013[18].\\nThe problem de\ufb01nition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classi\ufb01ca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classi\ufb01cation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan \ufb01nd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan \ufb01nd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a \ufb01xed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit\u2019s dif\ufb01cult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit\u2019s dif\ufb01cult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassi\ufb01cation. Besides, a classi\ufb01er is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classi\ufb01ers, the\\nDPM is a \ufb02exible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninef\ufb01cient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signi\ufb01cant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classi\ufb01cation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a \ufb01xed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]\u2013[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversi\ufb01ed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversi\ufb01ed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classi\ufb01cation and object detection, but\\npays little attention on detailing speci\ufb01c algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speci\ufb01c tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the over\ufb01tting of training, lack of large scale\\ntraining data, limited computation power and insigni\ufb01cance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n\u2022The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n\u2022Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n\u2022Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n\u2022Signi\ufb01cant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\nover\ufb01tting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite ef\ufb01cient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton\u2019s group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton\u2019s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and \u2018dropout\u2019 regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose \u2018pixel\u2019 can be viewed as a speci\ufb01c feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive \ufb01eld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as \ufb01ltering and pooling. Filtering (convolution)\\noperation convolutes a \ufb01lter matrix (learned weights) with\\nthe values of a receptive \ufb01eld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain \ufb01nal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain \ufb01nal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive \ufb01eld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be \ufb01ne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the \ufb01nal layer with different activation functions [6]\\nis added to get a speci\ufb01c conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassi\ufb01cation layer. The conv feature maps are produced by\\nconvoluting 3*3 \ufb01lter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n\u2022Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n\u2022 Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n\u2022 Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n\u2022 The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classi\ufb01cation and bounding box regression\\ninto a multi-task leaning manner).\\n\u2022 Bene\ufb01tting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research \ufb01elds, such as image super-resolution\\nreconstruction [54], [55], image classi\ufb01cation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]\u2013[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]\u2013[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the con\ufb01dences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at \ufb01rst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classi\ufb01cation problem, adopting a uni\ufb01ed\\nframework to achieve \ufb01nal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modi\ufb01es R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modi\ufb01es R-\\nCNN with a SPP layer). The regression /classi\ufb01cation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario \ufb01rstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the con\ufb01dences of\\nunderlying object categories.\\n1) R-CNN: It is of signi\ufb01cance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the \ufb02owchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a \ufb01xed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the \ufb01nal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassi\ufb01cation and localization. With pre-trained category-\\nspeci\ufb01c linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classi\ufb01cation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012\u2014achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speci\ufb01c \ufb01ne-tuning, yields a signi\ufb01-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\n\u02dcrbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the \ufb01rst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima\u2019s \u201cneocognitron\u201d [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classi\ufb01es each\\nregion using class-speci\ufb01c linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classi\ufb01cation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun\u2019s CNN (e.g., max(x, 0) rectifying non-linearities and\\n\u201cdropout\u201d regularization).\\nThe signi\ufb01cance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Cun\u2019s CNN (e.g., max(x, 0) rectifying non-linearities and\\n\u201cdropout\u201d regularization).\\nThe signi\ufb01cance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classi\ufb01cation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classi\ufb01cation and object detection.\\nThis paper is the \ufb01rst to show that a CNN can lead to dra-\\n1\\nFig. 3. The \ufb02owchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classi\ufb01es each region with class-speci\ufb01c linear SVMs.\\nbounding box regression and \ufb01ltered with a greedy non-\\nmaximum suppression (NMS) to produce \ufb01nal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insuf\ufb01cient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and \ufb01ltered with a greedy non-\\nmaximum suppression (NMS) to produce \ufb01nal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insuf\ufb01cient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN \ufb01rstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speci\ufb01c \ufb01ne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigni\ufb01cance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n\u2022Due to the existence of FC layers, the CNN requires a\\n\ufb01xed-size (e.g., 227\u00d7227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n\u2022Training of R-CNN is a multi-stage pipeline. At \ufb01rst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n\u2022Training of R-CNN is a multi-stage pipeline. At \ufb01rst,\\na convolutional network (ConvNet) on object proposals is\\n\ufb01ne-tuned. Then the softmax classi\ufb01er learned by \ufb01ne-\\ntuning is replaced by SVMs to \ufb01t in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n\u2022 Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n\u2022Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or re\ufb01ne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speci\ufb01c CNN classi\ufb01ers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classi\ufb01cation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net:FC layers must take a \ufb01xed-size input. That\u2019s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several \ufb01ner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]\u2020 56.07 74.41 \u00b11.0\\nLLC [18]\u2020 57.66 76.95 \u00b10.4\\nFK [19]\u2020 61.69 77.78 \u00b10.6\\nDeCAF [13] - 86.91 \u00b10.7\\nZeiler & Fergus [4] 75.90\u2021 86.5\u00b10.5\\nOquab et al. [34] 77.7 -\\nChat\ufb01eld et al. [6] 82.42 88.54\u00b10.3\\nours 82.44 93.42 \u00b10.5\\nTable 8: Classi\ufb01cation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). \u2020numbers reported\\nby [27]. \u2021our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brie\ufb02y review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN \ufb01rst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN \ufb01rst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a \ufb01xed size (227 \u00d7227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classi\ufb01er is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a \ufb01xed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a \ufb01xed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\nde\ufb01ne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n\u2026...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n\u2026...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the \u201cfast\u201d mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 \u00d71, 2\u00d72, 3\u00d73, 6\u00d76, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 \u00d750) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='d (256 \u00d750) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classi\ufb01er for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classi\ufb01er\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns \u2208 S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically \ufb01nd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s \u2208 S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 \u00d7224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, \ufb01ne-tuning a network with log loss, training SVMs,\\nand \ufb01nally \ufb01tting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the \ufb01ne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='tures, \ufb01ne-tuning a network with log loss, training SVMs,\\nand \ufb01nally \ufb01tting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the \ufb01ne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (\ufb01xed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that \ufb01xes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it\u2019s comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network \ufb01rst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a \ufb01xed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that \ufb01nally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all \u201cbackground\u201d class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all \u201cbackground\u201d class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes re\ufb01ned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a \ufb01xed spatial extent of H \u00d7 W (e.g., 7 \u00d7 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\nde\ufb01ned by a four-tuple (r, c, h, w ) that speci\ufb01es its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a \ufb01xed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h \u00d7 w RoI win-\\ndow into an H \u00d7 W grid of sub-windows of approximate\\nsize h/H \u00d7 w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with \ufb01ve max pooling layers and between \ufb01ve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is con\ufb01gured by setting H and W to be\\ncompatible with the net\u2019s \ufb01rst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network\u2019s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classi\ufb01-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network\u2019s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classi\ufb01-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speci\ufb01c bounding-box regressors).\\nThird, the network is modi\ufb01ed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let\u2019s elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inef\ufb01cient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inef\ufb01ciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to \ufb01xed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inef\ufb01ciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to \ufb01xed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the \ufb01nal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe \ufb01nal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 \u00d7(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection ef\ufb01ciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='also improves detection ef\ufb01ciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and ef\ufb01ciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network \ufb01ne-tuning, SVM training and bounding-\\nbox regressor \ufb01tting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the \ufb01ne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classi\ufb01cation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a \ufb01xed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\n\ufb01nally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one \u2018background\u2019\\nclass) and the other output layer encodes re\ufb01ned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is de\ufb01ned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is de\ufb01ned as below to jointly train\\nclassi\ufb01cation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +\u03bb[u\u22651]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =\u2212logpu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,\u00b7\u00b7\u00b7 ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is de\ufb01ned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u\u22651] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u\u22651] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to \ufb01t\\nbounding-box regressors as below\\nLloc(tu,v) =\\n\u2211\\ni\u2208x,y,w,h\\nsmoothL1 (tu\\ni \u2212vi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|\u22120.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inef\ufb01cient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at \ufb01rst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nef\ufb01ciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving ef\ufb01ciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speci\ufb01c conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers\u2014a box-regression layer ( reg)\\nand a box-classi\ufb01cation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\n\ufb01eld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='and a box-classi\ufb01cation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\n\ufb01eld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n\u00d7nconv layer followed by two sibling 1 \u00d71 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n\u00d7nconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW\u00d7H(typically \u223c2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) \u00d7800-dimensional output layer,\\nwhereas our method requires a (4+2)\u00d79-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of over\ufb01tting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these de\ufb01nitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is de\ufb01ned as:\\nL({pi},{ti}) = 1\\nNcls\\n\u2211\\ni\\nLcls (pi,p\u2217\\ni ) +\u03bb 1\\nNreg\\n\u2211\\ni\\np\u2217\\ni Lreg(ti,t\u2217\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K prede\ufb01ned anchor boxes are\\nconvoluted with each sliding window to produce \ufb01xed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K prede\ufb01ned anchor boxes are\\nconvoluted with each sliding window to produce \ufb01xed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn\u00d7n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classi\ufb01cation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n\u00d7n conv layer followed by two sibling 1 \u00d71 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n\u00d7n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n\u2211\\ni\\nLcls(pi,p\u2217'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n\u2211\\ni\\nLcls(pi,p\u2217\\ni) +\u03bb 1\\nNreg\\n\u2211\\ni\\np\u2217\\niLreg (ti,t\u2217\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p\u2217\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t\u2217\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg ), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassi\ufb01cation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speci\ufb01c spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='classi\ufb01cation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speci\ufb01c spatial pooling layer.\\nRecent state-of-the-art image classi\ufb01cation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it\u2019s\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll\u00b4ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signi\ufb01cant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object\u2019s scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this \ufb01gure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='In this \ufb01gure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classi\ufb01cation. In other words, shifting an object inside\\nan image should be indiscriminative in image classi\ufb01cation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a \ufb01xed grid of k\u00d7k \ufb01rstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a \ufb01xed grid of k\u00d7k \ufb01rstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classi\ufb01cation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at \ufb01rst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at \ufb01rst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 \u00d71 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3\u00d73\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the \ufb01nal feature map is\\ngenerated. This process is iterated until the \ufb01nest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacri\ufb01cing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classi\ufb01cation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m\u00d7m mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m\u00d7m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassi\ufb01cation and bounding box regression, an additional loss\\nfor segmentation mask branch is de\ufb01ned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classi\ufb01cation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classi\ufb01cation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='It affects classi\ufb01cation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a \ufb02exible and ef\ufb01cient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modi\ufb01cation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and \u2018stuff\u2019 (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classi\ufb01cation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classi\ufb01cation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive \ufb01elds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classi\ufb01ers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modi\ufb01cations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modi\ufb01ed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and ef\ufb01cient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classi\ufb01ers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classi\ufb01er\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signi\ufb01cant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signi\ufb01cant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classi\ufb01cation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\n\ufb01rstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the \u2018deep and thin\u2019 design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the \u2018deep and thin\u2019 design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /Classi\ufb01cation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classi\ufb01cation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classi\ufb01cation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classi\ufb01cation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We \ufb01rstly reviews some pioneer CNN\\nmodels, and then focus on two signi\ufb01cant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classi\ufb01cation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classi\ufb01cation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has dif\ufb01culty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is ef\ufb01cient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uni\ufb01ed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uni\ufb01ed\\nloss was introduced to bias both localization and con\ufb01dences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the \ufb01nal layer.\\nYoo et al. adopted an iterative classi\ufb01cation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inef\ufb01cient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inef\ufb01cient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\n\ufb01nding a path from a \ufb01xed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a \ufb01xed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndif\ufb01culty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both con\ufb01dences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S\u00d7S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding con\ufb01dence scores. Formally, con\ufb01-\\ndence scores are de\ufb01ned as Pr(Object) \u2217IOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding con\ufb01dence scores. Formally, con\ufb01-\\ndence scores are de\ufb01ned as Pr(Object) \u2217IOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) \u22650) and\\nshows con\ufb01dences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speci\ufb01c con\ufb01dence scores for each box\\nare achieved by multiplying the individual box con\ufb01dence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) \u2217IOUtruth\\npred \u2217Pr(Classi|Object)\\n= Pr(Classi) \u2217IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speci\ufb01c objects in the\\nbox and the \ufb01tness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) \u2217IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speci\ufb01c objects in the\\nbox and the \ufb01tness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\n\u03bbcoord\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n[\\n(xi \u2212\u02c6xi)2 + (yi \u2212\u02c6yi)2]\\n+\u03bbcoord\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n[(\u221awi \u2212\\n\u221a\\n\u02c6wi)2 + (\\n\u221a\\nhi \u2212\\n\u221a\\n\u02c6hi\\n)2]\\n+\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 obj\\nij\\n(\\nCi \u2212\u02c6Ci\\n)2\\n+\u03bbnoobj\\nS2\\n\u2211\\ni=0\\nB\u2211\\nj=0\\n1 noobj\\nij\\n(\\nCi \u2212\u02c6Ci\\n)2\\n+\\nS2\\n\u2211\\ni=0\\n1 obj\\ni\\n\u2211\\nc\u2208classes\\n(pi(c) \u2212\u02c6pi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents con\ufb01dence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classi\ufb01cation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classi\ufb01cation errors. Similarly,\\nwhen the predictor is \u2018responsible\u2019 for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 \u00d71 reduction layers followed by 3 \u00d73 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpli\ufb01ed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a dif\ufb01culty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\ncon\ufb01gurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speci\ufb01c feature map, instead of\\n\ufb01xed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\ufb01xed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated con\ufb01dences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and con\ufb01dence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale re\ufb01ned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signi\ufb01cantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signi\ufb01cantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300\u00d7300) runs\\nat 59 FPS, which is more accurate and ef\ufb01cient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated con\ufb01dences. Final detection results are obtained by conducting NMS on multi-scale re\ufb01ned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speci\ufb01c instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classi\ufb01cation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n\u2022If incorporated with a proper way, more powerful back-\\nbone CNN models can de\ufb01nitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n\u2022 With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n\u2022Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith \u201807\u2019 ,\u201807+12\u2019 and \u201807+12+coco\u2019).\\n\u2022Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith \u201807\u2019 ,\u201807+12\u2019 and \u201807+12+coco\u2019).\\n\u2022Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\n\ufb01ed classi\ufb01cation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n\u2022As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and \ufb01ne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n\u2022By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\u2022By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n\u2022 Multi-scale training and test are bene\ufb01cial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\u2022 Multi-scale training and test are bene\ufb01cial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n\u2022 Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/class\ufb01cation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/class\ufb01cation based approaches.\\n\u2022 Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n\u2022Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n\u2022Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n\u2022The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classi\ufb01ers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classi\ufb01cation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classi\ufb01cation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object con\ufb01dence+background con\ufb01dence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object con\ufb01dence+background con\ufb01dence\\n* \u2018+\u2019 denotes that corresponding techniques are employed while \u2018-\u2019 denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* \u201807\u2019: VOC2007 trainval, \u201807+12\u2019: union of VOC2007 and VOC2012 trainval, \u201807+12+COCO\u2019: trained on COCO trainval35k at \ufb01rst and then \ufb01ne-tuned on 07+12. The S in ION \u201807+12+S\u2019 denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* \u201807++12\u2019: union of VOC2007 trainval and test and VOC2012 trainval. \u201807++12+COCO\u2019: trained on COCO trainval35k at \ufb01rst then \ufb01ne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for \u2018SS\u2019 which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n\u2022 By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uni\ufb01ed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It\u2019s also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: \u2018fast mode\u2019 Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: \u2018fast mode\u2019 Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n\u2022It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n\u2022It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n\u2022Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modi\ufb01ed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signi\ufb01cance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by \ufb01ne-tuning DNNs\u2019\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and re\ufb01ned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and re\ufb01ned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uni\ufb01ed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is bene\ufb01cial. To learn internal represen-\\ntations of saliency ef\ufb01ciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency ef\ufb01ciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive \ufb01elds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signi\ufb01cance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signi\ufb01cance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye \ufb01xations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is ef\ufb01cient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is ef\ufb01cient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is de\ufb01ned as below\\nF\u03b2 = (1 +\u03b22)Presion \u00d7Recall\\n\u03b22Presion + Recall (7)\\nwhere \u03b22 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH\u00d7W\\nH\u2211\\ni=1\\nW\u2211\\nj=1\\n\u23d0\u23d0\u23d0\u02c6S(i,j) = \u02c6Z(i,j)\\n\u23d0\u23d0\u23d0 (8)\\nwhere \u02c6Z and \u02c6S represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can \ufb01nd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insuf\ufb01cient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signi\ufb01cance for measuring local\\nconspicuity. Finally, it\u2019s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]\u2013[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural con\ufb01gurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classi\ufb01ers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classi\ufb01ers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nef\ufb01ciency. However, this detector may degrade signi\ufb01cantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]\u2013[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]\u2013[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nef\ufb01cient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wF\u03b2 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wF\u03b2 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wF\u03b2 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wF\u03b2 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='SOD wF\u03b2 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wF\u03b2 is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modi\ufb01cations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uni\ufb01ed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a \ufb01xed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating prede\ufb01ned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating prede\ufb01ned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na con\ufb01guration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-\ufb01ne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and re\ufb01ne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to re\ufb01ne the positions of possible\\nfaces. Qin et al. proposed a uni\ufb01ed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can re\ufb02ect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can re\ufb02ect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the re\ufb02ection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signi\ufb01cant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is ef\ufb01cient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is ef\ufb01cient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nef\ufb01ciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signi\ufb01cance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identi\ufb01cation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identi\ufb01cation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]\u2013[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin \u2018plain\u2019 features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different con\ufb01gurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modi\ufb01ed the down-\\nstream classi\ufb01er by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modi\ufb01ed the down-\\nstream classi\ufb01er by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand \ufb01ne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassi\ufb01ers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of con\ufb01dences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 \ufb01ne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modi\ufb01cation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='45 \ufb01ne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modi\ufb01cation and\\nsimpli\ufb01cation is of signi\ufb01cance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classi\ufb01ers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classi\ufb01ers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely \u2018Person (clear identi\ufb01cations)\u2019,\\n\u2018Person? (unclear identi\ufb01cations)\u2019 and \u2018People (large group of\\nindividuals)\u2019, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10\u22122 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The \ufb01rst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classi\ufb01ers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classi\ufb01ers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe \ufb01rst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n\u2022 Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n\u2022 Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speci\ufb01c\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n\u2022Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all bene\ufb01cial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n\u2022Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n\u2022Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classi\ufb01ers at latter stages can\\nhandle more dif\ufb01cult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are \ufb01xed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n\u2022 Unsupervised and weakly supervised learning. It\u2019s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand re\ufb01ne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n\u2022 Network optimization. Given speci\ufb01c applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n\u2022 Network optimization. Given speci\ufb01c applications and\\nplatforms, it is signi\ufb01cant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n\u20223D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n\u20223D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n\u2022 Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical \ufb02ow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical \ufb02ow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modi\ufb01cations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrie\ufb02y reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrie\ufb02y reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n\u201cObject detection with discriminatively trained part-based models,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\u201cObject detection with discriminatively trained part-based models,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, \u201cExample-based learning for view-based\\nhuman face detection,\u201dIEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39\u201351, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, \u201cPedestrian detection:\\nAn evaluation of the state of the art,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, \u201cDetection of spicules on mammogram\\nbased on skeleton analysis.\u201d IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235\u2013245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for\\nfast feature embedding,\u201d in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classi\ufb01cation\\nwith deep convolutional neural networks,\u201d in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,\u201d in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classi\ufb01cation\\nwith deep convolutional neural networks,\u201d in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, \u201cRealtime multi-person\\n2d pose estimation using part af\ufb01nity \ufb01elds,\u201d in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, \u201cA multi-scale cascade fully convolutional\\nnetwork face detector,\u201d in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, \u201cDeepdriving:\\nLearning affordance for direct perception in autonomous driving,\u201d in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object\\ndetection network for autonomous driving,\u201d in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, \u201cEmbedded streaming\\ndeep neural networks accelerator with applications,\u201d IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572\u20131583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, \u201cLow-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572\u20131583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, \u201cLow-complexity\\napproximate convolutional neural networks,\u201dIEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1\u201312, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n\u201cCost-sensitive learning of deep feature representations from imbal-\\nanced data.\u201d IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1\u201315, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, \u201cFeature extraction with deep\\nneural networks by a generalized discriminant analysis.\u201d IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596\u2013608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature\\nhierarchies for accurate object detection and semantic segmentation,\u201d\\nin CVPR, 2014.\\n[16] R. Girshick, \u201cFast r-cnn,\u201d in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, \u201cFast r-cnn,\u201d in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look\\nonce: Uni\ufb01ed, real-time object detection,\u201d in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-\\ntime object detection with region proposal networks,\u201d in NIPS, 2015,\\npp. 91\u201399.\\n[19] D. G. Lowe, \u201cDistinctive image features from scale-invariant key-\\npoints,\u201d Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91\u2013110, 2004.\\n[20] N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human\\ndetection,\u201d in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, \u201cAn extended set of haar-like features for\\nrapid object detection,\u201d in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, \u201cSupport vector machine,\u201dMachine Learning,\\nvol. 20, no. 3, pp. 273\u2013297, 1995.\\n[23] Y . Freund and R. E. Schapire, \u201cA desicion-theoretic generalization of\\non-line learning and an application to boosting,\u201d J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663\u2013671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, \u201cA desicion-theoretic generalization of\\non-line learning and an application to boosting,\u201d J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663\u2013671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n\u201cObject detection with discriminatively trained part-based models,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627\u20131645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, \u201cThe pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),\u201d 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol.\\n521, no. 7553, pp. 436\u2013444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, \u201cPredicting eye \ufb01xations\\nusing convolutional neural networks,\u201d in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, \u201cLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,\u201d in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,\u201d in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, \u201cLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,\u201d in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, \u201cFace detection with the faster r-cnn,\u201d\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, \u201cJoint cascade face\\ndetection and alignment,\u201d in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, \u201cSupervised transformer network\\nfor ef\ufb01cient face detection,\u201d in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, \u201cA real-time\\npedestrian detector using deep learning for human-aware navigation,\u201d\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, \u201cExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassi\ufb01ers,\u201d in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, \u201cA survey of deep learning methods and\\nsoftware tools for image classi\ufb01cation and object detection,\u201d Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, \u201cHow we know universals the perception\\nof auditory and visual forms,\u201dThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127\u2013147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, \u201cHow we know universals the perception\\nof auditory and visual forms,\u201dThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127\u2013147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \u201cLearning internal\\nrepresentation by back-propagation of errors,\u201d Nature, vol. 323, no.\\n323, pp. 533\u2013536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality\\nof data with neural networks,\u201d Sci., vol. 313, pp. 504\u2013507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., \u201cDeep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,\u201d IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82\u201397, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet:\\nA large-scale hierarchical image database,\u201d in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,\u201d in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, \u201cBinary coding of speech spectrograms using a deep auto-\\nencoder,\u201d in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., \u201cPhone recognition with\\nthe mean-covariance restricted boltzmann machine,\u201d in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, \u201cImproving neural networks by preventing co-\\nadaptation of feature detectors,\u201d arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,\u201d in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n\u201cOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,\u201d arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\u201cOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,\u201d arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, \u201cGoing deeper with\\nconvolutions,\u201d in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\\nfor large-scale image recognition,\u201d arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\\nrecognition,\u201d in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted\\nboltzmann machines,\u201d in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , \u201cWeakly supervised\\nobject recognition with convolutional neural networks,\u201d in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \u201cLearning and transferring\\nmid-level image representations using convolutional neural networks,\u201d\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \u201cLearning and transferring\\nmid-level image representations using convolutional neural networks,\u201d\\nin CVPR, 2014.\\n[51] F. M. Wadley, \u201cProbit analysis: a statistical treatment of the sigmoid\\nresponse curve,\u201d Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549\u2013553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , \u201cLearning invariant\\nfeatures through topographic \ufb01lter maps,\u201d in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, \u201cLearning convolutional feature hierarchies for visual\\nrecognition,\u201d in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, \u201cDeconvolu-\\ntional networks,\u201d in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, \u201cLearning deconvolution network for\\nsemantic segmentation,\u201d in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, \u201cPlant leaf iden-\\nti\ufb01cation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,\u201d in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, \u201cPlant leaf iden-\\nti\ufb01cation via a growing convolution neural network with progressive\\nsample learning,\u201d in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, \u201cNeural codes\\nfor image retrieval,\u201d in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n\u201cDeep learning for content-based image retrieval: A comprehensive\\nstudy,\u201d in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. Barof\ufb01o, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, \u201cDeep convolutional neural networks for pedestrian detec-\\ntion,\u201d Signal Process.: Image Commun. , vol. 47, pp. 482\u2013489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, \u201cSubcategory-aware\\nconvolutional neural networks for object proposals and detection,\u201d in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, \u201cPedestrian\\ndetection based on fast r-cnn and batch normalization,\u201d in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, \u201cPedestrian\\ndetection based on fast r-cnn and batch normalization,\u201d in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n\u201cMultimodal deep learning,\u201d in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, \u201cModeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classi\ufb01-\\ncation,\u201d in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid pooling in deep\\nconvolutional networks for visual recognition,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904\u20131916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., \u201cR-fcn: Object detection via region-based\\nfully convolutional networks,\u201d in NIPS, 2016, pp. 379\u2013387.\\n[66] T.-Y . Lin, P. Doll \u00b4ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, \u201cFeature pyramid networks for object detection,\u201d in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll \u00b4ar, and R. B. Girshick, \u201cMask r-cnn,\u201d in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Belongie, \u201cFeature pyramid networks for object detection,\u201d in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll \u00b4ar, and R. B. Girshick, \u201cMask r-cnn,\u201d in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, \u201cScalable object\\ndetection using deep neural networks,\u201d in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, \u201cAttentionnet:\\nAggregating weak directions for accurate object detection,\u201d in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, \u201cG-cnn: an iterative grid\\nbased object detector,\u201d in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, \u201cSsd: Single shot multibox detector,\u201d in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, \u201cYolo9000: better, faster, stronger,\u201d\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, \u201cDssd:\\nDeconvolutional single shot detector,\u201d arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, \u201cDsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,\u201d arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, \u201cDsod:\\nLearning deeply supervised object detectors from scratch,\u201d in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, \u201cTransforming auto-\\nencoders,\u201d in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, \u201cLearning invariance\\nthrough imitation,\u201d in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, \u201cHistograms of sparse codes for object\\ndetection,\u201d in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n\u201cSelective search for object recognition,\u201d Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154\u2013171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, \u201cPedestrian\\ndetection with unsupervised multi-stage feature learning,\u201d in CVPR,\\n2013.\\n[80] P. Kr \u00a8ahenb\u00a8uhl and V . Koltun, \u201cGeodesic object proposals,\u201d in ECCV,\\n2014.\\n[81] P. Arbel \u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr \u00a8ahenb\u00a8uhl and V . Koltun, \u201cGeodesic object proposals,\u201d in ECCV,\\n2014.\\n[81] P. Arbel \u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n\u201cMultiscale combinatorial grouping,\u201d in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll \u00b4ar, \u201cEdge boxes: Locating object proposals\\nfrom edges,\u201d in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, \u201cDeepbox: Learning objectness\\nwith convolutional networks,\u201d in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll \u00b4ar, \u201cLearning to\\nre\ufb01ne object segments,\u201d in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, \u201cImproving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,\u201d in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel \u00b4aez, and J. Malik, \u201cLearning rich features\\nfrom rgb-d images for object detection and segmentation,\u201d in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,\u201d in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., \u201cDeepid-net: Deformable deep convolutional\\nneural networks for object detection,\u201d in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, \u201cR-cnn minus r,\u201d arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, \u201cBeyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,\u201d\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S \u00b4anchez, and T. Mensink, \u201cImproving the \ufb01sher kernel\\nfor large-scale image classi\ufb01cation,\u201d in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, \u201cRestructuring of deep neural network\\nacoustic models with singular value decomposition.\u201d in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\\nobject detection with region proposal networks,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137\u20131149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink-\\ning the inception architecture for computer vision,\u201d in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll \u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in\\ncontext,\u201d in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, \u201cInside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,\u201d in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, \u201cPixelwise instance segmentation with a\\ndynamically instantiated network,\u201d in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, \u201cInstance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, \u201cPixelwise instance segmentation with a\\ndynamically instantiated network,\u201d in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, \u201cInstance-aware semantic segmentation via\\nmulti-task network cascades,\u201d in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, \u201cFully convolutional instance-\\naware semantic segmentation,\u201d in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n\u201cSpatial transformer networks,\u201d in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, \u201cStuffnet: Using stuffto\\nimprove object detection,\u201d in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, \u201cHypernet: Towards accurate\\nregion proposal generation and joint object detection,\u201d in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, \u201cCurriculum learning\\nof multiple tasks,\u201d in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, \u201cRotating your\\nface using multi-task deep neural network,\u201d in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,\u201d in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, \u201cRotating your\\nface using multi-task deep neural network,\u201d in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, \u201cMulti-stage object\\ndetection with group recursive learning,\u201d arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, \u201cA uni\ufb01ed multi-scale\\ndeep convolutional neural network for fast object detection,\u201d in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, \u201csegdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,\u201d in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, \u201cScene labeling\\nwith lstm recurrent neural networks,\u201d in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, \u201cLearning to detect and\\nlocalize many objects from few examples,\u201d arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, \u201cGated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,\u201d arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, \u201cGated bi-\\ndirectional cnn for object detection,\u201d in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, \u201cObject detection via a multi-region and\\nsemantic segmentation-aware cnn model,\u201d in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, \u201cBidirectional recurrent neural net-\\nworks,\u201d IEEE Trans. Signal Process. , vol. 45, pp. 2673\u20132681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll \u00b4ar, \u201cA multipath network for object detection,\u201d\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, \u201cTraining region-based\\nobject detectors with online hard example mining,\u201d in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, \u201cObject detection\\nnetworks on convolutional feature maps,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476\u20131481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, \u201cObject detection\\nnetworks on convolutional feature maps,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476\u20131481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, \u201cFactors in \ufb01netuning\\ndeep model for object detection with long-tail distribution,\u201d in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, \u201cPvanet:\\nLightweight deep neural networks for real-time object detection,\u201d\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, \u201cUnderstanding and\\nimproving convolutional neural networks via concatenated recti\ufb01ed\\nlinear units,\u201d in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, \u201cDeep neural networks for object\\ndetection,\u201d in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll \u00b4ar, \u201cLearning to segment object\\ncandidates,\u201d in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, \u201cScalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll \u00b4ar, \u201cLearning to segment object\\ncandidates,\u201d in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, \u201cScalable,\\nhigh-quality object detection,\u201d arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n\u201cThe pascal visual object classes challenge 2012 (voc2012) results\\n(2012),\u201d in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolu-\\ntional networks,\u201d in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll \u00b4ar, Z. Tu, and K. He, \u201cAggregated residual\\ntransformations for deep neural networks,\u201d in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n\u201cDeformable convolutional networks,\u201d arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, \u201cAutocollage,\u201dACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847\u2013852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='\u201cDeformable convolutional networks,\u201d arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, \u201cAutocollage,\u201dACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847\u2013852, 2006.\\n[126] C. Jung and C. Kim, \u201cA uni\ufb01ed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,\u201d IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272\u20131283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, \u201cReal-time salient object\\ndetection with a minimum spanning tree,\u201d in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, \u201cTop-down visual saliency via joint crf and\\ndictionary learning,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576\u2013588, 2017.\\n[129] P. L. Rosin, \u201cA simple method for detecting salient regions,\u201d Pattern\\nRecognition, vol. 42, no. 11, pp. 2363\u20132371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n\u201cLearning to detect a salient object,\u201d IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363\u20132371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n\u201cLearning to detect a salient object,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353\u2013367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks\\nfor semantic segmentation,\u201d in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, \u201cDiscriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989\u20131005, 2009.\\n[133] S. Xie and Z. Tu, \u201cHolistically-nested edge detection,\u201d in ICCV, 2015.\\n[134] M. K \u00a8ummerer, L. Theis, and M. Bethge, \u201cDeep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,\u201d\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, \u201cSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,\u201d\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, \u201cSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,\u201d\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, \u201cDeep networks for saliency\\ndetection via local estimation and global search,\u201d in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, \u201cWeakly supervised top-down\\nsalient object detection,\u201d arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, \u201cSaliency detection by\\nmulti-context deep learning,\u201d in CVPR, 2015.\\n[139] C \u00b8 . Bak, A. Erdem, and E. Erdem, \u201cTwo-stream convolutional networks\\nfor dynamic saliency prediction,\u201d arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, \u201cSupercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,\u201d\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330\u2013344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330\u2013344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, \u201cDeepsaliency: Multi-task deep neural network model for\\nsalient object detection,\u201d IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919\u20133930, 2016.\\n[142] Y . Tang and X. Wu, \u201cSaliency detection via combining region-level\\nand pixel-level predictions with cnns,\u201d in ECCV, 2016.\\n[143] G. Li and Y . Yu, \u201cDeep contrast learning for salient object detection,\u201d\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, \u201cEdge preserving and\\nmulti-scale contextual neural network for salient object detection,\u201d\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, \u201cA deep multi-level\\nnetwork for saliency prediction,\u201d in ICPR, 2016.\\n[146] G. Li and Y . Yu, \u201cVisual saliency detection based on multiscale deep\\ncnn features,\u201d IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012\u2013\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, \u201cVisual saliency detection based on multiscale deep\\ncnn features,\u201d IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012\u2013\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O\u2019Connor,\\n\u201cShallow and deep convolutional networks for saliency prediction,\u201d in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, \u201cRecurrent attentional networks for\\nsaliency detection,\u201d in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, \u201cDeeply-supervised recurrent convolutional\\nneural network for saliency detection,\u201d in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, \u201cContextual\\nhypergraph modeling for salient object detection,\u201d in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, \u201cGlobal\\ncontrast based salient region detection,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569\u2013582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, \u201cSalient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,\u201d IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569\u2013582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, \u201cSalient object\\ndetection: A discriminative regional feature integration approach,\u201d in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, \u201cDeep saliency with encoded low level\\ndistance map and high level features,\u201d in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n\u201cNon-local deep features for salient object detection,\u201d in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n\u201cDeeply supervised salient object detection with short connections,\u201d\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, \u201cHierarchical saliency detection,\u201d in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, \u201cThe secrets of\\nsalient object segmentation,\u201d in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, \u201cDesign and perceptual validation of\\nperformance measures for salient object segmentation,\u201d in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, \u201cSalient object detection:\\nA benchmark,\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706\u2013\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, \u201cGraphical representation for\\nheterogeneous face recognition,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301\u2013312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, \u201cGraphical representation for\\nheterogeneous face recognition,\u201d IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301\u2013312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, \u201cFace recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,\u201d in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, \u201cFace sketchcphoto synthesis\\nand retrieval using sparse representation,\u201d IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213\u20131226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, \u201cA comprehensive survey\\nto face hallucination,\u201d Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9\u201330, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, \u201cMultiple\\nrepresentations-based face sketch-photo synthesis.\u201dIEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201\u20132215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, \u201cAutomatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201\u20132215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, \u201cAutomatic facial\\nexpression recognition system using deep network-based data fusion,\u201d\\nIEEE Trans. Cybern. , vol. 48, pp. 103\u2013114, 2018.\\n[166] P. Viola and M. Jones, \u201cRobust real-time face detection,\u201d Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137\u2013154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, \u201cUnitbox: An advanced\\nobject detection network,\u201d in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, \u201cMulti-view face detection\\nusing deep convolutional neural networks,\u201d in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, \u201cFrom facial parts responses\\nto face detection: A deep learning approach,\u201d in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, \u201cFace detection through\\nscale-friendly deep convolutional networks,\u201d in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, \u201cScale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,\u201d in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, \u201cScale-aware face\\ndetection,\u201d in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, \u201cFace r-cnn,\u201d arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, \u201cFace detection using deep learning: An\\nimproved faster rcnn approach,\u201d arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, \u201cDensebox: Unifying landmark\\nlocalization with end to end object detection,\u201d arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, \u201cface detection with end-to-end\\nintegration of a convnet and a 3d model,\u201d in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, \u201cJoint face detection and\\nalignment using multitask cascaded convolutional networks,\u201d IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499\u20131503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, \u201cCompact convolutional neural\\nnetwork cascadefor face detection,\u201d in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499\u20131503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, \u201cCompact convolutional neural\\nnetwork cascadefor face detection,\u201d in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, \u201cJoint training of cascaded cnn for\\nface detection,\u201d in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, \u201cFddb: A benchmark for face detection\\nin unconstrained settings,\u201d Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, \u201cA convolutional neural\\nnetwork cascade for face detection,\u201d in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cAggregate channel features for\\nmulti-view face detection,\u201d in IJCB, 2014.\\n[182] N. Marku \u02c7s, M. Frljak, I. S. Pand \u02c7zi\u00b4c, J. Ahlberg, and R. Forchheimer,\\n\u201cObject detection with pixel intensity comparisons organized in deci-\\nsion trees,\u201d arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, \u201cFace\\ndetection without bells and whistles,\u201d in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='sion trees,\u201d arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, \u201cFace\\ndetection without bells and whistles,\u201d in ECCV, 2014.\\n[184] J. Li and Y . Zhang, \u201cLearning surf cascade for fast and accurate object\\ndetection,\u201d in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, \u201cA fast and accurate unconstrained\\nface detector,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211\u2013223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cConvolutional channel features,\u201d\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, \u201cHyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,\u201d arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, \u201cFinding tiny faces,\u201d in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, \u201cMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,\u201d IEEE Trans.\\nImage Process., vol. 27, pp. 1361\u20131375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, \u201cMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,\u201d IEEE Trans.\\nImage Process., vol. 27, pp. 1361\u20131375, 2018.\\n[190] D. Gavrila and S. Munder, \u201cMulti-cue pedestrian detection and tracking\\nfrom a moving vehicle,\u201d Int. J. of Comput. Vision , vol. 73, pp. 41\u201359,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, \u201cJointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidenti\ufb01cation,\u201d in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, \u201cStepwise metric promotion for unsuper-\\nvised video person re-identi\ufb01cation,\u201d in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, \u201cCooperative robots to observe\\nmoving targets: Review,\u201d IEEE Trans. Cybern. , vol. 48, pp. 187\u2013198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\\nThe kitti dataset,\u201d Int. J. of Robotics Res. , vol. 32, pp. 1231\u20131237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\\nThe kitti dataset,\u201d Int. J. of Robotics Res. , vol. 32, pp. 1231\u20131237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, \u201cLearning complexity-aware\\ncascades for deep pedestrian detection,\u201d in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\\nfor pedestrian detection,\u201d in CVPR, 2015.\\n[197] P. Doll \u00b4ar, R. Appel, S. Belongie, and P. Perona, \u201cFast feature pyramids\\nfor object detection,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532\u20131545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, \u201cFiltered channel features for\\npedestrian detection,\u201d in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, \u201cPedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,\u201d\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243\u20131257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, \u201cDiscriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243\u20131257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, \u201cDiscriminatively trained\\nand-or graph models for object shape detection,\u201d IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959\u2013972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, \u201cHandling\\nocclusions with franken-classi\ufb01ers,\u201d in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, \u201cDetection and tracking of\\noccluded people,\u201d Int. J. of Comput. Vision, vol. 110, pp. 58\u201369, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, \u201cIs faster r-cnn doing well for\\npedestrian detection?\u201d in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\\nfor pedestrian detection,\u201d in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, \u201cMultispectral deep\\nneural networks for pedestrian detection,\u201d arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cPedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,\u201d arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cPedestrian detection aided by\\ndeep learning semantic tasks,\u201d in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, \u201cFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,\u201d in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, \u201cPushing\\nthe limits of deep cnns for pedestrian detection,\u201d IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom \u00b4e, L. Bondi, L. Barof\ufb01o, S. Tubaro, E. Plebani, and D. Pau,\\n\u201cReduced memory region based deep convolutional neural network\\ndetection,\u201d in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, \u201cTaking a deeper\\nlook at pedestrians,\u201d in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, \u201cScale-aware fast\\nr-cnn for pedestrian detection,\u201d arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,\u201d in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, \u201cScale-aware fast\\nr-cnn for pedestrian detection,\u201d arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, \u201cVisual-textual\\njoint relevance learning for tag-based social image search,\u201dIEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363\u2013376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, \u201cRon: Reverse\\nconnection with objectness prior networks for object detection,\u201d in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, \u201cGenerative adversarial nets,\u201d\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, \u201cObject\\ndetection meets knowledge graphs,\u201d in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, \u201cSaliency-based sequential\\nimage attention with multiset prediction,\u201d in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,\u201d in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, \u201cSaliency-based sequential\\nimage attention with multiset prediction,\u201d in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, \u201cLearning detection with diverse\\nproposals,\u201d in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, \u201cEnd-to-end\\nmemory networks,\u201d in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, \u201cReal time image saliency for black box\\nclassi\ufb01ers,\u201d in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cCraft objects from images,\u201d in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, \u201cUnsupervised learning\\nfrom video to detect foreground objects in single images,\u201d in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, \u201cWeakly supervised object\\nlocalization with latent category learning,\u201d in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n\u201cTraining object class detectors with click supervision,\u201d inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, \u201cSpeed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, \u201cSpeed/accuracy\\ntrade-offs for modern convolutional object detectors,\u201d in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, \u201cMimicking very ef\ufb01cient network for object\\ndetection,\u201d in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a\\nneural network,\u201d Comput. Sci., vol. 14, no. 7, pp. 38\u201339, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, \u201cFitnets: Hints for thin deep nets,\u201d Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, \u201c3d object proposals for accurate object class detection,\u201d\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,\u201d in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,\u201d in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n\u201cObject detection in videos with tubelet proposal networks,\u201d in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor\u2019s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Arti\ufb01cial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/object_detection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'object_detection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b5b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542c54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 401 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e044c4a9d9954ac0a5e40c3f086bc96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully with shape (401, 384)\n",
      "Adding 401 documents to the vector store...\n",
      "401 documents added successfully to the vector store\n",
      "total documents in the collection: 401\n"
     ]
    }
   ],
   "source": [
    "## Generate Embeddings\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "\n",
    "## Add embeddings to vector store\n",
    "\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "497cb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query based retrival from vector store\"\"\"\n",
    "    def __init__(self, vectorstore:VectorStore, embedding_manager: EmbeddingManager):\n",
    "\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query:str, top_k:int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \n",
    "        print(f\"Retrieving top {top_k} documents for query: {query}\")\n",
    "        print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "\n",
    "        query_embedding = self.embedding_manager.generate_embedding([query])[0]\n",
    "\n",
    "        ## Perform similarity search\n",
    "        try:\n",
    "            results = self.vectorstore.collection.query(\n",
    "                query_embeddings = [query_embedding.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "            # print(results)\n",
    "\n",
    "            ## Process results\n",
    "\n",
    "            retrived_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                scores = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                # print(f\"Documents: {documents}\")\n",
    "                # print(f\"Metadatas: {metadatas}\")\n",
    "                # print(f\"Scores: {scores}\")\n",
    "                # print(f\"IDs: {ids}\")\n",
    "\n",
    "                for i, (doc, metadata, score, id) in enumerate(zip(documents, metadatas, scores, ids)):\n",
    "                    similarity_score = 1 - (score / 2)\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrived_docs.append({\n",
    "                            \"id\": id,\n",
    "                            \"content\": doc,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            \"distance\": score,\n",
    "                            \"rank\": i+1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrived {len(retrived_docs)} documents with score greater than {score_threshold}\")\n",
    "            else:\n",
    "                print(\"No documents found in the vector store\")\n",
    "            \n",
    "            return retrived_docs\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during Retrival: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d478feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriver = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2273ec8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x137849d30>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7dc4f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 5 documents for query: what is role of Deep learning in Pedestrian Detection\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86cf054aa2240a78f9988ade42cf7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully with shape (1, 384)\n",
      "Retrived 5 documents with score greater than 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_399f4332_200',\n",
       "  'content': 'ditions. It should be noticed that the covered domains are\\ndiversi\ufb01ed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classi\ufb01cation and object detection, but\\npays little attention on detailing speci\ufb01c algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-',\n",
       "  'metadata': {'contect_length': 957,\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'doc_index': 200,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2',\n",
       "   'keywords': '',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'page': 1,\n",
       "   'total_pages': 21,\n",
       "   'title': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'subject': '',\n",
       "   'source': '../data/pdf/object_detection.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'trapped': '/False',\n",
       "   'author': '',\n",
       "   'page_label': '2',\n",
       "   'source_file': 'object_detection.pdf'},\n",
       "  'similarity_score': 0.8579398691654205,\n",
       "  'distance': 0.28412026166915894,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_72473384_393',\n",
       "  'content': 'neural networks for pedestrian detection,\u201d arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, \u201cPedestrian detection aided by\\ndeep learning semantic tasks,\u201d in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, \u201cFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,\u201d in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, \u201cPushing\\nthe limits of deep cnns for pedestrian detection,\u201d IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom \u00b4e, L. Bondi, L. Barof\ufb01o, S. Tubaro, E. Plebani, and D. Pau,\\n\u201cReduced memory region based deep convolutional neural network\\ndetection,\u201d in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, \u201cTaking a deeper\\nlook at pedestrians,\u201d in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, \u201cScale-aware fast\\nr-cnn for pedestrian detection,\u201d arXiv:1510.08160, 2015.',\n",
       "  'metadata': {'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'page_label': '20',\n",
       "   'page': 19,\n",
       "   'total_pages': 21,\n",
       "   'contect_length': 935,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2',\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'keywords': '',\n",
       "   'source_file': 'object_detection.pdf',\n",
       "   'title': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'source': '../data/pdf/object_detection.pdf',\n",
       "   'author': '',\n",
       "   'doc_index': 393},\n",
       "  'similarity_score': 0.8524312674999237,\n",
       "  'distance': 0.2951374650001526,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_7f7891fb_335',\n",
       "  'content': 'main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different con\ufb01gurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modi\ufb01ed the down-\\nstream classi\ufb01er by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'author': '',\n",
       "   'total_pages': 21,\n",
       "   'contect_length': 948,\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'source_file': 'object_detection.pdf',\n",
       "   'page_label': '15',\n",
       "   'source': '../data/pdf/object_detection.pdf',\n",
       "   'page': 14,\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2',\n",
       "   'title': '',\n",
       "   'doc_index': 335},\n",
       "  'similarity_score': 0.8455846309661865,\n",
       "  'distance': 0.30883073806762695,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_fac0bdc1_190',\n",
       "  'content': 'deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modi\ufb01cations and useful tricks\\nto improve detection performance further. As distinct speci\ufb01c\\ndetection tasks exhibit different characteristics, we also brie\ufb02y\\nsurvey several speci\ufb01c tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in',\n",
       "  'metadata': {'author': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'source_file': 'object_detection.pdf',\n",
       "   'trapped': '/False',\n",
       "   'title': '',\n",
       "   'source': '../data/pdf/object_detection.pdf',\n",
       "   'page_label': '1',\n",
       "   'keywords': '',\n",
       "   'page': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2',\n",
       "   'doc_index': 190,\n",
       "   'total_pages': 21,\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'contect_length': 990},\n",
       "  'similarity_score': 0.8420966863632202,\n",
       "  'distance': 0.31580662727355957,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_7bc4f7d8_361',\n",
       "  'content': 'THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, \u201cSupervised transformer network\\nfor ef\ufb01cient face detection,\u201d in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, \u201cA real-time\\npedestrian detector using deep learning for human-aware navigation,\u201d\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, \u201cExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassi\ufb01ers,\u201d in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, \u201cA survey of deep learning methods and\\nsoftware tools for image classi\ufb01cation and object detection,\u201d Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, \u201cHow we know universals the perception\\nof auditory and visual forms,\u201dThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127\u2013147, 1947.',\n",
       "  'metadata': {'doc_index': 361,\n",
       "   'page_label': '18',\n",
       "   'page': 17,\n",
       "   'creationdate': '2019-04-17T00:45:22+00:00',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'trapped': '/False',\n",
       "   'contect_length': 956,\n",
       "   'total_pages': 21,\n",
       "   'keywords': '',\n",
       "   'author': '',\n",
       "   'moddate': '2019-04-17T00:45:22+00:00',\n",
       "   'title': '',\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'source_file': 'object_detection.pdf',\n",
       "   'source': '../data/pdf/object_detection.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2'},\n",
       "  'similarity_score': 0.840392142534256,\n",
       "  'distance': 0.31921571493148804,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"what is role of Deep learning in Pedestrian Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eca99e",
   "metadata": {},
   "source": [
    "#### Integration of context to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0057565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline using GROQ API\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\", temperature=0, max_tokens = 1024)\n",
    "\n",
    "def rag_simple(query:str, retriever, llm, top_k:int = 5):\n",
    "    \"\"\" Simple RAG pipeline \"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top K: {top_k}\")\n",
    "\n",
    "    ## Retrieve\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "    context = \"\\n\\n\".join([result['content'] for result in results]) if results else \"\"\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant context found in the document\"\n",
    "\n",
    "    ## Generate response using Groq LLM\n",
    "    prompt = f\"\"\" Use the following question to answer the question concisely.\n",
    "\n",
    "        context: {context}\n",
    "\n",
    "        question: {query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67e232c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: why is xgboost so popular\n",
      "Top K: 5\n",
      "Retrieving top 5 documents for query: why is xgboost so popular\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d17b714184a4247bdbd8ba0b4534541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully with shape (1, 384)\n",
      "Retrived 5 documents with score greater than 0.0\n",
      "XGBoost is so popular due to its scalability in all scenarios, running more than ten times faster than existing popular solutions on a single machine and scaling to billions of examples in distributed or memory-limited settings. Its success can be attributed to several key innovations, including:\n",
      "\n",
      "1. A novel tree learning algorithm for handling sparse data.\n",
      "2. A theoretically justified weighted quantile sketch procedure for handling instance weights.\n",
      "3. Parallel and distributed computing, making learning faster and enabling quicker model exploration.\n",
      "4. The ability to exploit out-of-core data sets with given resources.\n",
      "\n",
      "These innovations, combined with its ability to handle large datasets and provide state-of-the-art results on a wide range of problems, have made XGBoost the consensus choice of learner in many competitions and real-world applications.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"why is xgboost so popular\", rag_retriver, llm)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}